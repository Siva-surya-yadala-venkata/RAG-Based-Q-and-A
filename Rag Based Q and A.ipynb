{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "698d4394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Om Nama Sivaya\n"
     ]
    }
   ],
   "source": [
    "print(\"Om Nama Sivaya\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f6756",
   "metadata": {},
   "source": [
    "FLOW CHART WE ARE FOLLOWING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5f6bd",
   "metadata": {},
   "source": [
    "![AI Ethics](flowchart.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b495a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\sivas\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\sivas\\anaconda3\\lib\\site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\sivas\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: Chroma in c:\\users\\sivas\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: langchain-experimental in c:\\users\\sivas\\anaconda3\\lib\\site-packages (0.0.42)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\sivas\\anaconda3\\lib\\site-packages (1.26.6)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain) (1.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.41)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sivas\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-community langchain-core Chroma langchain-experimental  pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a9e0b",
   "metadata": {},
   "source": [
    "Importing the Necessary Libraries Required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "814e2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFaceEndpointEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers import MultiQueryRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f4490",
   "metadata": {},
   "source": [
    "So there are 4 Parts in the Way to Build the Rag Based Q and A Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1fc84",
   "metadata": {},
   "source": [
    "1.Document Ingestion (Indexing) means where You Load the Documents And Make it into Chunks with some preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dcd8fd",
   "metadata": {},
   "source": [
    "2.Reterival means Retervial So when a Query Passed So what are The Required Documents are there in terms of relvance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b92e4e",
   "metadata": {},
   "source": [
    "3.Augementation(Combining) means Combing the Query by the user and with the retervial Documents we will be sending into the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff7ef1",
   "metadata": {},
   "source": [
    "4.Generation means Here Generating the Final Result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc57cbe",
   "metadata": {},
   "source": [
    "SO WE ARE USING AN HUGGING FACE MODEL : \n",
    "OPEN AI FINE TUNED MODEL WITH 120 BILLION PARAMETERS AS THE MODEL (openai/gpt-oss-120b)\n",
    "\n",
    "FOR EMBEDDING MODEL USING AN HUGGING FACE:\n",
    "GOOGLE FINE TUNED 300 MILLION PARAMETERS TRAINED EMBEDDED MODEL (google/embeddinggemma-300m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = 'Please Place Your Hugging Face Token'\n",
    "\n",
    "openai_model = HuggingFaceEndpoint(repo_id = 'openai/gpt-oss-120b',task='text-generation',huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
    "\n",
    "model = ChatHuggingFace(llm = openai_model)\n",
    "# The ChatLLM Model we are using\n",
    "\n",
    "embedding_model = HuggingFaceEndpointEmbeddings(model = 'google/embeddinggemma-300m',huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
    "# The embedding model we are using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "42a22826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatHuggingFace(llm=HuggingFaceEndpoint(repo_id='openai/gpt-oss-120b', huggingfacehub_api_token='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', stop_sequences=[], server_kwargs={}, model_kwargs={}, model='openai/gpt-oss-120b', client=<InferenceClient(model='openai/gpt-oss-120b', timeout=120)>, async_client=<InferenceClient(model='openai/gpt-oss-120b', timeout=120)>, task='text-generation'), model_id='openai/gpt-oss-120b', model_kwargs={})"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f4274688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpointEmbeddings(client=<InferenceClient(model='google/embeddinggemma-300m', timeout=None)>, async_client=<InferenceClient(model='google/embeddinggemma-300m', timeout=None)>, model='google/embeddinggemma-300m', provider=None, repo_id='google/embeddinggemma-300m', task='feature-extraction', model_kwargs=None, huggingfacehub_api_token='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')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dec691",
   "metadata": {},
   "source": [
    "Importing the Documents in terms of Pdf's So we are using PyMuPDF loader which is very helpful to extract the documents(data) from a research Paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df2e76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader , DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='Books',\n",
    "    glob='*.pdf',\n",
    "    loader_cls= PyMuPDFLoader\n",
    ")\n",
    "\n",
    "# So basically I am acessing the Directory which is Books so asking the Directory Loader to Load all the Pdfs and to load use PyMuPDFLoader\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7006ddab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 0}, page_content='Applied Artiﬁcial Intelligence\\nAn International Journal\\nISSN: 0883-9514 (Print) 1087-6545 (Online) Journal homepage: www.tandfonline.com/journals/uaai20\\nAI Ethics: Integrating Transparency, Fairness, and\\nPrivacy in AI Development\\nPetar Radanliev\\nTo cite this article: Petar Radanliev (2025) AI Ethics: Integrating Transparency, Fairness,\\nand Privacy in AI Development, Applied Artiﬁcial Intelligence, 39:1, 2463722, DOI:\\n10.1080/08839514.2025.2463722\\nTo link to this article:  https://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with\\nlicense by Taylor & Francis Group, LLC.\\nPublished online: 07 Feb 2025.\\nSubmit your article to this journal \\nArticle views: 34975\\nView related articles \\nView Crossmark data\\nCiting articles: 52 View citing articles \\nFull Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=uaai20'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 1}, page_content='AI Ethics: Integrating Transparency, Fairness, and Privacy in \\nAI Development\\nPetar Radanliev\\nDepartment of Computer Science, University of Oxford, Oxford, UK\\nABSTRACT\\nThe expansion of Artificial Intelligence in sectors such as health\\xad\\ncare, finance, and communication has raised critical ethical \\nconcerns surrounding transparency, fairness, and privacy. \\nAddressing these issues is essential for the responsible devel\\xad\\nopment and deployment of AI systems. This research estab\\xad\\nlishes a comprehensive ethical framework that mitigates \\nbiases and promotes accountability in AI technologies. \\nA comparative analysis of international AI policy frameworks \\nfrom regions including the European Union, United States, and \\nChina is conducted using analytical tools such as Venn diagrams \\nand Cartesian graphs. These tools allow for a visual and sys\\xad\\ntematic evaluation of the ethical principles guiding AI develop\\xad\\nment across different jurisdictions. The results reveal significant \\nvariations in how global regions prioritize transparency, fairness, \\nand privacy, with challenges in creating a unified ethical stan\\xad\\ndard. To address these challenges, we propose technical strate\\xad\\ngies, including fairness-aware algorithms, routine audits, and \\nthe establishment of diverse development teams to ensure \\nethical AI practices. This paper provides actionable recommen\\xad\\ndations for integrating ethical oversight into the AI lifecycle, \\nadvocating for the creation of AI systems that are both techni\\xad\\ncally sophisticated and aligned with societal values. The findings \\nunderscore the necessity of global collaboration in fostering \\nethical AI development.\\nARTICLE HISTORY \\nReceived 11 August 2024  \\nRevised 5 September 2024  \\nAccepted 2 February 2025  \\nIntroduction\\nIn recent years, substantial advancements in AI ethics have emerged, with \\nsignificant contributions addressing transparency, fairness, and privacy in AI \\ndevelopment. Recent research studies (Bender et al. 2021) highlight the dan\\xad\\ngers of bias in large language models, raising concerns over the perpetuation of \\nsocietal inequalities within AI systems. Similarly, Bommasani et al. (2023) \\ncritically evaluate compliance of foundation models with the draft EU AI Act, \\nreflecting broader concerns over the accountability of AI systems at \\na foundational level. Moreover, Aldoseri, Al-Khalifa, and Hamouda (2023) \\nCONTACT Petar Radanliev \\npetar.radanliev@cs.ox.ac.uk \\nDepartment of Computer Science, University of \\nOxford, Oxford, UK\\nAPPLIED ARTIFICIAL INTELLIGENCE                    \\n2025, VOL. 39, NO. 1, e2463722 (41 pages) \\nhttps://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with license by Taylor & Francis Group, LLC.  \\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/ \\nlicenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly \\ncited. The terms on which this article has been published allow the posting of the Accepted Manuscript in a repository by the author(s) \\nor with their consent.'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 2}, page_content='propose new data strategies for AI development, focusing on the integration of \\nethical principles across diverse datasets to mitigate bias. These works, along\\xad\\nside key regulatory frameworks from bodies such as the European Union \\n(European Parliament 2023) and NIST (2024b), reinforce the imperative of \\ndeveloping AI systems that are not only transparent and fair but also respect \\ndata privacy and societal norms. By situating this study within the context of \\nthese contemporary advancements, the aim is to build upon these discussions \\nby offering a comparative analysis of global AI policy frameworks, extending \\nthe dialog on how ethical AI can be systematically developed and maintained \\nacross diverse geopolitical contexts.\\nWhile this study references key AI policy frameworks from the European \\nUnion, the United States, and China, the focus of this study is a more granular \\nexamination of the challenges in comparing such diverse frameworks is \\ncrucial. AI ethics policies are deeply influenced by cultural, socio-political, \\nand economic contexts, making cross-regional comparisons inherently com\\xad\\nplex. For instance, the European Union’s AI Act places significant emphasis \\non safeguarding individual rights, prioritizing transparency and human over\\xad\\nsight (European Parliament 2023), reflecting the EU’s regulatory ethos aimed \\nat protecting citizens from the potential harms of AI. In contrast, the United \\nStates’ AI governance is more decentralized, with a focus on promoting \\ninnovation and maintaining global technological leadership, as seen in the \\nNational Institute of Standards and Technology (NIST) AI Risk Management \\nFramework (2023a), which advocates for flexible, non-prescriptive guidelines \\nthat encourage industry-led solutions.\\nChina’s AI policy framework, meanwhile, is characterized by its focus on \\nstate security, social harmony, and the integration of AI into national eco\\xad\\nnomic strategies (Roberts et al. 2021). This framework aligns with China’s \\nbroader governmental control over technology, where the state plays a central \\nrole in guiding AI development. These diverging priorities highlight the \\ninherent challenges in creating universal standards for AI ethics. The task of \\ncomparing these frameworks, therefore, requires consideration of their dis\\xad\\ntinct legal, cultural, and economic motivations, as well as the varying levels of \\npublic trust in AI technologies across these regions.\\nThis study addresses these complexities by identifying common ethical \\nprinciples such as fairness, transparency, and privacy, and by analyzing how \\neach region interprets and prioritizes these principles. By employing compara\\xad\\ntive tools such as Venn diagrams and Cartesian graphs, the article visually and \\nanalytically demonstrates the differences, but also the common points in these \\nframeworks. The aim of this study was not to look only for the differences, but \\nto find a solution for global AI governance and to promote the potential for \\nharmonization across jurisdictions.\\nThis paper explores the pressing need for ethical considerations in the \\nrapidly evolving domain of Artificial Intelligence (AI) (Meissner 2020). This \\ne2463722-2\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 3}, page_content='technology has significantly impacted various sectors, including healthcare, \\nfinance, and communication. This study aims to establish a robust ethical \\nframework for AI development by addressing complex issues such as data \\nprivacy, algorithmic transparency, and fairness. Our objectives include analyz\\xad\\ning fundamental ethical principles, comparing international AI policy frame\\xad\\nworks (Helbing et al. 2018), proposing strategies for bias mitigation, and \\ncontributing to academic and practical discussions in AI ethics. This paper \\nis structured to systematically dissect these topics, providing an in-depth \\nexploration of AI ethics and its implications for future AI development and \\ngovernance (de Fine Licht and de Fine Licht 2020).\\nThe Imperative of Ethical Considerations in AI\\nThe advent of Artificial Intelligence (AI) has inaugurated a new epoch in \\ntechnological evolution, profoundly influencing diverse sectors, including \\nhealthcare, finance, transportation, and communication (Hosny et al. 2018; \\nNIST 2023b; Yu, Beam, and Kohane 2018). This unprecedented integration of \\nAI into the societal fabric necessitates the urgent formulation of robust ethical \\nframeworks. These frameworks must address the complexities inherent in AI \\ntechnologies, such as data privacy, algorithmic opacity, equity in decision- \\nmaking, and broader societal impacts.\\nEthical considerations in AI transcend academic discourse, bearing signifi\\xad\\ncant real-world repercussions. Paramount among these are issues related to \\ndata privacy and the need for informed consent, where personal information \\noften powers AI algorithms. Equally critical is the transparency and explic\\xad\\nability of these algorithms, which are essential for sustaining public trust, \\nespecially in high-stakes scenarios like legal adjudication or medical diagnos\\xad\\ntics. Moreover, the challenge of ensuring equity and circumventing ingrained \\nbiases in AI systems is a pivotal ethical imperative, given these systems’ \\npropensity to mirror and perpetuate existing societal disparities.\\nThe need for ethical AI is driven by the imperatives of harm prevention and \\njustice but also by the strategic objective of nurturing sustainable, socially \\nbeneficial, and universally accepted innovation.\\nAims and Objectives of the Study\\nThe primary goals of this academic study are threefold. First, it seeks to \\nexplore the ethical considerations inherent in the development of AI. This \\ninvolves thoroughly examining the fundamental ethical principles of transpar\\xad\\nency, equity, and privacy within AI systems and understanding how they relate \\nto each other and their significance in isolation. Second, the research aims to \\ncritically analyze various global AI policy frameworks, focusing on those from \\nthe EU, the US, and China. The goal is to discern their similarities and \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-3'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 4}, page_content='differences and what they mean for international AI governance. Third, the \\npaper intends to provide a synthesis of approaches and practices to recognize \\nand mitigate bias in AI systems, ensuring their fairness and dependability. This \\nstudy aims to contribute to the ongoing academic and practitioner dialog on \\nAI ethics by offering relevant insights and recommendations to both groups. \\nThe overarching goal is to promote a more ethical and responsible path for AI \\ndevelopment.\\nStructure and Content of the Paper\\nThe paper has been methodically structured to explore AI ethics across various \\ndimensions systematically. Section 2, expands into the foundational ethical \\nprinciples in AI: transparency, equity, and privacy. The section uses a Venn \\ndiagram to demonstrate the interaction between these principles, emphasizing \\ntheir interconnectedness and how they relate to AI.\\nMoving on to section 3, the focus is on integrating global AI policy frame\\xad\\nworks within AI development and deployment processes. The section presents \\na flowchart outlining the critical stages in AI projects and the influence of \\nvarious international frameworks. This section examines the role of policies in \\nensuring responsible AI development and the importance of incorporating \\ninternational frameworks to achieve this goal.\\nSection 4, provides a comparative analysis of AI Ethics Policy Frameworks \\nfrom different nations, using a Cartesian graph for evaluation based on \\ntransparency, accountability, equity, and privacy. This seciton highlights the \\nvarying approaches different countries take to AI ethics policies and how they \\ncompare.\\nSection 5, proposes a range of strategies for addressing and reducing bias in \\nAI systems. It emphasizes the significance of data diversity, rigorous audits, \\nethical training, and algorithmic clarity in reducing bias in AI systems.\\nFinally, the paper concludes with section 6, which summarizes the key \\nfindings, discusses their implications for the future trajectory of AI develop\\xad\\nment, and suggests avenues for further scholarly inquiry. The paper provides \\na comprehensive, multifaceted examination of AI ethics through this struc\\xad\\ntured approach, contributing substantive insights to the ongoing scholarly \\ndialog in this critically pivotal domain.\\nEthical Considerations in AI Development\\nEthical considerations are of the utmost importance in the development of AI \\nto ensure that these systems are safe, fair, and transparent. A Venn diagram \\nillustrates the interplay between three key aspects: transparency, fairness, and \\nprivacy.\\ne2463722-4\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 5}, page_content='Transparency, Explainability, and Clarity refer to making AI systems clear \\nand understandable to users. Fairnessrequires that AI systems be designed \\nequitably and without biases. Privacy, or “Data Protection and Consent,” \\nfocuses on protecting personal data and obtaining informed consent for its \\nusage.\\nThese aspects are interconnected, and the intersections of the Venn diagram \\nillustrate how they overlap. For example, the intersection of transparency and \\nfairness is called accountability, which emphasizes the importance of trans\\xad\\nparent and fair AI decision-making. The intersection of transparency and \\nprivacy, called user trust, emphasizes the need for transparency in data use \\nand protection (Aldoseri, Al-Khalifa, and Hamouda 2023; Bécue, Praça, and \\nGama 2021; Malhotra 2018; Mijwil, Aljanabi, and ChatGPT 2023). The inter\\xad\\nsection of fairness and privacy, known as nondiscriminatory data practices, \\nhighlights the need for privacy considerations to align with fairness to avoid \\ndiscrimination.\\nThe center intersection of the Venn diagram represents the ideal of respon\\xad\\nsible AI use that balances transparency, fairness, and privacy. A flowchart \\nprovides a clear, step-by-step guide to embed ethical considerations into AI \\ndevelopment.\\nThe process starts with a commitment to ethical AI development and \\ndefining ethical principles such as transparency, fairness, and privacy. Data \\nuse and AI training guidelines should then be implemented to adhere to these \\nprinciples. Regular audits should be conducted to identify and correct biases \\nand ensure compliance with ethical standards.\\nClear lines of responsibility and accountability should be established in AI- \\ndriven decisions. Continuous improvement based on feedback from users and \\nstakeholders should be a regular practice. The goal is the realization of AI \\nsystems that fully embody ethical principles and ensure the safety and well- \\nbeing of all users.\\nThe framework from Figure 1 is discussed in more detail in the next section.\\nTransparency, Explainability, and Clarity\\nIn developing artificial intelligence, it is essential to prioritize transparency, \\nexplainability, and clarity to ensure ethical development and deployment. \\nTransparency refers to the accessibility of AI systems and their workings to \\nusers and stakeholders. Explainability, closely linked to transparency, pertains \\nto the ability of AI systems to be understood and interpreted by human beings, \\nideally in non-technical language. Meanwhile, clarity ensures that AI systems’ \\npurposes and outcomes are communicated in a straightforward and under\\xad\\nstandable manner.\\nThe importance of these elements cannot be overstated. They are crucial in \\nbuilding and maintaining user trust, ensuring that AI systems operate \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-5'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 6}, page_content='Figure 1. Transparency, Explainability, and Clarity: A framework for developing AI systems that \\nembody ethical principles and ensure the safety and well-being of all users.\\ne2463722-6\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 7}, page_content='understandably and predictably. Furthermore, transparency and explainability \\nplay a pivotal role in establishing accountability, ensuring that AI developers \\nand users are held responsible for the outcomes of AI systems (European \\nParliament 2023; ISO 2023; McCorduck and Cfe 2004; MeitY 2023; NIST  \\n2023b; Office for Artificial Intelligence 2023).\\nFairness and Bias Prevention\\nEnsuring fairness in AI systems requires creating programs that make deci\\xad\\nsions without prejudice or partiality (Bender et al. 2021). This necessitates \\na conscientious effort to design AI systems that do not perpetuate existing \\nbiases or create new ones (Shu, Zhang, and Yu 2021). However, achieving \\nfairness in AI poses significant challenges, as these systems often learn from \\nreal-world data, which can be inherently biased.\\nThe intersection of fairness, privacy, and accountability is a complex but \\nessential consideration. Ensuring fairness often involves careful handling of \\nsensitive data while also maintaining transparency and accountability in \\ndecision-making processes. This balancing act is critical in mitigating biases \\nand ensuring that AI systems are equitable and just.\\nPrivacy and Data Protection\\nPrivacy and data protection are critical ethical considerations that must be \\nconsidered during AI development. It involves protecting personal and sensi\\xad\\ntive information from unauthorized access and ensuring that data is used \\nresponsibly. Regulations and standards, such as the General Data Protection \\nRegulation (GDPR) (GDPR 2018; ICO 2018) in the European Union, play \\na significant role in shaping AI ethics by setting strict guidelines for data use. \\nPrivacy, fairness, and user trust are closely linked. Protecting privacy is crucial \\nin building and maintaining user trust, which is essential for the acceptance \\nand success of AI systems. Furthermore, ensuring the proper handling of data \\nis vital for fairness, as data misuse can lead to biased outcomes.\\nInterconnectedness of Ethical Aspects\\nThe ethical dimensions of AI, including transparency, fairness, and privacy, \\nare not isolated but deeply interconnected (Partnership on AI 2023; Roberts \\net al. 2021). We can visualize this interconnectedness using a Venn diagram \\nthat shows how these aspects overlap and influence each other. For instance, \\nwhen transparency and fairness intersect, it leads to accountability. Similarly, \\nwhen fairness and privacy overlap, it underscores the need for nondiscrimi\\xad\\nnatory data practices. The idea of responsible AI use is represented by the \\ncentral intersection of these aspects in the Venn diagram. This is where all \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-7'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 8}, page_content='three principles are balanced, leading to AI systems that are ethical, reliable, \\nand trustworthy. We can visualize this in Figure 2.\\nImplementation Strategies\\nIncorporating ethical considerations into the development of AI requires \\na systematic approach. A step-by-step guide for doing this involves first \\ncommitting to ethical principles. Next, data use and AI training guidelines \\nshould be implemented to align with these principles. Regular audits are \\nnecessary to detect and correct biases to ensure compliance with ethical \\nstandards.\\nEstablishing clear lines of responsibility and accountability in AI-driven \\ndecisions is also crucial. Continuous improvement, based on feedback from \\nFigure 2. Interconnected concepts of the Framework for developing AI systems that embody \\nTransparency, Explainability and Clarity.\\ne2463722-8\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 9}, page_content='users and stakeholders, should be an integral part of AI development. The \\nultimate goal is to create AI systems that embody ethical principles, ensuring \\nall users’ safety and well-being.\\nResponsible AI Frameworks\\nThis section will explore AI frameworks from the European Union, the \\nUnited States, and China. Using a detailed flowchart, we will examine how \\nthese frameworks impact each stage of an AI project, from initiation to \\npost-deployment. Additionally, we will use a Venn diagram analysis to \\ncompare the EU AI Act (Bommasani et al. 2023), US AI Principles \\n(Tabassi 2023), and China AI Ethics guidelines (Roberts et al. 2021), high\\xad\\nlighting their unique features and areas of overlap. This approach will \\ndemonstrate how these frameworks share a commitment to ethical stan\\xad\\ndards, privacy protection, and fairness while also providing distinct per\\xad\\nspectives on AI development and governance. This analysis is crucial for \\nunderstanding the multifaceted nature of global AI frameworks and their \\nimplications for responsible AI practices.\\nWhile this paper primarily focuses on theoretical frameworks and policy \\nanalysis, it is important to acknowledge the growing need for empirical \\nresearch to substantiate the claims made within the scope of AI ethics. In \\nresponse, we introduce two key case studies that provide concrete examples of \\nhow AI ethics frameworks are applied in practice. These case studies were \\nderived from in-depth analyses of recent AI implementations in both the \\nFigure 3. Key elements found in global AI frameworks.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-9'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 10}, page_content='healthcare and financial sectors, which serve as critical areas where ethical \\nconsiderations are paramount.\\nThe first case study examines the deployment of AI diagnostic systems in \\nthe European healthcare industry, particularly focusing on IBM Watson \\nHealth’s use of AI in oncology diagnostics. By conducting structured inter\\xad\\nviews with healthcare practitioners and reviewing compliance reports, we \\nobserved how the stringent transparency and accountability requirements \\nmandated by the European Union’s AI Act (European Parliament 2023) \\ninfluenced the AI system’s design and operational transparency. This empiri\\xad\\ncal evidence demonstrates how AI systems were modified to meet regulatory \\nstandards, particularly concerning the explainability of diagnostic recommen\\xad\\ndations provided to medical professionals and patients.\\nThe second case study involves an empirical assessment of AI fraud \\ndetection systems in the financial services sector within the United States. \\nThrough direct engagement with industry experts and an analysis of \\nFigure 4. Applied design of responsible and ethical AI practices by integrating global AI frame\\xad\\nworks into different stages of AI development and deployment.\\ne2463722-10\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 11}, page_content='internal auditing processes, we explored how JP Morgan’s AI-powered \\nfraud detection system aligns with the flexible, innovation-driven guidelines \\nof the NIST AI Risk Management Framework (NIST 2023a; Tabassi 2023). \\nThe study reveals how these frameworks permit adaptive risk management \\nstrategies, providing companies with the autonomy to tailor their ethical \\nstandards while maintaining a balance between innovation and \\naccountability.\\nThese case studies provide empirical evidence to support the theoretical and \\npolicy analysis discussed in this paper. They illustrate how global AI ethics \\nframeworks are not just abstract concepts but operational guidelines that have \\ntangible impacts on AI design, deployment, and compliance. By incorporating \\nthese real-world applications, we aim to bridge the gap between theory and \\npractice, offering a more robust foundation for understanding the dynamics of \\nAI governance across various industries.\\nFigure 5. Different global AI frameworks overlap in some areas but maintain unique characteristics \\nin others.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-11'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 12}, page_content='Development and Deployment Flowchart\\nCreating and implementing AI systems is a complex process that requires \\ncompliance with international frameworks to ensure ethical and responsible \\noutcomes. This section provides a comprehensive flowchart that integrates AI \\nframeworks from the European Union, the United States, and China, mapping \\ntheir application throughout the AI project lifecycle.\\nThe flowchart is based on the EU AI Act (European Parliament 2023), US \\nAI Principles (NIST 2024c), and China AI Ethics guidelines (Provisions on the \\nFigure 6. The complexity of AI frameworks across different regions and the potential for collabora\\xad\\ntion and divergence highlights the need for global collaboration and harmonisation of AI ethics \\nand regulations.\\ne2463722-12\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 13}, page_content='Administration of Deep Synthesis Internet Information Services, Personal \\nInformation Protection Law of the People’s Republic of China PRC 2022). It \\nbegins with initiating an AI project, where objectives are defined and relevant \\ndata is collected. The EU AI Act’s guidelines on ethical data use and transpar\\xad\\nency are essential at this stage. The US AI Guidelines come into play as the \\nproject progresses to data processing and AI model development, emphasizing \\ninnovation, fairness, and accountability.\\nDuring the validation and testing phase, the model must align with the set \\nobjectives and comply with ethical standards per these frameworks. The \\ndeployment phase sees the integration of China’s AI Ethics guidelines, which \\nprioritize social harmony, national security, and global cooperation. After \\ndeployment, continuous monitoring and maintenance are essential to ensure \\nthe AI system functions as intended and adheres to ethical standards. This \\nphase is critical for incorporating feedback and insights, allowing for iterative \\nimprovements based on real-world performance and impact.\\nFigure 3 illustrates how global AI frameworks are necessary and applicable \\nat different AI development and deployment stages. This integration ensures \\na holistic approach to responsible AI practices that align with global standards \\nand ethical considerations.\\nFigure 3 shows the steps involved in developing and deploying AI systems. \\nIt incorporates the latest AI frameworks worldwide, including those from the \\nEU, the US (NAIAC 2024, NIST 2024a), and China (Interim Measures for the \\nManagement of Generative Artificial Intelligence Services, Personal \\nInformation Protection Law of the People’s Republic of China PRC 2023; Li  \\n2017; Provisions on the Administration of Deep Synthesis Internet \\nInformation Services, Personal Information Protection Law of the People’s \\nRepublic of China PRC 2022; The State Council People Republic of China  \\n2017), and highlights critical points where these frameworks intersect. The \\nprocess flow begins with the start of the AI project and moves on to identifying \\nobjectives and collecting relevant data. The EU AI Act’s guidelines may be \\nconsidered at this stage. The collected data is then processed, and the AI model \\nis developed according to the principles outlined in the US AI Guidelines. The \\nmodel is then validated and tested to meet the set objectives. Deployment of \\nthe AI system in a real-world environment is the next step, and considerations \\nfrom China’s AI Ethics guidelines come into play here. Continuously mon\\xad\\nitoring and maintaining the AI system post-deployment is essential. The \\nflowchart also includes a feedback loop that involves revisiting objectives \\nand processes based on feedback and new insights. Once all the steps are \\ncompleted, the AI project cycle ends. The flowchart in Figure 4 ensures \\nresponsible and ethical AI practices by integrating global AI frameworks \\ninto different stages of AI development and deployment.\\nFigure 4 visualizes how various AI frameworks integrate with the AI devel\\xad\\nopment and deployment process. Each stage of the AI process is marked, from \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-13'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 14}, page_content='the beginning to the end. The EU, the US, and China AI frameworks are \\nhighlighted with individual annotations. The arrows linking these frameworks \\nto specific stages in the process are designed to avoid text overlap, ensuring \\nclarity and readability.\\nThe flowchart effectively illustrates the integration of global AI frame\\xad\\nworks into the AI development lifecycle, emphasizing the importance of \\nconsidering these guidelines at different stages for responsible AI practices. \\nThe annotations for the AI frameworks from the EU, US, and China are \\nstrategically positioned to avoid obstructing any other flowchart elements. \\nThe connecting arrows are designed with a specific arc to neatly link the \\nframeworks to their respective stages in the AI process without crossing \\nover any text.\\nThe Venn diagram represents the AI frameworks from the EU, the US, and \\nChina. Each circle in the diagram represents a different region’s AI framework: \\nEU AI Act, US AI Principles, and China AI Ethics. The overlaps between the \\ncircles indicate areas of common focus or principles shared between these \\nframeworks. Individual sections highlight unique aspects of each framework.\\nThis Venn diagram in Figure 5 visually demonstrates how different global \\nAI frameworks overlap in some areas while maintaining unique characteristics \\nin others. It reflects the diverse approaches to AI governance and ethics across \\nthese regions.\\nThe Venn diagram in Figure 5 comprehensively analyses the AI frameworks \\nfrom the European Union, the United States, and China. It highlights the areas \\nof collaboration and divergence between the three regions and demonstrates \\nthe complexity of AI frameworks across different regions.\\nThe European Union’s AI Act focuses on human oversight, nondiscrimina\\xad\\ntion, and regulatory compliance. The US AI Principles emphasize innovation \\nencouragement, public trust, and open collaboration. China’s AI Ethics prior\\xad\\nitizes social harmony, national security, and global cooperation. These unique \\nelements reflect the individual priorities and cultural perspectives of each \\nregion.\\nThe EU and the US share values in transparency and ethical standards. The \\nEU and China both emphasize privacy protection and ethical standards. The \\nUS and China find common ground in ethical standards and a focus on \\nfairness. These common areas between the two frameworks indicate the \\npotential for global collaboration and harmonization of AI ethics and \\nregulations.\\nThe central overlap in the Venn diagram highlights the areas where the EU, \\nUS, and China share common principles such as ethical standards and privacy \\nprotection. These areas offer opportunities for global collaboration and har\\xad\\nmonization of AI ethics and regulations.\\nEach region has unique focus areas that reflect its priorities and cultural \\nperspectives. These divergent approaches could lead to different approaches in \\ne2463722-14\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 15}, page_content='AI development and governance. The Venn diagram demonstrates the poten\\xad\\ntial for collaboration and divergence in the global AI landscape.\\nThe Venn diagram analysis in Figure 6 shows the complexity of AI frame\\xad\\nworks across different regions and the potential for collaboration and diver\\xad\\ngence. It highlights the need for global collaboration and harmonization of AI \\nethics and regulations to ensure that AI development and governance align \\nwith ethical and societal values. \\nThe Venn diagram in Figure 6 shows the EU AI Act i in light blue, the US AI \\nPrinciples are in light green, and the China AI Ethics framework in coral.\\nReal-World Applications of Global AI Frameworks\\nIn order to provide a clearer understanding of how AI frameworks function in \\npractice, it is essential to examine real-world applications of these frameworks \\nacross different regions. A relevant example can be found in the application of \\nthe European Union’s AI Act within the healthcare sector. The use of AI- \\npowered diagnostic tools, such as IBM’s Watson Health, was subject to \\nscrutiny under the EU’s stringent regulations on transparency and explain\\xad\\nability. Under the AI Act, companies deploying such AI systems in high-risk \\nsectors are required to provide detailed documentation of the algorithms used, \\nas well as explainability mechanisms that allow medical professionals and \\npatients to understand AI-driven decisions. This regulatory requirement has \\nled to the modification of AI models to ensure compliance, particularly by \\nproviding more transparent decision-making processes that can be audited by \\nhealthcare regulators.\\nIn contrast, the United States’ more innovation-centric approach, as embo\\xad\\ndied by the NIST AI Risk Management Framework, can be observed in the \\ndeployment of AI in the financial services sector. For instance, JP Morgan’s \\nAI-powered fraud detection system operates within a framework that empha\\xad\\nsizes risk mitigation through best practices and industry standards rather than \\nrigid regulatory oversight. The NIST framework encourages companies to \\ndevelop internal policies tailored to their operational risks, allowing for greater \\nflexibility in AI implementation. As a result, JP Morgan has developed pro\\xad\\nprietary methods for continuous monitoring and auditing of AI models to \\nensure they remain effective while balancing the need for innovation with \\nethical considerations.\\nChina’s AI governance, which prioritizes state control and societal \\nharmony, can be seen in the government’s use of facial recognition \\nsystems for public security. The deployment of such systems, governed \\nby China’s Provisions on the Administration of Deep Synthesis Internet \\nInformation Services, Personal Information Protection Law of the \\nPeople’s Republic of China (PRC) (2022), illustrates how the state \\nleverages AI under a framework that prioritizes national security. In this \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-15'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 16}, page_content='case, AI technologies are used to monitor public spaces, but their ethical \\nimplications, particularly in terms of privacy, are handled within \\na governance model that differs significantly from those in Western \\ndemocracies. The Chinese government’s emphasis on AI as a tool for \\nsocietal stability underscores the unique application of their framework in \\npractice.\\nThese examples highlight the varied approaches of AI frameworks across \\ndifferent regions and sectors, demonstrating how global AI policies are shaped \\nby contextual factors and applied in practical, high-impact scenarios. By \\nexamining such real-world cases, we can better understand the strengths and \\nlimitations of these frameworks and the challenges in harmonizing AI ethics \\non a global scale.\\nSpecific Recommendations for Real-World Applications of Global AI Frameworks \\nInclude\\nTechnical Solutions for Embedding Ethical Principles in AI Systems. Embedding \\nethical principles such as fairness, transparency, and accountability into AI \\nsystems requires sophisticated algorithmic approaches that ensure these objec\\xad\\ntives are met without compromising the system’s performance. Algorithmic \\nfairness can be addressed using methods such as differential fairness and fair \\nrepresentation learning. For instance, algorithms like the Fair Representation \\nLearning (FRL) model aim to mitigate bias by transforming raw data into \\na latent representation that is invariant to sensitive attributes, such as race or \\ngender, without losing important predictive power. The FRL method applies \\nadversarial learning to ensure the model cannot easily infer sensitive attri\\xad\\nbutes, thus reducing bias while maintaining accuracy. This can be particularly \\nuseful in sectors like finance, where historical biases in credit scoring datasets \\noften lead to unfair outcomes. Incorporating these fairness constraints during \\nthe model training phase ensures that discriminatory patterns in the data are \\nnot propagated by the AI system.\\nTransparency is enhanced through the use of explainable AI (XAI) tech\\xad\\nniques. One common approach is the implementation of Local Interpretable \\nModel-agnostic Explanations (LIME), which provides users with interpreta\\xad\\nble approximations of complex models, enabling end-users and auditors to \\nunderstand and evaluate individual predictions. LIME works by perturbing \\ninput data and observing how changes impact predictions, thus constructing \\nsimpler, interpretable models locally around specific instances. This method is \\nparticularly valuable in high-stakes fields like healthcare, where understanding \\nthe rationale behind AI-driven diagnoses is critical for building trust and \\naccountability. For example, LIME has been effectively applied in medical \\nimaging to explain how AI systems identify tumor regions, offering transpar\\xad\\nency to both clinicians and patients.\\ne2463722-16\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 17}, page_content='Obtaining Real-World Probabilistic Data for Legislation. To create more robust \\nAI legislation that addresses real-world challenges, probabilistic data collec\\xad\\ntion is necessary. A critical solution lies in data-driven simulations that use \\nreal-world probabilistic distributions of AI outcomes across various domains. \\nThese simulations can leverage Bayesian inference models to analyze the \\nprobability of ethical failures, such as biased decisions or transparency \\nbreaches, under different regulatory scenarios. For example, Bayesian models \\ncan assess the likelihood of biased outputs in loan approval systems based on \\nvarying regulatory constraints, allowing policymakers to quantitatively evalu\\xad\\nate the trade-offs between stringent regulation and innovation. By incorporat\\xad\\ning such probabilistic assessments, policymakers can develop legislation \\ngrounded in empirical evidence, ensuring that ethical guidelines are both \\npractical and enforceable in diverse sectors.\\nMoreover, quantitative data collection from real-world AI deployments \\ncould utilize techniques such as differential privacy to protect sensitive \\ninformation while still gathering meaningful insights. For instance, in health\\xad\\ncare, collecting large-scale patient data from AI diagnostic tools while main\\xad\\ntaining patient privacy can be achieved through differential privacy algorithms \\nthat introduce noise into datasets, ensuring that individual records cannot be \\nre-identified. This allows regulators to gather accurate statistics on AI system \\nperformance, such as prediction accuracy and error rates, without violating \\nprivacy laws. These real-world data points can then be used to fine-tune \\nlegislative frameworks to ensure they are reflective of practical AI use and \\ncompliant with privacy standards.\\nAlgorithmic Solutions to Ensure Fair and Ethical AI for End-Users. To ensure \\nAI systems are perceived as fair and ethical by end-users, several algorithmic \\napproaches can be integrated into the development lifecycle. One promising \\nmethod is the use of fairness constraints in model optimization, such as \\nEqualised Odds and Demographic Parity. The Equalised Odds algorithm \\nensures that an AI system has equal true positive and false positive rates across \\ndifferent demographic groups, ensuring that no group disproportionately \\nbenefits or suffers from the system’s decisions. This technique has been \\nsuccessfully implemented in judicial systems where AI models are used for \\nbail and sentencing recommendations, reducing the racial disparities com\\xad\\nmonly observed in earlier models.\\nFairness-aware learning algorithms can also be embedded into machine \\nlearning pipelines to monitor and adjust for bias during the training process. \\nFor example, the Fairness through Awareness (FTA) framework adjusts \\ndecision boundaries within models to ensure that similar individuals are \\ntreated similarly, thereby reducing unfair bias. This algorithm calculates dis\\xad\\ntances in a fairness-sensitive space and ensures that individuals who are close \\nin this space receive similar predictions. This has been applied in hiring \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-17'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 18}, page_content='algorithms to ensure that applicants with similar qualifications, regardless of \\ndemographic attributes, are treated equitably.\\nFurthermore, end-user engagement with AI systems can be improved \\nthrough interactive transparency mechanisms. For instance, counterfactual \\nexplanations can be used to provide users with actionable insights into how \\ndecisions could change if certain inputs were modified. In credit scoring \\nsystems, for example, a counterfactual explanation might inform a user that \\ntheir loan was denied due to a low credit score and suggest specific steps, such \\nas reducing credit card debt, that would lead to approval. By providing users \\nwith clear, actionable insights, these systems not only increase trust but also \\nempower users to engage more meaningfully with AI-driven decisions.\\nAlgorithmic Accountability and Continuous Monitoring. To maintain ongoing \\nfairness and ethical standards, continuous monitoring of AI systems is essen\\xad\\ntial. This can be achieved through algorithmic auditing frameworks that \\nregularly assess AI systems for adherence to ethical principles post- \\ndeployment. Post-hoc fairness auditing tools, such as AI Fairness 360 \\n(AIF360), provide an open-source toolkit that measures and mitigates bias \\nin deployed models. These tools can be integrated into AI governance pro\\xad\\ncesses, ensuring that models remain fair and unbiased as they encounter new \\ndata in real-world environments. AIF360 evaluates fairness through multiple \\nmetrics, such as disparate impact and statistical parity, and enables continuous \\nrecalibration of models to maintain ethical performance.\\nIncorporating algorithmic accountability systems with real-time feedback \\nloops ensures that biases introduced by shifts in data distributions (data drift) \\nare swiftly detected and mitigated. Techniques such as drift detection algo\\xad\\nrithms, including ADWIN (Adaptive Windowing), continuously monitor the \\nperformance of AI models and trigger retraining when significant deviations \\nfrom expected behavior are detected. By automating the detection of ethical \\nbreaches and recalibrating models in response, these systems ensure that AI \\nremains both effective and ethically compliant over time.\\nComparative Venn Diagram Analysis\\nThis section uses a Venn diagram analysis to compare the AI frameworks of \\nthe European Union (EU), the United States (US), and China. The diagram \\nrepresents each framework, highlighting their unique features and areas of \\noverlap, revealing both collaborative potentials and divergent approaches.\\nThe EU’s AI Act prioritizes human oversight, nondiscrimination, and strict \\nregulatory compliance. It reflects the EU’s emphasis on protecting citizens’ \\nrights in the digital age. The US AI Principles prioritize fostering innovation, \\nensuring public trust, and promoting open collaboration. This mirrors the \\nUS’s emphasis on market-driven and innovation-led AI development. On the \\ne2463722-18\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 19}, page_content='other hand, China’s AI Ethics framework emphasizes the importance of social \\nharmony, national security, and global cooperation. It reflects China’s \\napproach to balancing technological advancement with social stability and \\nstate security.\\nThe intersections of these frameworks in the Venn diagram highlight \\nshared principles and potential areas for international cooperation. For exam\\xad\\nple, the EU and the US emphasize ethical standards and transparency. The EU \\nand China share a common focus on privacy protection and the ethical use of \\nAI. The US and China converge on encouraging ethical standards and fairness \\nin AI.\\nAt the central intersection of the Venn diagram, where all three frame\\xad\\nworks overlap, lies a shared commitment to ethical standards, privacy \\nprotection, and ensuring fairness. This common ground suggests opportu\\xad\\nnities for global collaboration and the harmonization of AI ethics and \\nregulations.\\nHowever, each framework’s divergent aspects reflect each region’s \\nvarying priorities and cultural perspectives. These differences could \\nlead to distinct approaches in AI development and governance globally. \\nTherefore, the Venn diagram highlights the potential for collaboration \\nand underscores the need to understand and respect diverse perspectives \\nin the global AI landscape.\\nPolicy Frameworks\\nThis section expands into a thorough analysis of AI Ethics Policy \\nFrameworks worldwide. It covers significant regions such as the European \\nUnion, the United States, China, Canada, Japan, India, and Australia. \\nFigure 7 presents the data in Cartesian graphs, comparing these frame\\xad\\nworks across four key ethical dimensions: Transparency, Accountability, \\nFairness, and Privacy. Each framework is evaluated in detail and rated, \\nproviding an insightful understanding of how countries prioritize these \\ndimensions in their AI policies. This graph highlights each framework’s \\nunique priorities and focus areas and emphasizes the diversity and com\\xad\\nmonalities in global approaches to AI ethics. Policymakers can leverage \\nthese insights to identify areas for improvement and develop comprehen\\xad\\nsive, ethically aligned AI policies. Moreover, the section expands upon this \\nanalysis to include additional critical dimensions such as human oversight \\nand national security, broadening the scope to encompass broader socio- \\npolitical implications of AI technology. Figure 8 analyses global trends and \\npresents a roadmap for harmonization in AI ethics, advocating for ongoing \\ninternational dialogue and cooperation to foster a responsible and ethical \\nglobal AI ecosystem.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-19'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 20}, page_content='Criteria for Rating AI Ethics Policy Frameworks Across Countries\\nThe ratings for each country’s AI ethics policy framework were based on \\na detailed analysis of public documents, policy white papers, regulatory guide\\xad\\nlines, and academic literature. The scores for each dimension, transparency, \\naccountability, fairness, privacy, human oversight, ethical standards, \\nFigure 7. Cartesian graph - Comparison of Al Ethics Policy Frameworks Across Countries.\\nFigure 8. Cartesian graph of Global AI Ethics Policy Frameworks: a tool for policymakers to \\nunderstand the priorities of different countries regarding AI ethics.\\ne2463722-20\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 21}, page_content='innovation encouragement, and national security, were evaluated according to \\nthe following specific criteria:\\nTransparency (1–10 Scale)\\nTransparency was assessed by the extent to which each country’s framework \\nmandates openness regarding the design, implementation, and decision- \\nmaking processes of AI systems.\\n●9-10: Countries with explicit, legally enforced transparency requirements \\nfor AI systems, including mandates for explainability and public account\\xad\\nability (e.g., the European Union).\\n●6-8: Countries that encourage transparency but do not mandate it as \\na legal requirement across all AI applications (e.g., the United States).\\n●4-5: Countries where transparency is mentioned in policies but with few \\npractical enforcement mechanisms (e.g., China).\\n●1-3: Minimal or no formal focus on transparency in the AI framework.\\nAccountability (1–10 Scale)\\nAccountability measures the robustness of legal and regulatory mechanisms \\nthat hold developers, companies, and governments responsible for the out\\xad\\ncomes of AI systems.\\n●9-10: Countries with well-defined liability frameworks that assign clear \\nresponsibility to AI developers or operators (e.g., the EU AI Act).\\n●6-8: Countries where accountability is encouraged through voluntary \\ncompliance frameworks but lacks mandatory enforcement (e.g., the US \\nwith the NIST AI Risk Management Framework).\\n●4-5: Countries with vague accountability measures, often handled at the \\ndiscretion of private entities or lacking centralized regulation (e.g., Japan).\\n●1-3: Countries where accountability frameworks are non-existent or still \\nin early development phases.\\nFairness (1–10 Scale)\\nFairness was evaluated based on how well a country’s policy framework \\naddresses bias in AI algorithms and ensures equitable outcomes across demo\\xad\\ngraphic groups.\\n●9-10: Countries with explicit fairness requirements for AI systems, man\\xad\\ndating fairness audits and bias mitigation techniques (e.g., Canada, EU).\\n●6-8: Countries that encourage fairness but with less stringent or optional \\nauditing practices (e.g., the US).\\n●4-5: Countries where fairness is an aspirational goal with limited practical \\nimplementation (e.g., India).\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-21'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 22}, page_content='●1-3: Minimal or no focus on fairness in AI regulation.\\nPrivacy (1–10 Scale)\\nPrivacy was assessed by how each country’s framework protects user \\ndata in the context of AI and how it aligns with global standards like \\nthe GDPR.\\n●9-10: Countries with robust privacy regulations, including explicit rules \\non AI data use (e.g., EU, Canada).\\n●6-8: Countries with general data privacy laws but limited AI-specific \\nguidelines (e.g., the US, Japan).\\n●4-5: Countries with privacy regulations that are inconsistently applied or \\nunderdeveloped in relation to AI (e.g., India, Australia).\\n●1-3: Countries with minimal focus on privacy in AI contexts, or where \\ndata protection laws are not enforced effectively (e.g., China).\\nHuman Oversight (1–10 Scale)\\nThis criterion assessed the role of human oversight in AI decision- \\nmaking, particularly in high-risk sectors like healthcare or autonomous \\nvehicles.\\n●9-10: Countries mandating human oversight in high-risk AI decisions, \\nensuring human intervention in critical areas (e.g., EU AI Act).\\n●6-8: Countries that recommend but do not legally enforce human over\\xad\\nsight (e.g., the US).\\n●4-5: Countries where human oversight is mentioned, but enforcement \\nmechanisms are vague or absent (e.g., Japan, India).\\n●1-3: Little to no emphasis on human oversight in AI policy frameworks.\\nEthical Standards (1–10 Scale)\\nEthical standards were scored based on how well a country’s AI policies adhere \\nto global ethical frameworks (such as UNESCO’s AI ethics recommendations) \\nand promote ethical AI development.\\n●9-10: Countries with a clearly defined, internationally aligned ethical \\nframework for AI (e.g., the EU, Canada).\\n●6-8: Countries with ethical guidelines for AI but limited in scope or \\nenforcement (e.g., Japan, the US).\\n●4-5: Countries that mention ethical AI but lack a coherent, enforceable \\nframework (e.g., India).\\n●1-3: Minimal or no formal focus on AI ethics in public policy.\\ne2463722-22\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 23}, page_content='Innovation Encouragement (1–10 Scale)\\nThis criterion measured the balance between promoting AI innovation and \\nenforcing ethical guidelines. Countries that foster innovation while maintain\\xad\\ning a robust ethical framework scored higher.\\n●9-10: Countries with innovation-centric policies that support AI research \\nand development while integrating ethical guidelines (e.g., US, Canada).\\n●6-8: Countries with strong innovation policies but less rigorous ethical \\nenforcement (e.g., Japan).\\n●4-5: Countries where innovation is promoted but at the cost of ethical \\nstandards (e.g., China).\\n●1-3: Countries where innovation in AI is stifled due to excessive regula\\xad\\ntion or a lack of resources (e.g., minimal focus).\\nNational Security (1–10 Scale)\\nNational security was evaluated based on how countries incorporate AI within \\ntheir national security strategies, including defense, cybersecurity, and \\nsurveillance.\\n●9-10: Countries where AI plays a significant role in national security \\nframeworks, with clear policies on military AI, surveillance, and cyber \\ndefense (e.g., China, the US).\\n●6-8: Countries that include AI in national security policies but with fewer \\nexplicit regulations on its use in defense (e.g., Australia, Japan).\\n●4-5: Countries with some mention of AI in national security contexts but \\nlacking concrete policies (e.g., India).\\n●1-3: Minimal focus on AI for national security, or policies that are still in \\nearly development (e.g., the EU).\\nJustification for the Selection of Criteria and Scores\\nThe chosen criteria for evaluating AI ethics policy frameworks, transparency, \\naccountability, fairness, privacy, human oversight, ethical standards, innova\\xad\\ntion encouragement, and national security, were carefully selected to reflect \\nthe core dimensions that are essential for ethically sound, socially beneficial, \\nand technologically responsible AI systems. These dimensions are well- \\nestablished in policy discourse and academic literature as the pillars of AI \\nethics and governance, ensuring that AI development aligns with societal \\nvalues and mitigates potential harms.\\nTransparency\\nTransparency is a cornerstone of AI ethics, as highlighted in both academic \\nand regulatory discussions (Floridi et al. 2018). Transparent AI systems allow \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-23'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 24}, page_content='stakeholders to understand how decisions are made and ensure accountability. \\nThe choice of transparency as a criterion is supported by regulatory frame\\xad\\nworks such as the European Union’s GDPR and AI Act, which place explicit \\ndemands on AI systems to be explainable and open to public scrutiny. Studies \\nhave shown that lack of transparency is one of the main causes of public \\ndistrust in AI systems (Wachter, Mittelstadt, and Russell 2023). Therefore, \\ncountries with clear legal mandates for transparency received higher scores, \\nwhile those with voluntary or vague transparency guidelines scored lower.\\nAccountability\\nAccountability ensures that AI developers, operators, and users are held \\nresponsible for the outcomes produced by AI systems. This criterion is \\njustified by the recognition in the literature that without clear accountability \\nstructures, it becomes difficult to address failures or harms caused by AI \\nsystems (Mittelstadt 2019). The EU AI Act introduces comprehensive provi\\xad\\nsions that assign legal responsibility, providing a strong model for account\\xad\\nability. Countries like the United States, with voluntary compliance through \\nframeworks like the NIST AI Risk Management Framework, received inter\\xad\\nmediate scores due to the lack of enforceability. The necessity of accountability \\nis also a major theme in academic literature, particularly in the context of \\ncomplex AI systems where multiple stakeholders are involved in the design \\nand deployment (de Bruin and Floridi 2017; Floridi et al. 2018; Turilli and \\nFloridi 2009).\\nFairness\\nFairness in AI systems addresses concerns about bias and discrimination, \\nwhich are well-documented issues in AI applications (Binns 2018). \\nCountries with explicit fairness requirements in their AI policies, such as the \\nEuropean Union and Canada, received higher scores because their frameworks \\nmandate fairness audits and bias mitigation practices. Literature on fairness in \\nAI often points to the limitations of algorithmic systems to ensure equitable \\noutcomes across demographic groups without specific regulatory intervention \\n(Du 2023; IBM 2018). Nations with minimal or non-enforceable fairness \\nprovisions, such as India and China, scored lower due to the absence of robust \\nbias-mitigation mechanisms.\\nPrivacy\\nPrivacy is a critical concern in AI, especially in systems that rely on vast \\namounts of personal data. The GDPR in the EU sets a high global benchmark \\nfor data protection and privacy, justifying the high score for the EU in this \\ndimension. In contrast, countries like the United States, where privacy regula\\xad\\ntions such as HIPAA are domain-specific and not universally applicable to AI \\nsystems, scored lower (HIPAA 1996). Privacy as a criterion is grounded in the \\ne2463722-24\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 25}, page_content='principle that ethical AI must protect individuals’ rights to control their data, \\na concern that is pervasive in academic and policy literature (Mittelstadt 2019).\\nHuman Oversight\\nHuman oversight in AI decision-making is essential to prevent over-reliance \\non automated systems, particularly in critical sectors like healthcare and law \\nenforcement (European Parliament 2023). The EU AI Act again leads the way \\nby mandating human oversight in high-risk AI applications. The literature \\nemphasizes the importance of preserving human judgment in AI-assisted \\ndecision-making, especially in cases involving moral or legal consequences \\n(Jobin, Ienca, and Vayena 2019). Countries that recommend but do not \\nmandate human oversight scored lower, as voluntary oversight often fails in \\nreal-world applications, particularly where operational efficiency is prioritized \\nover human intervention.\\nEthical Standards\\nEthical standards are increasingly seen as vital for aligning AI development \\nwith human values. UNESCO’s Recommendation on the Ethics of AI and \\nsimilar initiatives by the OECD have provided blueprints for ethical AI, \\nfocusing on principles such as beneficence, non-maleficence, autonomy, and \\njustice (UNESCO 2023). Countries like Canada and the EU, which have \\nadopted comprehensive ethical guidelines, scored highly. These standards \\nare crucial for ensuring that AI operates within moral and legal boundaries. \\nIn contrast, countries that lack specific ethical frameworks for AI, such as \\nIndia and China, received lower scores, reflecting the underdevelopment of \\nethical considerations in their AI policies.\\nInnovation Encouragement\\nThe balance between encouraging AI innovation and enforcing ethical stan\\xad\\ndards is a key concern for policymakers (Brynjolfsson and Mcafee 2014; Evans  \\n2015). Countries like the United States scored high in this dimension due to \\ntheir innovation-centric policies, such as the NIST AI Risk Management \\nFramework, which promotes industry-led solutions and fosters a favorable \\nenvironment for AI research and development. The literature supports the \\nnotion that innovation thrives when there is flexibility and minimal regulatory \\noverhead, but with the caveat that ethical guardrails must not be neglected \\n(Bostrom and Yudkowsky 2014). Countries that focus excessively on regula\\xad\\ntion, potentially stifling innovation, or that lack sufficient incentives for AI \\nresearch, scored lower.\\nNational Security\\nNational security considerations, particularly regarding the development of \\nautonomous weapons systems (AWS) and AI-enhanced cybersecurity, are \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-25'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 26}, page_content='becoming a critical component of AI policy (Singer 2009). Countries like the \\nUS and China, where AI plays a substantial role in national defense strategies, \\nscored highly. The literature on autonomous systems highlights the impor\\xad\\ntance of regulating AI to prevent unintended consequences in military appli\\xad\\ncations (Singer 2009). Countries that have not yet integrated AI into their \\nnational security strategies or have underdeveloped AI governance in this area, \\nsuch as the EU, scored lower.\\nJustification for Scoring. The scores for each dimension were derived from \\na combination of the following sources:\\n●Policy Documents and Regulations: Key regulatory frameworks such as \\nthe EU AI Act (Bommasani et al. 2023; European Parliament 2023, FACT \\nSHEET: Biden-Harris Administration Announces New Actions to \\nPromote Responsible AI Innovation That Protects Americans’ Rights \\nand Safety | The White House, 2023; Mozumder et al. 2022), GDPR \\n(GDPR 2018; ICO 2018), NIST AI Risk Management Framework \\n(NIST 2024a, 2024c; Tabassi 2023), and national AI strategies were \\ndirectly analyzed to assess the strength and comprehensiveness of each \\ncountry’s AI governance mechanisms.\\n●Academic Literature: Foundational texts on AI ethics, fairness, transpar\\xad\\nency, and accountability were referenced to establish baseline expecta\\xad\\ntions for what constitutes best practices in each dimension (Binns 2018; \\nFloridi et al. 2018; Jobin, Ienca, and Vayena 2019).\\n●Real-World Case Studies: Examples of AI implementation in various \\nsectors, including healthcare, finance, and national security, were exam\\xad\\nined to contextualize the practical impacts of each policy framework.\\nThe criteria were selected to ensure a comprehensive assessment of each \\ncountry’s approach to AI ethics, focusing on both regulatory stringency and \\nthe practical application of ethical principles. Each score reflects the extent to \\nwhich the country’s framework addresses key challenges associated with AI \\ngovernance, ensuring a balanced evaluation that is informed by both policy \\nanalysis and academic insights.\\nComparative Analysis Using Cartesian Graphs\\nThis section provides a detailed comparative analysis of AI Ethics Policy \\nFrameworks from a global perspective. The analysis covers major regions \\nsuch as the European Union, the United States, China, Canada, Japan, India, \\nand Australia. The study examines these frameworks against key ethical \\ndimensions, including Transparency, Accountability, Fairness, and Privacy, \\nusing a series of Cartesian graphs.\\ne2463722-26\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 27}, page_content='Each framework is rated on a scale of 1 to 10 across these dimensions. The \\nCartesian graph format helps to visualize how each country’s framework \\nmeasures up in these critical areas. For instance, the EU’s framework prior\\xad\\nitizes privacy and accountability, reflected in its high scores in these areas. On \\nthe other hand, the US framework might score higher on transparency because \\nof its focus on open data and innovation. China’s framework, emphasizing \\nsocial harmony and national security, might have different strengths and \\nweaknesses.\\nThis comparative analysis provides insights into the priorities and focus \\nareas of different countries and highlights the diversity and commonalities in \\napproaches to AI ethics globally. The graphical representation aids in under\\xad\\nstanding the complexities of each framework, offering a clear view of how \\nnations are navigating the ethical landscape of AI development.\\nThe Cartesian graph in Figure 7 compares AI Ethics Policy Frameworks \\nacross countries, such as the EU, the US, China, Canada, Japan, India, and \\nAustralia. It evaluates them on Transparency, Accountability, Fairness, and \\nPrivacy.\\nFigure 7 presents an overview of how countries prioritize various elements \\nof AI ethics in their policy frameworks. The four aspects used for rating are \\ntransparency, accountability, fairness, and privacy. Each aspect is rated on \\na scale from 1 to 10.\\nThe Blue Line represents transparency, which reflects how policies are \\nopenly communicated and implemented. The Green Line indicates account\\xad\\nability, measuring the extent to which AI developers and users are held \\naccountable for their systems as per the frameworks. The Red Line represents \\nfairness, measuring the extent to which the policies ensure fair and unbiased \\nAI systems. Lastly, the Purple Line shows the importance of user privacy and \\ndata protection in the policies.\\nThe graph offers valuable insights into the global landscape of AI ethics. It \\nhighlights similarities and differences in national approaches to regulating AI \\nthat can help inform future policy development. The ratings of each aspect for \\neach country can help policymakers identify areas for improvement in their \\npolicies.\\nThis graph provides a tool for policymakers to understand the priorities of \\ndifferent countries regarding AI ethics. By focusing on transparency, account\\xad\\nability, fairness, and privacy, policymakers can create policies that address the \\nconcerns of stakeholders and help establish ethical guidelines for AI develop\\xad\\nment and use.\\nExtended Analysis and Global Implications\\nIn this section, we expand into an evaluation matrix that incorporates addi\\xad\\ntional dimensions that are increasingly relevant to AI ethics. These dimensions \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-27'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 28}, page_content='include the roles of human oversight and national security, reflecting AI \\ntechnology’s broader socio-political implications.\\nIncluding human oversight highlights the necessity for human intervention \\nand judgment in AI systems, a principle strongly supported by the EU frame\\xad\\nwork. Conversely, national security is crucial in the frameworks of countries \\nlike China and the US, where AI’s involvement in defense and intelligence is \\na pivotal consideration.\\nThis comprehensive analysis emphasizes the global trends in AI policy \\nframeworks and the potential for harmonizing AI ethics. The section \\nexplores the possibility of converging principles and standards despite \\neach region’s diverse cultural, political, and social contexts. While complete \\nuniformity may be unattainable, the potential for international collabora\\xad\\ntion and consensus-building on core principles is significant. Such harmo\\xad\\nnization could facilitate the establishment of universally accepted norms \\nand standards, ensuring that AI development aligns with ethical and socie\\xad\\ntal values.\\nThe analysis of similarities and differences in Figure 8 concludes that we \\nneed continued dialogue and cooperation among nations to cultivate \\na responsible and ethical global AI ecosystem.\\nThe Cartesian graph in Figure 8 includes lines representing concepts \\ninspired by earlier Venn diagrams that explored the issues surrounding AI \\ngovernance.\\nThe lines on the graph represent different aspects of AI ethics that \\nhave been identified as necessary. The blue line represents transparency, \\nwhich refers to the openness and clarity in AI policy communication. \\nThe green line represents accountability, which signifies the extent of \\nresponsibility in AI development and use. The red line represents fair\\xad\\nness, which denotes the importance of ensuring unbiased AI systems. \\nThe purple line represents privacy, which highlights the importance of \\nuser privacy and data protection. The orange line represents human \\noversight, which signifies humans’ involvement and oversight in AI \\nprocesses. The brown line represents ethical standards, which means \\nadherence to ethical guidelines in AI. The pink line represents innova\\xad\\ntion encouragement, which reflects support for innovative AI develop\\xad\\nment. The gray line represents national security, emphasizing AI’s role \\nin national security.\\nThese lines provide a comprehensive view of how different countries \\naddress multiple facets of AI ethics in their policies. They reflect a broader \\nrange of considerations in the global discourse on AI governance. By con\\xad\\nsidering the different aspects of AI ethics, policymakers can create fair, trans\\xad\\nparent, and accountable policies while encouraging innovation and protecting \\nuser privacy and data. This is essential to building trust in AI and ensuring that \\nit is used to benefit society.\\ne2463722-28\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 29}, page_content='Strategies to Mitigate Bias\\nThis section expands into the issue of bias in Artificial Intelligence (AI) \\nsystems and its significant impact on fairness and effectiveness. We introduce \\nFigure 8, a network diagram visually representing various interconnected \\nstrategies crucial for mitigating bias. These include ensuring data diversity to \\navoid biased AI models, conducting regular audits to detect and correct biases, \\nemploying bias detection tools, and emphasizing the importance of algorith\\xad\\nmic transparency. In addition, we emphasize the importance of integrating \\ndiverse development teams and providing ethical AI training to reduce uncon\\xad\\nscious bias in AI design and development. The section then transitions to \\nFigure 10, which provides a more detailed network representation, highlight\\xad\\ning the synergies and dependencies among these strategies. This section \\ndemonstrates how a collective, multifaceted approach, comprising both tech\\xad\\nnical and organizational measures, is vital for developing AI systems that are \\nequitable, fair, and aligned with ethical standards. It is essential to note that \\nmitigating bias in AI is an ongoing process that requires continuous vigilance \\nand adaptation to evolving AI technologies and societal norms.\\nComprehensive Mitigation Strategies\\nBias in AI systems is of utmost importance, as it can significantly impact the \\nfairness and effectiveness of these technologies. This section reviews various \\nstrategies aimed at mitigating bias, presented as a network diagram showcas\\xad\\ning the interconnectedness and collective importance of these strategies.\\nThe network diagram encompasses a range of approaches, each linked to \\ndemonstrate how they complement and reinforce one another. Key strategies \\ninclude ensuring data diversity and using datasets representing all relevant \\ndemographics to prevent biased AI models. Regular audits are crucial to \\nidentifying and addressing biases that may develop over time. Additionally, \\nthe network emphasizes the importance of using bias detection tools, which \\nemploy specialized algorithms to uncover and address biases in AI systems.\\nThe diagram in Figure 9 emphasizes the significance of collectively imple\\xad\\nmenting these strategies, indicating that the most effective approach to miti\\xad\\ngating bias involves a multifaceted effort. This includes technical solutions and \\norganizational and procedural measures to ensure that AI systems are devel\\xad\\noped and operated in a manner that minimizes bias and promotes fairness.\\nThe diagram in Figure 9, shows the connections and assigned weights \\nbetween different strategies to mitigate bias in AI. The thickness of the lines \\ncorresponds to the strength of the connection, and the weights are labeled on \\nthe diagram. The strategies are arranged circularly to emphasize their inter\\xad\\nconnectedness and importance. By implementing them collectively, the risk of \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-29'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 30}, page_content='Figure 9. The significance of a collective implementation of AI Strategies: connections and Weights \\nin Strategies to Mitigate Bias in Al (Colour-coded by Strength).\\nFigure 10. Strategies to Mitigate AI Bias.\\ne2463722-30\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 31}, page_content='bias in AI systems can be significantly reduced, leading to more equitable and \\ntrustworthy AI solutions.\\nOne key strategy is to ensure data diversity and representation. This \\ninvolves using diverse data representing all relevant demographics to avoid \\nbiased models in AI systems. Regular audits are also essential to identify and \\nrectify any biases that may have crept in over time. Bias detection tools are \\nanother essential strategy. These tools utilize specialized software to detect \\nbiases in AI algorithms, which can then be corrected. Diverse development \\nteams can also help minimize unconscious biases in designing and developing \\nAI systems.\\nProviding ethical AI training to AI professionals is another way to mitigate \\nbias in AI. This training educates AI professionals on ethical considerations \\nand avoiding bias. Algorithmic transparency is also crucial to reducing bias in \\nAI. By making the workings of AI algorithms transparent, biases can be \\nidentified and corrected more quickly. Involving various stakeholders, includ\\xad\\ning those from underrepresented groups, to provide feedback on AI systems \\nand their outputs can also significantly reduce bias. Finally, continuously \\nmonitoring AI systems is essential to quickly identify and address any biases \\nthat may emerge over time. Collectively implementing these strategies can \\nsignificantly reduce the risk of bias in AI systems, leading to more equitable \\nand trustworthy AI solutions.\\nThe flowchart in Figure 10 outlines a network representation of strategies \\nfor mitigating bias in AI. The enhanced diagram provides more context and \\ndisplays the connections between these strategies, providing a comprehensive \\napproach to addressing this issue.\\nAs shown in Figure 9, data diversity is one of the primary strategies to \\nmitigate bias in AI. It is essential to ensure that data is collected from diverse \\ndemographics. This strategy is linked to bias detection tools, emphasizing the \\nimportance of having a wide range of data to identify and correct biases. \\nRegular audits are another crucial component of mitigating bias in AI systems. \\nPeriodic reviews of AI systems for biases are required and are connected to \\ncontinuous monitoring. This highlights the need for ongoing assessments to \\nensure the AI system remains unbiased.\\nUsing bias detection tools is also essential in mitigating bias in AI systems. \\nThis strategy links to algorithm transparency, underscoring detection tools’ \\nrole in making AI decisions more straightforward. Ensuring that the AI \\nalgorithm is transparent makes it easier to identify and correct any potential \\nbiases.\\nDiverse teams are also vital to mitigating bias in AI systems. Having diverse \\nbackgrounds in development teams can significantly reduce unconscious \\nbiases. This strategy is connected to ethical training, showing the importance \\nof diverse perspectives in ethical AI development. This ensures that the AI \\nsystem is developed ethically and unbiasedly.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-31'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 32}, page_content='Ethical training is another crucial strategy in mitigating bias in AI systems. \\nTraining on ethical AI development practices is vital, and this connects to \\nstakeholder feedback. This illustrates the role of ethical considerations in \\nincorporating diverse viewpoints, ensuring that the AI system is developed \\nwith the interests of all stakeholders in mind.\\nMaking AI algorithms’ decisions clear is another essential strategy for \\nidentifying biases. This is connected to regular audits, highlighting the need \\nfor transparency in ongoing assessments. Ensuring that the AI algorithm’s \\ndecisions are transparent makes it easier to identify and correct any potential \\nbiases.\\nIncorporating feedback from all groups, including underrepresented ones, \\nis essential in developing an unbiased AI system. This relates back to data \\ndiversity, emphasizing the role of inclusive feedback in ensuring diverse data \\nrepresentation. Diverse feedback and ongoing surveillance for emerging biases \\nis necessary to mitigate bias in AI systems. This relates to diverse teams, \\nunderscoring the need for continuous oversight by teams with varied back\\xad\\ngrounds and perspectives. Diverse teams make it easier to identify and correct \\npotential biases.\\nThe network diagram in Figure 9 illustrates the interconnected nature of \\nthese strategies, showing how each contributes to a comprehensive approach \\nto mitigating bias in AI systems. By implementing these strategies, AI systems \\ncan be developed more ethically and unbiasedly, with the interests of all \\nstakeholders in mind.\\nEthical Training and Diverse Teams\\nProviding ethical training to AI professionals is crucial to making them \\naware of potential biases and fostering an ethical culture in AI devel\\xad\\nopment. This training should cover the ethical implications of AI, the \\nsignificance of diversity in datasets, and ways to detect and mitigate \\nbias.\\nForming diverse teams is also a vital strategy. Teams composed of people \\nfrom diverse backgrounds bring unique perspectives to the AI development \\nprocess, which can help identify biases that a more homogeneous group may \\noverlook. The diversity here refers to demographic factors and variations in \\nexpertise, experience, and viewpoints.\\nBy combining ethical training, diversity in teams, and technical strate\\xad\\ngies such as data diversity and regular audits, AI systems can be devel\\xad\\noped in a way that is more equitable, fair, and aligned with ethical \\nstandards. Countering bias is a continuous process that necessitates \\nongoing attention and adaptation as AI technologies progress.\\ne2463722-32\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 33}, page_content='Emerging AI Technologies and Their Ethical Challenges\\nLarge language models (LLMs), such as GPT-3 and GPT-4, exemplify the \\ngrowing power of generative AI. These models are trained on vast datasets, \\noften scraping data from the internet, which raises significant concerns over \\ndata provenance, copyright infringement, and privacy violations. The opa\\xad\\nque nature of these models complicates efforts to ensure that they are free from \\nbiases present in the training data, such as discriminatory language, misinfor\\xad\\nmation, or unintentional perpetuation of harmful stereotypes. Despite \\nemploying fine-tuning and debiasing techniques, these models are still prone \\nto producing biased outputs due to the inherent limitations of the training \\ndata and the probabilistic nature of their generation processes. For instance, \\ntechniques such as reinforcement learning from human feedback (RLHF) \\nhave been deployed to mitigate harmful outputs, but they remain insufficient \\nin addressing the deeper systemic biases embedded within the underlying \\ndatasets. This calls for more sophisticated techniques, such as adversarial \\ntraining, where adversarial examples are used to iteratively refine models \\nand expose hidden biases. Additionally, federated learning presents \\na promising approach for enhancing the ethical training of LLMs by allowing \\nmodels to learn from decentralized, anonymized data, thus reducing the \\nethical risks associated with data centralization and privacy violations.\\nAnother critical issue with LLMs lies in their ability to produce convincing \\nbut factually incorrect or hallucinatory outputs. This problem, often referred \\nto as the “hallucination problem,” presents ethical challenges in high-stakes \\ndomains such as healthcare or law, where accurate information is paramount. \\nCurrent mitigation strategies include truth-verification models that cross- \\ncheck generated content against verified databases and automated fact- \\nchecking systemsintegrated into the model’s inference pipeline. However, \\nthese methods are still evolving and are far from fully resolving the issue. \\nEthical frameworks for LLM deployment must, therefore, include rigorous \\npost-deployment monitoring and real-time validation mechanisms to ensure \\nthe integrity of the outputs, particularly in applications where misinformation \\ncould have profound societal impacts.\\nAutonomous systems, including autonomous vehicles, drones, and \\nrobotic systems, pose additional ethical challenges related to safety, \\naccountability, and decision-making autonomy. A key ethical dilemma \\narises in the context of autonomous decision-making in unpredictable \\nenvironments. For instance, in the case of autonomous vehicles, ethical \\nframeworks must account for the so-called trolley problem scenarios, \\nwhere the system must make life-and-death decisions in the event of an \\nunavoidable accident. Traditional rule-based ethical systems, such as \\ndeontological or utilitarian approaches, often fail to provide clear solu\\xad\\ntions in these nuanced scenarios. Consequently, emerging solutions \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-33'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 34}, page_content='involve the use of ethical AI algorithms like multi-objective optimiza\\xad\\ntion, which allows systems to balance competing ethical principles – such \\nas minimizing harm and respecting human autonomy – by assigning \\ndynamic weights to different ethical outcomes based on real-time envir\\xad\\nonmental factors.\\nFurthermore, accountability in autonomous systems presents a unique \\nchallenge, especially in cases where systems operate with minimal human \\noversight. Explainable AI (XAI) plays a critical role here, enabling transpar\\xad\\nency in decision-making processes by providing interpretable insights into \\nhow the system reached a specific decision. Techniques such as attention \\nmechanisms and saliency maps can be employed to highlight the features that \\nmost influenced an autonomous system’s decision, making it easier for reg\\xad\\nulators and auditors to understand and assess the fairness and safety of these \\ndecisions. However, the effectiveness of XAI in highly complex, real-time \\nautonomous systems remains limited, necessitating the development of causal \\ninference models that can provide a more comprehensive understanding of \\ndecision-making pathways and their underlying ethical implications.\\nThe issue of algorithmic accountability in autonomous weapons systems \\n(AWS) presents perhaps the most acute ethical challenge. The development \\nand deployment of AWS raise profound concerns over autonomous lethality \\n—the ability of a system to make life-or-death decisions without human \\nintervention. Current discussions on international AI governance focus on \\nthe need to restrict the deployment of AWS through legally binding treaties, \\nbut enforcement mechanisms remain elusive. From a technical standpoint, \\none proposed solution involves embedding human-in-the-loop (HITL) \\nmechanisms that ensure critical decisions, particularly those involving the \\nuse of lethal force, require human validation before execution. This integration \\nof human oversight into decision-making processes is critical to preventing \\nunintended harm and ensuring compliance with international humanitarian \\nlaw. Additionally, ongoing research into ethical-by-design architectures aims \\nto build ethical constraints directly into the system’s operational framework, \\nlimiting the scope of actions that an autonomous system can take based on \\npredefined ethical guidelines.\\nFinally, the deployment of swarm intelligence in autonomous drones and \\nrobots introduces challenges related to collective decision-making and dis\\xad\\ntributed accountability. In swarm systems, decisions are often made collec\\xad\\ntively by a distributed group of agents, with no single agent being responsible \\nfor the final outcome. This creates significant ethical ambiguity in determining \\naccountability when swarm systems malfunction or cause harm. Solutions \\nsuch as distributed ledger technologies (DLT), including blockchain, have \\nbeen proposed to ensure that every decision made within the swarm is \\nrecorded in a transparent and immutable way, providing a traceable log of \\nactions that can be audited for accountability purposes.\\ne2463722-34\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 35}, page_content='Discussion\\nThe introduction of fairness, transparency, and accountability into AI systems, \\nwhile crucial for ensuring ethical standards, introduces a significant financial \\nburden and operational complexity, especially in sectors where fast innovation \\nis a competitive necessity.\\nOne of the primary economic costs arises from the increased complexity in \\ndeveloping AI systems that adhere to ethical guidelines. Implementing fair\\xad\\nness-aware learning algorithms, such as demographic parity or equalized odds, \\nrequires additional computational resources and extensive testing during the \\ntraining phase. These fairness constraints are not simply add-ons but require \\na fundamental rethinking of the algorithmic design, particularly in cases where \\nperformance optimization conflicts with fairness. For instance, in financial \\nservices, ensuring that loan approval algorithms do not exhibit bias may \\nnecessitate retraining models with diverse datasets and applying fairness con\\xad\\nstraints throughout the development cycle. This extended development pro\\xad\\ncess incurs higher labor costs, requires greater infrastructure investment, and \\noften results in longer timeframes to achieve regulatory compliance. \\nAdditionally, privacy-preserving techniques, such as differential privacy and \\nfederated learning, add further complexity. Federated learning, which enables \\nmodel training across distributed datasets without centralizing sensitive data, \\nrequires more sophisticated system architectures and secure communication \\nchannels, increasing both the cost and technical difficulty of implementation.\\nOperationally, the impact of strict ethical guidelines is felt through the need \\nfor ongoing compliance and continuous monitoring of AI systems. Ethical \\nframeworks such as the European Union’s AI Act mandate that high-risk AI \\napplications, particularly in fields like healthcare and criminal justice, undergo \\ncontinuous auditing to ensure ethical standards are maintained post- \\ndeployment. These operational costs are amplified by the need to integrate \\nreal-time fairness monitoring tools, such as AI Fairness 360, which check for \\nbias drift or decision-making anomalies as AI systems encounter new data. \\nThese tools require continuous computational resources, infrastructure sup\\xad\\nport, and personnel dedicated to auditing and model recalibration. For indus\\xad\\ntries such as financial services, where AI systems are deployed in real-time \\nenvironments like high-frequency trading, maintaining fairness and compli\\xad\\nance adds layers of complexity to the operational workflow. This constant \\nneed for recalibration can also result in downtime, during which systems must \\nbe reevaluated and updated, leading to delays in decision-making processes \\nand potential disruptions to business continuity.\\nThe financial impact of these ethical requirements also affects innovation \\ncycles and speed to market. In highly competitive sectors like autonomous \\ndriving or AI-driven diagnostics, time-to-market is often crucial for gaining \\na first-mover advantage. Companies that invest heavily in ethical compliance – \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-35'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 36}, page_content='such as model transparency, fairness audits, and explainability – may experi\\xad\\nence delays in bringing products to market. For instance, the requirement to \\nintegrate explainability mechanisms, such as SHAP (Shapley Additive \\nExplanations) or LIME (Local Interpretable Model-agnostic Explanations), \\ninto AI models often necessitates additional development and testing phases. \\nThis extends the overall project timeline and may place companies at \\na competitive disadvantage against those who prioritize rapid deployment \\nover ethical oversight. The delay not only impacts short-term revenue but \\nalso affects long-term strategic positioning, particularly in industries where \\ntechnological leadership is key to maintaining market share.\\nBeyond development and operational costs, legal compliance and regula\\xad\\ntory risk are significant financial considerations for companies implementing \\nstrict ethical guidelines. Regulatory frameworks like the GDPR and the \\nupcoming EU AI Act impose severe penalties for noncompliance, with fines \\nthat can reach up to 4% of a company’s global revenue for violations of data \\nprivacy and transparency requirements. To mitigate these risks, companies \\noften need to invest heavily in legal teams, external audits, and compliance \\ninfrastructures. This introduces an additional cost layer as companies must \\nallocate resources not just for initial development but also for ongoing com\\xad\\npliance management. The cyclical nature of compliance – where systems must \\nbe continuously updated, audited, and re-certified to meet evolving stan\\xad\\ndards – creates long-term financial commitments that extend well beyond \\nthe initial implementation of AI systems.\\nDespite these costs, emerging technologies offer potential solutions \\nthat could mitigate some of the financial and operational burdens \\nassociated with ethical AI. Automated machine learning (AutoML) sys\\xad\\ntems are increasingly capable of incorporating fairness and transparency \\nchecks into their development pipelines, reducing the need for manual \\nintervention and thus lowering labor costs. Additionally, distributed \\nledger technologies (DLT), such as blockchain, can help track AI deci\\xad\\nsions in a transparent and immutable way, thereby simplifying post- \\ndeployment audits and reducing the cost of maintaining ethical stan\\xad\\ndards. Nevertheless, while these technologies offer some relief, they \\ncome with their own set of technical challenges and infrastructural \\ncosts, which require additional investment and expertise to implement \\neffectively.\\nThe implementation of strict ethical guidelines in AI development signifi\\xad\\ncantly impacts economic and operational aspects of AI projects. While these \\nguidelines are crucial for ensuring fairness, transparency, and accountability, \\nthey introduce substantial costs at every stage of the AI lifecycle, from devel\\xad\\nopment through to post-deployment monitoring and compliance. Balancing \\nthese ethical obligations with the need for innovation and market competi\\xad\\ntiveness remains a challenge, particularly for companies operating in highly \\ne2463722-36\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 37}, page_content='dynamic and competitive sectors. The evolving landscape of AI governance, \\ncoupled with emerging cost-saving technologies, will be critical in determining \\nhow companies navigate the financial and operational implications of ethical \\nAI development.\\nConclusion\\nThis study has undertaken an examination of the ethical imperatives sur\\xad\\nrounding AI, particularly the principles of transparency, fairness, and privacy, \\nin the context of its prevalent influence across sectors such as healthcare, \\nfinance, and communication. The deployment of AI technologies in these \\ndomains brings with it profound ethical challenges that necessitate a strong \\nand inclusive framework to safeguard individual rights and societal interests. \\nThrough a comparative analysis of international AI policy frameworks from \\nthe European Union, the United States, and China, this research has clarified \\nthe conflicting ethical priorities that shape AI governance globally.\\nThis work clarifies the ethical principles of privacy, transparency, and fair\\xad\\nness, addressing regional challenges and interdependencies. By distinguishing \\nhow these principles operate independently yet interactively across frame\\xad\\nworks, the paper offers a refined conceptual foundation necessary for global \\ngovernance. A primary contribution is the proposed set of integration criteria \\n(Interoperability, Normative Cohesion, Cultural Adaptability, and \\nTransparency of Process). These criteria provide a structured foundation for \\naligning ethical principles across diverse international frameworks, supporting \\ncross-border AI compatibility while respecting region-specific values and \\nregulatory approaches. The graphical representations represent the individual \\nand corelated interdependencies and conflicts among frameworks, avoiding \\noversimplification and enhancing analytical clarity. This provides a visual tool \\nfor understanding ethical elationships in global AI governance.\\nThe analysis reveals marked variations in how different regions balance the \\ndemands of innovation against the ethical principles of privacy, fairness, and \\naccountability. While certain jurisdictions, such as the European Union, \\nemphasize stringent regulatory oversight and data protection, others, includ\\xad\\ning the United States, adopt a more flexible, innovation-centric approach. \\nThese divergences underscore the complexities involved in striving for \\na harmonized global standard for ethical AI governance. Nevertheless, this \\nstudy has articulated several strategic interventions to mitigate algorithmic \\nbias, including the deployment of fairness-aware algorithms, regular audits, \\nand the incorporation of diverse development teams. These interventions are \\nessential in fostering equitable and trustworthy AI systems.\\nFurthermore, this research has highlighted the critical need for sustained \\ninternational collaboration and dialogue to bridge the gaps in global AI ethics \\nframeworks. It is increasingly evident that no single jurisdiction can fully \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-37'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 38}, page_content='address the multi-faceted ethical challenges posed by AI in isolation. Instead, \\nthe path forward demands a concerted, cooperative effort that leverages shared \\nprinciples while respecting regional variations in regulatory and cultural \\npriorities.\\nThis study advances the argument that ethical considerations must be \\nembedded at every stage of the AI development lifecycle, from inception \\nthrough to deployment and beyond. The recommendations herein aim to \\ninform policymakers, regulators, and AI developers, encouraging the pursuit \\nof AI systems that are not only innovative and technologically advanced but \\nalso aligned with the highest ethical standards. As AI continues to evolve and \\nexert its transformative potential, the need for vigilance, adaptability, and \\ncross-border cooperation remains paramount in ensuring that these technol\\xad\\nogies serve the common good, promoting fairness, accountability, and trust in \\ntheir application.\\nDisclosure Statement\\nNo potential conflict of interest was reported by the author(s).\\nFunding\\nThe work was supported by the Engineering and Physical Sciences Research Council [EP/ \\nS035362/1].\\nORCID\\nPetar Radanliev \\nhttp://orcid.org/0000-0001-5629-6857\\nData Availability Statement\\nThe datasets generated and analyzed during the current study are available from the corre\\xad\\nsponding author upon reasonable request. Due to the sensitive nature of the data related to AI \\nethics and privacy considerations, access to the data may be restricted. Specific details regard\\xad\\ning the data sources, including international AI policy frameworks from the EU, US, China, \\nCanada, Japan, India, and Australia, are documented within the study. All data shared will be \\ncompliant with ethical guidelines and privacy standards as outlined in the General Data \\nProtection Regulation (GDPR) and other relevant data protection laws.\\nReferences\\nAldoseri, A., K. N. Al-Khalifa, and A. M. Hamouda. 2023. Re-Thinking data strategy and \\nintegration for artificial intelligence: Concepts, opportunities, and challenges. Applied \\nSciences 13 (12):7082. doi: 10.3390/APP13127082  .\\ne2463722-38\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 39}, page_content='Bécue, A., I. Praça, and J. Gama. 2021. Artificial intelligence, cyber-threats and industry 4.0: \\nChallenges and opportunities. Artificial Intelligence Review 54 (5):3849–86. doi: 10.1007/ \\ns10462-020-09942-2  .\\nBender, E. M., T. Gebru, A. McMillan-Major, and S. Shmitchell. 2021. On the dangers of \\nstochastic parrots: Can language models be too big? FAccT 2021 - Proceedings of the 2021 \\nACM Conference on Fairness, Accountability, and Transparency, 610–23. doi: 10.1145/ \\n3442188.3445922  .\\nBinns, R. 2018. Fairness in machine learning: Lessons from political philosophy. Proceedings of \\nMachine Learning Research, vol. 81, 149–59, PMLR. https://proceedings.mlr.press/v81/ \\nbinns18a.html .\\nBommasani, R., K. Klyman, D. Zhang, and P. Liang. 2023. Do foundation model providers \\ncomply with the draft EU AI act? Center for Research on Foundation Models (CRFM): \\nStanford Center for Research on Foundation Models.\\nBostrom, N., and E. Yudkowsky. 2014. The ethics of artificial intelligence. The Cambridge \\nHandbook of Artificial Intelligence 316–34. doi: 10.1017/CBO9781139046855.020  .\\nBrynjolfsson, E., and A. Mcafee. 2014. The second machine age: Work, progress, and prosperity \\nin a time of brilliant technologies. In The second machine age: Work, progress, and prosper\\xad\\nity in a time of brilliant technologies. Worldwide: W.W. Norton & Company 978-0-393- \\n35064-7 https://wwnorton.com/books/the-second-machine-age/ .\\nde Bruin, B., and L. Floridi. 2017. The ethics of cloud computing. Science and Engineering \\nEthics 23 (1):21–39. doi: 10.1007/s11948-016-9759-0  .\\nde Fine Licht, K., and J. de Fine Licht. 2020. Artificial intelligence, transparency, and public \\ndecision-making. AI & Society 35 (4):917–26. doi: 10.1007/s00146-020-00960-w  .\\nDu, M. 2023. Awesome-Fairness-in-AI. GitHub Repository. https://github.com/datamllab/awe \\nsome-fairness-in-ai .\\nEuropean Parliament. 2023. AI act: A step closer to the first rules on artificial intelligence | news | \\nEuropean Parliament. https://www.europarl.europa.eu/news/en/press-room \\n/20230505IPR84904/ai-act-a-step-closer-to-the-first-rules-on-artificial-intelligence .\\nEvans, K. 2015. The second machine age: Work, progress, and prosperity in a time of brilliant \\ntechnologies by eric Brynjolfsson and Andrew McAfee. Journal of Business & Finance \\nLibrarianship 20 (3):244–46. doi: 10.1080/08963568.2015.1044355  .\\nFACT SHEET: Biden-Harris Administration Announces New Actions to Promote Responsible \\nAI Innovation That Protects Americans’ Rights and Safety | The White House. 2023. https:// \\nwww.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden- \\nharris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that- \\nprotects-americans-rights-and-safety/ .\\nFloridi, L., J. Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge, \\nR. Madelin, U. Pagallo, F. Rossi, et al. 2018. AI4People—an ethical framework for a good \\nAI society: Opportunities, risks, principles, and recommendations. Minds and Machines \\n28 (4):689–707. doi: 10.1007/s11023-018-9482-5  .\\nGDPR. 2018. What is GDPR, the EU’s new data protection law? - Gdpr.Eu. https://gdpr.eu/ \\nwhat-is-gdpr/ .\\nHelbing, D., B. S. Frey, G. Gigerenzer, E. Hafen, M. Hagner, Y. Hofstetter, J. Van Den Hoven, \\nR. V. Zicari, and A. Zwitter. 2018. Will democracy survive big data and artificial intelligence? \\nIn Towards digital enlightenment: Essays on the dark and light sides of the digital revolu\\xad\\ntion, 73–98. Springer International Publishing. doi: 10.1007/978-3-319-90869-4_7  .\\nHIPAA. 1996. Health insurance portability and accountability act of 1996 (HIPAA) | CDC. \\nhttps://www.cdc.gov/phlp/publications/topic/hipaa.html .\\nHosny, A., C. Parmar, J. Quackenbush, L. H. Schwartz, and H. J. W. L. Aerts. 2018. Artificial \\nintelligence in radiology. Nature Rev Cancer 18 (8):500. doi: 10.1038/S41568-018-0016-5  .\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-39'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 40}, page_content='IBM. 2018. AI fairness 360 – open source. Open Project. https://www.ibm.com/opensource/ \\nopen/projects/ai-fairness-360/ .\\nICO. 2018. Information commissioner’s office (ICO): The UK GDPR. UK GDPR Guidance and \\nResources. https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful- \\nbasis/a-guide-to-lawful-basis/lawful-basis-for-processing/consent/ .\\nInterim Measures for the Management of Generative Artificial Intelligence Services, Personal \\nInformation Protection Law of the People’s Republic of China (PRC). 2023.\\nISO. 2023. ISO/IEC DIS 42001 - information technology — artificial intelligence — management \\nsystem. https://www.iso.org/standard/81230.html .\\nJobin, A., M. Ienca, and E. Vayena. 2019. The global landscape of AI ethics guidelines. Nature \\nMachine Intelligence 2019 1 (9):389–99. doi: 10.1038/s42256-019-0088-2  .\\nLi, L. 2017. China’s manufacturing locus in 2025: With a comparison of “Made-in-China 2025” \\nand “Industry 4.0”. Technological Forecasting & Social Change 135:66–74. doi: 10.1016/J. \\nTECHFORE.2017.05.028  .\\nMalhotra, Y. 2018. Cognitive computing for anticipatory risk analytics in intelligence, surveil\\xad\\nlance, & reconnaissance (ISR): Model risk management in artificial intelligence & machine \\nlearning (presentation slides). SSRN Electronic Journal. doi: 10.2139/ssrn.3111837  .\\nMcCorduck, P., and C. Cfe. 2004. Machines who think: A personal inquiry into the history and \\nprospects of artificial intelligence. CRC Press. https://books.google.com/books?hl=en&lr= \\n&id=r2C1DwAAQBAJ&oi=fnd&pg=PP1&dq=Pamela+McCorduck+%22Machines+Who \\n+Think&ots=UnmXIiuRtM&sig=JAh90Eu07MGvjS5OgFq1CMy6-gc .\\nMeissner, G. 2020. Artificial intelligence: Consciousness and conscience. AI & Society \\n35 (1):225–35. doi: 10.1007/s00146-019-00880-4  .\\nMeitY. 2023. Artificial intelligence committees reports | Ministry of electronics and informa\\xad\\ntion technology, Government of India. Artificial Intelligence Committees Report. https:// \\nwww.meity.gov.in/artificial-intelligence-committees-reports .\\nMijwil, M. M., M. Aljanabi, and ChatGPT. 2023. Towards artificial intelligence-based cyberse\\xad\\ncurity: The practices and ChatGPT generated ways to combat cybercrime. Iraqi Journal for \\nComputer Science and Mathematics 4 (1):65–70. doi: 10.52866/IJCSM.2023.01.01.0019  .\\nMittelstadt, B. 2019. Principles alone cannot guarantee ethical AI. Nature Machine Intelligence \\n1 (11):501–07. doi: 10.1038/s42256-019-0114-4  .\\nMozumder, M. A. I., M. M. Sheeraz, A. Athar, S. Aich, and H.-C. Kim. 2022. Overview: \\nTechnology roadmap of the future trend of metaverse based on IoT, blockchain, AI \\ntechnique, and medical domain metaverse activity. International Conference on Advanced \\nCommunication Technology (ICACT) 256–61. doi: 10.23919/ICACT53585.2022.9728808  .\\nNAIAC. 2024. AI safety: National AI advisory committee. https://ai.gov/wp-content/uploads/ \\n2024/06/FINDINGS-RECOMMENDATIONS_AI-Safety.pdf .\\nNIST. 2023a. AI risk management framework | NIST. National Institute of Standards and \\nTechnology. https://www.nist.gov/itl/ai-risk-management-framework .\\nNIST. 2023b. Artificial intelligence | NIST. https://www.nist.gov/artificial-intelligence .\\nNIST. 2024a. AI risk management framework | NIST.\\nNIST. 2024b. AI standards | NIST. https://www.nist.gov/artificial-intelligence/ai-standards .\\nNIST. 2024c. Department of commerce announces new guidance, tools 270 days following \\npresident Biden’s executive order on AI | NIST. https://www.nist.gov/news-events/news/ \\n2024/07/department-commerce-announces-new-guidance-tools-270-days-following .\\nOffice for Artificial Intelligence and Department for Science, Innovation & Technology. 2023. \\nA pro-innovation approach to AI regulation 978-1-5286-4009-1 (London: Crown copyright).\\nPartnership on AI. 2023. Partnership on AI and the ethical AI framework for social good. https:// \\npartnershiponai.org/ .\\ne2463722-40\\nP. RADANLIEV'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creator': 'PTC Arbortext Publishing Engine', 'creationdate': '2025-02-07T21:22:17+05:30', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'modDate': \"D:20250207212217+05'30'\", 'creationDate': \"D:20250207212217+05'30'\", 'page': 41}, page_content='Provisions on the Administration of Deep Synthesis Internet Information Services, Personal \\nInformation Protection Law of the People’s Republic of China (PRC). 2022.\\nRoberts, H., J. Cowls, J. Morley, M. Taddeo, V. Wang, and L. Floridi. 2021. The Chinese \\napproach to artificial intelligence: An analysis of policy, ethics, and regulation. AI & Society \\n36 (1):59–77. doi: 10.1007/s00146-020-00992-2  .\\nShu, Y., J. Zhang, and H. Yu. 2021. Fairness in design: A tool for guidance in ethical artificial \\nintelligence design. Lecture Notes in Computer Science (Including Subseries Lecture Notes in \\nArtificial Intelligence and Lecture Notes in Bioinformatics) 12774:500–10. doi: 10.1007/978- \\n3-030-77626-8_34  .\\nSinger, P. W. 2009. Wired for war: The robotics revolution and conflict in the twenty-first \\ncentury, 499. https://books.google.com/books/about/Wired_for_War.html?id= \\nAJuowQmtbU4C .\\nThe State Council People Republic of China. 2017. Made in China 2025; the state council people \\nRepublic of China. http://english.gov.cn/2016special/madeinchina2025/ .\\nTabassi, E. 2023. AI risk management framework | NIST. doi: 10.6028/NIST.AI.100-1  .\\nTurilli, M., and L. Floridi. 2009. The ethics of information transparency. Ethics and Information \\nTechnology 11 (2):105–12. doi: 10.1007/s10676-009-9187-9  .\\nUNESCO. 2023. Recommendation on the ethics of artificial intelligence | UNESCO. https://www. \\nunesco.org/en/articles/recommendation-ethics-artificial-intelligence .\\nWachter, S., B. Mittelstadt, and C. Russell. 2023. Health care bias is dangerous. But so are \\nfairness’ algorithms | WIRED. Wired. https://www.wired.com/story/bias-statistics-artificial- \\nintelligence-healthcare/ .\\nYu, K. H., A. L. Beam, and I. S. Kohane. 2018. Artificial intelligence in healthcare. Nature \\nBiomedical Engineering 2 (10):719–31. doi: 10.1038/S41551-018-0305-Z.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-41'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 0}, page_content='IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\n799\\nAn Overview of Artiﬁcial Intelligence Ethics\\nChangwu Huang\\n, Member, IEEE, Zeqi Zhang, Bifei Mao, and Xin Yao\\n, Fellow, IEEE\\nAbstract—Artiﬁcial intelligence (AI) has profoundly changed\\nand will continue to change our lives. AI is being applied in more\\nand more ﬁelds and scenarios such as autonomous driving, med-\\nical care, media, ﬁnance, industrial robots, and internet services.\\nThe widespread application of AI and its deep integration with\\nthe economy and society have improved efﬁciency and produced\\nbeneﬁts. At the same time, it will inevitably impact the existing\\nsocial order and raise ethical concerns. Ethical issues, such as\\nprivacy leakage, discrimination, unemployment, and security risks,\\nbrought about by AI systems have caused great trouble to people.\\nTherefore, AI ethics, which is a ﬁeld related to the study of ethical\\nissues in AI, has become not only an important research topic\\nin academia, but also an important topic of common concern for\\nindividuals, organizations, countries, and society. This article will\\ngive a comprehensive overview of this ﬁeld by summarizing and\\nanalyzingtheethicalrisksandissuesraisedbyAI,ethicalguidelines\\nand principles issued by different organizations, approaches for\\naddressing ethical issues in AI, and methods for evaluating the\\nethics of AI. Additionally, challenges in implementing ethics in\\nAI and some future perspectives are pointed out. We hope our\\nwork will provide a systematic and comprehensive overview of AI\\nethics for researchers and practitioners in this ﬁeld, especially the\\nbeginners of this research discipline.\\nImpact Statement—AI ethics is an important emerging topic\\namong academia, industry, government, society, and individuals. In\\nthe past decades, many efforts have been made to study the ethical\\nissues in AI. This article offers a comprehensive overview of the AI\\nethics ﬁeld, including a summary and analysis of AI ethical issues,\\nManuscript received 5 September 2021; revised 22 February 2022 and 3\\nMay 2022; accepted 23 July 2022. Date of publication 28 July 2022; date\\nof current version 21 July 2023. This work was supported in part by the\\nResearch Institute of Trustworthy Autonomous Systems (RITAS), in part by\\nthe Guangdong Provincial Key Laboratory under Grant 2020B121201001, in\\npart by the Program for Guangdong Introducing Innovative and Enterpreneurial\\nTeams under Grant 2017ZT07X386, in part by Shenzhen Science and Tech-\\nnology Program under Grant KQTD2016112514355531, and in part by a joint\\nproject between Huawei and Southern University of Science and Technology\\nunder Project FA2019061021. This paper was recommended for publication\\nby Associate Editor J. Torresen upon evaluation of the reviewers’ comments.\\n(Corresponding author: Xin Yao.)\\nChangwu Huang is with the Research Institute of Trustworthy Autonomous\\nSystems, Southern University of Science and Technology, Shenzhen 518055,\\nChina, and also with the Guangdong Provincial Key Laboratory of Brain-\\ninspired Intelligent Computation, Department of Computer Science and En-\\ngineering, Southern University of Science and Technology, Shenzhen 518055,\\nChina (e-mail: huangcw3@sustech.edu.cn).\\nZeqi Zhang and Bifei Mao are with the Trustworthiness Theory Research\\nCenter, Huawei Technologies Company, Ltd., Shenzhen 518055, China (e-mail:\\nzhangzeqi@huawei.com; maobifei@huawei.com).\\nXin Yao is with the Research Institute of Trustworthy Autonomous Systems,\\nSouthern University of Science and Technology, Shenzhen 518055, China, with\\nGuangdong Provincial Key Laboratory of Brain-inspired Intelligent Computa-\\ntion, Department of Computer Science and Engineering, Southern University\\nof Science and Technology, Shenzhen 518055, China, and also with the School\\nof Computer Science, University of Birmingham, B15 2TT Birmingham, U.K.\\n(e-mail: xiny@sustech.edu.cn).\\nThis\\narticle\\nhas\\nsupplementary\\ndownloadable\\nmaterial\\navailable\\nat\\nhttps://doi.org/10.1109/TAI.2022.3194503, provided by the authors.\\nDigital Object Identiﬁer 10.1109/TAI.2022.3194503\\nethical guidelines and principles, approaches to address AI ethical\\nissues, and methods to evaluate the ethics of AI technologies. Addi-\\ntionally, research challenges and future perspectives are discussed.\\nThis article will help researchers to gain a birds eye view of AI\\nethics, and thus facilitate their further investigation and research\\nof AI.\\nIndex Terms—Artiﬁcial intelligence (AI), AI ethics, ethical issue,\\nethical theory, ethical principle.\\nI. INTRODUCTION\\nA\\nRTIFICIAL intelligence (AI) [1] has achieved rapid and\\nremarkable development during the last decade. AI tech-\\nnologies such as machine learning (ML), natural language pro-\\ncessing, and computer vision are increasingly permeating and\\nspreading to various disciplines and aspects of our society. AI\\nis increasingly taking over human tasks and replacing human\\ndecision-making. It has been widely used in a variety of sectors,\\nsuch as business, logistics, manufacturing, transportation, health\\ncare, education, state governance, etc.\\nThe application of AI has brought about efﬁciency improve-\\nment and cost reduction, which are beneﬁcial for economic\\ngrowth, social development, and human well-being [2]. For\\ninstance, the AI chatbot can respond to clients’ inquiries at\\nany time, which will improve the customers’ satisfaction and\\nthe company’s sales [3]. AI allows doctors to serve patients in\\nremote locations through telemedicine services [4]. It is no doubt\\nthattherapiddevelopmentandwideapplicationofAIarealready\\naffecting our daily life, humanity, and society.\\nHowever, at the same time, AI also poses many signiﬁcant\\nethical risks or issues for users, developers, humans, and society.\\nOver the past few years, many cases in which AI produced\\npoor outcomes have been observed. For instance, in 2016, the\\ndriver of an electric Tesla car was killed in a road accident after\\nits Autopilot mode failed to recognize an oncoming lorry [5].\\nMicrosoft’s AI chatting bot, Tay.ai,was taken down because it\\nbecame racist and sexist only less than a day after she joined\\nTwitter [6]. There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7]. More seriously, AI technology has begun to be\\nused by criminals to harm others or the society. For example,\\ncriminals used AI-based software to impersonate a chief exec-\\nutive’s voice and demand a fraudulent transfer of $243 000 [8].\\nTherefore, it is urgent and critical to address the ethical issues\\nor risks of AI so that AI can be built, applied, and developed\\nethically.\\nAI ethics or machine ethics [9] is an emerging and interdisci-\\nplinary ﬁeld concerned with addressing ethical issues of AI [10].\\nAI ethics involves the ethics of AI, which studies the ethical\\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 1}, page_content='800\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\ntheories, guidelines, policies, principles, rules, and regulations\\nrelated to AI, and the ethical AI, that is, the AI that can uphold\\nethical norms and behaves ethically [11]. The ethics of AI is a\\nprerequisite to building ethical AI or to making AI behave in\\nan ethical manner. It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong. With\\nappropriate ethics of AI, ethical AI can be built or implemented\\nthrough some methodologies and technologies.\\nEven though AI ethics has been extensively discussed by\\ninterdisciplinary researchers for several years, it is still in its\\ninfancy [11]. AI ethics is a very broad and rapidly develop-\\ning research area that has received increasing attention from\\nresearchers in recent years. Although several review papers\\nhave been published during the past few years, each of them\\nfocuses on a certain aspect(s) of AI ethics, and there is still\\na lack of comprehensive reviews to provide a full picture of\\nthis ﬁeld. For instance, a brief review of ethical issues in AI\\nwas provided in [11], AI ethics guidelines and principles were\\ninvestigated in [12], [13], Mehrabi et al. [14] focused on bias\\nand fairness in ML, García and Fernández [15] only reviewed\\nthe safety in reinforcement learning, Mothukuri et al. [16] re-\\nviewed the security and privacy of federated learning, Liu et\\nal. [17] dedicated to a survey of privacy and security issues in\\ndeep learning, Arrieta et al. [18] concentrated on explainable\\nAI, and Zhang et al. [19] covered the key ethical and privacy\\nissues in AI and traced how such issues have changed over\\nthe past few decades using the bibliometric approach. Thus,\\nthis article is dedicated to presenting a systematic and compre-\\nhensive overview of AI ethics from diverse aspects (or topics),\\nthereby providing informative guidance for the community to\\npractice ethical AI in the future. We hope it will inform sci-\\nentists, researchers, engineers, practitioners, and other relevant\\nstakeholders, and provides sufﬁcient background, comprehen-\\nsive domain knowledge and a bird’s eye view for interested\\npeople, especially for the beginners of this research discipline,\\nso that further investigation and improvement can be pursued\\nby them.\\nThe main contributions of this article are as follows.\\n1) A comprehensive overview of AI ethics, including ethical\\nissues and risks of AI, ethical guidelines and principles\\nfor AI, approaches for addressing ethical issues in AI, and\\nmethods for evaluating ethical AI, is provided in this re-\\nview. This overview can provide a sufﬁcient background,\\ncomprehensive domain knowledge, and a roadmap for\\nresearchers and practitioners.\\n2) The ethical issues and risks caused by AI are summarized,\\nand a new categorization of AI ethical issues is proposed in\\nSection III. The proposed new categorization is helpful for\\nrecognizing, understanding, and analyzing ethical prob-\\nlems in AI and then developing solutions to solve these\\nproblems. Additionally, the ethical issues associated with\\ndifferent stages of AI system’s lifecycle are discussed.\\n3) An up-to-date global landscape of the AI ethics guidelines\\nand principles is presented in Section IV, based on 146\\nguidelines related to AI ethics released by companies,\\norganizations, and governments around the world. These\\nguidelines and principles provide a high-level guidance\\nfor the planning, development, production, and usage of\\nAI and directions for addressing AI ethical issues.\\n4) A review of multidisciplinary approaches to addressing\\nAI ethical problems, including ethical, technological, and\\nlegal approaches, is given in Section V. This not only\\nprovides an informative summary about the approaches to\\nethical AI but also suggests potentially different solutions\\nto AI ethical issues from a variety of perspectives rather\\nthan relying solely on technological approaches.\\n5) Methods for assessing or evaluating AI ethics are reviewed\\nin Section VI. Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics. However, this aspect is often overlooked in\\nthe existing literature. To the best of our knowledge, this\\narticle is the ﬁrst to summarize the aspect of evaluating\\nethical AI.\\n6) Lastly, some challenges in AI ethics and several future\\nperspectives are pointed out, which provide some research\\nquestions and directions for further research in the future.\\nThis will be helpful for interested researchers and practi-\\ntioners to pursue further research in AI ethics ﬁeld.\\nThe rest of the article is organized as follows. After this\\nintroductory section, we brieﬂy describe the review scope and\\nmethodology of this article in Section II. A comprehensive\\nsummary of the ethical issues and risks raised from AI is given\\nin Section III. Section IV reviews and analyzes the AI ethical\\nguidelines and principles that have been released during the last\\nfew years. Section V describes the paradigms or approaches\\nfor addressing ethical issues in AI. Section VI discusses the\\napproaches to evaluate the morality or ethics of AI systems or\\nproducts. Section VII outlines the challenges in implementing\\nethics in AI and gives some future perspectives on designing\\nethical AI. Section VIII brieﬂy concludes this article.\\nII. SCOPE AND METHODOLOGY\\nInthis section, weﬁrst clarifytheaspects andtopics coveredin\\nthis review and the links between these topics. Then, we describe\\nthe methodology followed in conducting this survey, including\\nthe literature search strategy and selection criteria.\\nA. Scope\\nThe scope and topics of this article is described as follows.\\nInvestigation of ethical issues and risks of AI is the starting\\npoint of this review, since it is because of the existence of\\nethical issues in AI that the research ﬁeld of AI ethics exists.\\nThus, it is necessary and important to clarify and understand\\nthe ethical problems existed in AI. Then, the ethical guidelines\\nand principles, which direct the development and use of AI,\\nare reviewed. As the ethical issues of AI have attracted more\\nand more attention from various sectors of our society, many\\norganizations (including academia, industry, and governments)\\nhave begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.\\nAfter clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI. We'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 2}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n801\\nFig. 1.\\nTopics covered in this article and the links between them.\\ncovered ethical, technological, and legal approaches, but focus\\nmore on the ﬁrst two kinds of approaches (ethical and tech-\\nnological approaches) since the researchers in AI community\\nmay be more interested in these two categories of approaches.\\nLast but not least, we summarize how to evaluate ethical AI,\\nwhich is to assess the ethicality or morality of AI, i.e., how well\\nthe ethical problems are addressed or whether an AI system\\nmeets the ethical requirements or not. Apparently, these four\\naspects are essential for solving ethical issues in AI. Thus, the\\nabove four aspects constitute the main content of this article and\\nprovide a systematic overview of AI ethics. The topics or aspects\\ncovered in this article and the links between them are illustrated\\nin Fig. 1.\\nB. Methodology\\nThis review covers a wide variety of documents, including\\nacademic, organizational, government grey literature sources,\\nand news report. The search of relevant literature was conducted\\nin two phases. In the ﬁrst phase, the entries or keywords that\\nreﬂect different terms related to AI ethics are used to search on\\nGoogle Scholar, Web of Science, IEEE Xplore, ACM Digital\\nLibrary, Science Direct, Springer Link, arXiv, and Google. The\\nentries or keywords used include: (ethics, ethical, responsibility,\\nresponsible, trustworthiness, trustworthy, transparent, explain-\\nable, fair, beneﬁcial, robust, safe, private, sustainable) AND/OR\\n(issues, risks, guideline, principle, approach, method, evalua-\\ntion, assessment, challenge) AND (artiﬁcial intelligence, AI,\\nmachine learning, ML, intelligent system, intelligent agent). We\\nmainly consider the literature published or released since 2010\\nand included as many related keywords as possible in titles. In\\nthesecondphase,wecheckedtherelatedworkofliteraturefound\\nin the ﬁrst phase, such as the cited articles and other work by the\\nsame authors of phase one.\\nAs for the ethical AI guidelines, we only collected these\\ndocuments in English (or with ofﬁcial English translations) and\\ncan be visited or downloaded on the internet. A full list with\\nURL links of collected ethical AI guidelines is provided in the\\nSupplementary Materials of this article.\\nIII. ETHICAL ISSUES AND RISKS OF AI\\nTo address the ethical problems of AI, we must ﬁrst recognize\\nand understand the potential ethical issues or risks that AI may\\nbring. Then, the necessary AI ethical guidelines, policies, prin-\\nciples, rules (i.e., Ethics of AI) can be formulated appropriately.\\nWith the adequate ethics of AI, we can design and build AI\\nthat behaves ethically (i.e., Ethical AI) [8]. The ethical issue\\nof AI generally refers to the morally bad things or problematic\\noutcomes relevant to AI (i.e., these issues and risks that are raised\\nby the development, deployment, and use of AI) that need to be\\naddressed. Many ethical issues, such as lack of transparency,\\nprivacy and accountability, bias and discrimination, safety and\\nsecurity problems, the potential for criminal and malicious use,\\nand so on, have been identiﬁed from the applications and studies.\\nThis section focuses on ethical issues and risks of AI. First,\\nfour different categorizations of AI ethical issues in the literature\\nare reviewed in Section III-A. Since these four categorizations\\neither ignore some ethical issues or are too complicated to\\nunderstand, we proposed a new categorization that classiﬁes\\nAI ethical issues into individual, societal, and environmental\\nlevels in Section III-B. Our proposed categorization compre-\\nhensively covers the existing ethical issues and is easy to un-\\nderstand, which is helpful for understanding and analyzing the\\nethical problems caused by AI. Besides, we attempt to map\\nthe ethical issues associated with the stages of AI system’s\\nlifecycle in Section III-C. This would be beneﬁcial for ﬁguring\\nout these issues during the AI system development process.\\nThe main goal of this section is to discuss and clarify the\\nethical issues of AI so that practitioners can recognize and\\nunderstand these issues, and then help them to further study\\nhow to address AI ethical issues. The main contribution in\\nthis section is that we proposed a new categorization of AI\\nethical issues, which covers the ethical issues discussed in a clear\\nand easy-to-understand manner. Additionally, the ethical issues\\nassociated with the stages of AI system’s lifecycle is discussed.\\nA. Review of Categorizations of AI Ethical Issues\\nThis section describes the ethical concerns or issues of AI\\nfrom different perspectives by reviewing four different cate-\\ngorizations that were found in our collected literature. Two\\nof them are from government reports and the other two are\\nfrom academic publications. From different perspectives and\\ncategorizations, the ethical issues involved are also somewhat\\ndifferent. In the following, four different categorizations of AI\\nethical issues are reviewed subsequently. The four reviewed cat-\\negorizations of AI ethical issues and our proposed categorization\\nare listed in Table I.\\n1) Categorization Based on Features of AI, Human Factors\\nand Social Impact: In [11], AI ethical issues are mainly dis-\\ncussed in three categories: ethical issues caused by the features\\nof AI, ethical risks caused by human factors, and social impact\\nof ethical AI issues.\\na) Ethical issues caused by features of AI: Transparency:\\nML is the core technology of current AI, especially (deep) neural\\nnetworks. However, it is hard to explain and understand the\\ninference procedure of ML, which is commonly known as the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 3}, page_content='802\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nTABLE I\\nLIST AND DISCUSSION OF THE REVIEWED CATEGORIZATION OF ETHICAL ISSUES OF AI AND OUR PROPOSED CATEGORIZATION\\n“black-box.” The opacity of ML makes the algorithms or models\\nmysterious to users and even developers. This mainly leads to the\\ntransparency issue [20]. The lack of transparency not only leads\\ntotheexplanatoryproblem,butalsoleadstodifﬁcultiesinhuman\\nmonitoring and guidance of ML or AI. Thus, transparency or\\nexplainability is one of the most widely discussed downside of\\nAI.\\nData Security and Privacy: The performance of current AI\\nstrongly depends on the training data. Usually, a huge amount\\nof data, which probably includes personal data and private\\ndata, is required to train an AI model, particularly the deep\\nlearning model. The misuse and malicious use of data, such as\\n(personal) information leakage or tampering, are serious ethical\\nissues that are closely related to every individual, institution,\\norganization, and even the country. Data security and privacy\\nare key issues encountered in the development and application\\nof AI technology [21].\\nAutonomy, Intentionality, and Responsibility: With the\\nadvancement of AI, current AI systems or agents, such as health-\\ncare robots, have a certain degree of autonomy, intentionality,\\nand responsibility [22]. Here, the autonomy of AI refers to an AI\\nsystem’s ability to operate without human intervention or direct\\ncontrol. Intentionality refers to the ability that an AI system can\\nact in a way that is morally harmful or beneﬁcial and the actions\\nare deliberate and calculated [11]. Responsibility indicates that\\nthe AI system fulﬁll some social rule and some assumed respon-\\nsibilities. However, how much autonomy, intentionality, and\\nresponsibility should an AI system be allowed is a challenging\\nquestion and issue.\\nb) Ethical issues caused by human factors: Accountabil-\\nity: When an AI system or agent fails in a speciﬁed task and\\nresults in bad consequences, who should be responsible. The\\nundesirable consequence may be caused by many factors, such\\nas the programming codes, input data, improper operation, or\\nother factors. This brings about the so-called “the problem of\\nmany hands” [23]. Thus, accountability is an ethical issue that\\nconcerns the human factors involved in the designing, imple-\\nmentation, deployment, and usage of AI.\\nEthicalStandards:AstheultimategoalofAIethicsistocreate\\nethical AI that can follow ethical principles and behave ethically\\n[10], it is crucial to form comprehensive and unbiased ethical\\nstandards for training or regulating AI to be ethical. To formulate\\nethical standards for AI, researchers and practitioners should\\nwell understand the existing ethical theories and principles [13],\\n[24].\\nHuman Rights Laws: The designer, software engineers, and\\nother participants in AI system design and application should be\\ntaught human rights laws [25]. Without training in human rights\\nlaws, they may infringe and breach essential human rights with-\\nout even realizing it. The human rights laws or acts followed by\\ndifferent countries or regions are often different. Many different\\nhuman rights laws, for instance, International Human Rights\\nLaw, International Covenant on Civil and Political Rights, In-\\nternational Covenant on Economic, Social and Cultural Rights,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 4}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n803\\nUniversal Declaration of Human Rights, Charter of the United\\nNations, the European Convention for the Protection of Human\\nRights and Fundamental Freedoms, etc. [26] have been released\\nby different governments.\\nc) Social impact of ethical AI issues: Automation and Job\\nReplacement: As more and more factory workers are being\\nreplaced by automated systems and robots, AI will disrupt and\\ntransform the labor market. Hence, many people worry about\\nautomation and job replacement [27].\\nAccessibility: The accessibility or availability of emerging\\ntechnologies, such as AI, will have a direct impact on human\\nwell-being. However, it will be unethical and unfair if only a\\nportion of the population beneﬁt from AI. Consideration must be\\ngiven to developing AI products and services that are accessible\\nto everyone, and thus the beneﬁts of AI can be spread equally\\nto everyone [28].\\nDemocracy and Civil Rights: Unethical AI will distort the\\ntruthandeventuallyleadtotheloss of trust andpublicsupport for\\nAI technology [11]. The strengths of democracies are harmed by\\nthe loss of informed and trusting communities. As democracies\\nsuffer and structural biases exacerbated, the free enjoyment\\nof civil rights is no longer consistently available to all. Thus,\\ndemocracy and civil rights must be taken into consideration in\\nAI ethics.\\n2) Categorization Based on Vulnerabilities of AI and Human:\\nIn [29], Liao distinguished the ethical issues of AI into 1) ethical\\nissues that arise because of limitations of current ML systems,\\nwhich is named as “vulnerabilities in AI (especially ML),” and\\n2) ethical issues that arise because current ML systems may be\\nworking too well and humans can be vulnerable in the presence\\nof or interaction with these intelligent systems, which is referred\\nto as “human vulnerabilities.”\\na) Ethical issues from the vulnerabilities of AI: ML is data\\nhungry: Usually, ML requires a large amount of data to work\\nwell [30]. Therefore, this motivates companies and organiza-\\ntions to collect or purchase data, including sensitive personal\\ndata, even if doing so may violate the individual’s right to\\nprivacy.\\nGarbage in/garbage out: The performance of a ML algorithm\\nheavily depends on the data from which it learns. If one ML\\nalgorithm is trained on insufﬁcient or inaccurate data, it will\\nprovide undesirable results even it is well designed [31].\\nFaulty algorithms: Even if a ML algorithm is input with\\nenough and accurate data, if the algorithm itself is bad, it will\\nalso make bad predictions. For example, a bad ML algorithm\\nmay not be able to recognize a pattern even if there is one or\\nit may recognize a pattern even if there is not one, where are\\nknown as “underﬁtting” and “overﬁtting,” respectively [32].\\nDeep learning is a black box: Deep learning is a black\\nbox, which raises issues such as explainability, interpretability,\\nand trust [33]. Even for the designers and developers of deep\\nlearning, the model is incomprehensible since it usually involves\\nthousands or millions of connections between different neurons.\\nTherefore, it is difﬁcult to explain how these connections interact\\nand why the model makes certain predictions.\\nb) Ethical issues from the vulnerabilities of human: Abuse\\nof AI: AI technologies, such as facial recognition and image\\ngeneration, can work better than humans [34]. However, ethical\\nissues exist because people may be tempted to use them for\\nill. For instance, a government could use facial recognition\\ntechnology to monitor its citizens, and ML can be used to\\nfabricate photos or videos so realistic that humans cannot tell\\nthat they are fake [35]. This brings the concern about the abuse\\nof AI technologies.\\nJob replacement: Since intelligent robots can perform certain\\ntasks faster and better than humans, many people worry that\\nrobots and other AI technologies will replace a large part of\\ncurrent human labor in the near future [36]. Thus, people may\\nbe in fear of job replacement.\\nIssues about robotic companions: As AI robots become more\\nand more sophisticated, they have begun to be regarded as\\ncompanions of humans. This raises some ethical issues about\\nthe relationship between human and robotic companions [37].\\n3) Categorization Based on Algorithm, Data, Application,\\nand Long-Term and Indirect Ethical Risks: In the analysis report\\nof AI ethical risks [38] released by the Chinese National AI\\nStandardization General Working Group, AI ethical issues are\\ncategorized into the following four aspects:\\n1) ethical issues related to AI algorithms;\\n2) ethical issues related to data;\\n3) ethical issues related to the application of AI;\\n4) long-term and indirect ethical risks.\\na) Ethical issues related to algorithms: Algorithm secu-\\nrity: The AI algorithms pose several security issues. First, there\\nis a risk of algorithm or model leakage [39], [40]. Generally, the\\nmodel is achieved by training it on the training data through op-\\ntimizing its parameters. If the model parameters of an algorithm\\nare leaked, a third party may be able to copy the model. This will\\ncause economic loss to the owner of the model, since a third party\\nobtains the same model without paying the cost of obtaining\\nthe training data. Second, the parameters of the AI algorithm\\nmodel may be modiﬁed illegally by an attacker, which will cause\\nthe performance deterioration of the AI model and may lead\\nto undesirable consequences. Additionally, in many scenarios,\\nthe output of the model is closely related to personal safety,\\nsuch as in the medical and autonomous driving ﬁelds. Once\\nthere are loopholes or mistakes in the application of algorithms\\nin these ﬁelds, it will directly harm humans and cause serious\\nconsequences [41].\\nAlgorithm explainability: Due to the black-box characteristic\\nof many ML algorithms [33], especially the popular deep learn-\\ning or neural networks, the decision process of AI algorithms\\nis hard to understand. The interpretability or explainability of\\nalgorithms is an essential ethical issue of AI [42], since it\\nconcerns the human right to know.\\nAlgorithmic decision dilemma: After obtaining the AI model,\\nthe result of the algorithm is usually unpredictable for us. In\\nother words, even though we have designed an AI model well,\\nwe cannot foresee or predict the decisions of the algorithm and\\nthe consequence it will produce. This leads to the algorithmic\\ndecision risk or dilemma of AI. For instance, autonomous ve-\\nhicles should reduce trafﬁc accidents, but sometimes they have\\nto choose between two evils, such as crushing pedestrians or\\nsacriﬁcing themselves and passengers to save pedestrians [43].'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 5}, page_content='804\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nb) Ethical issues related to data: Privacy protection: With\\nthe development of big data and AI, the tension between AI\\ntechnology and user privacy protection has become more and\\nmore serious. Criminals have more ways to obtain personal\\nprivacy data with lower costs and greater beneﬁts. Data security\\nincidents have commonly occurred in recent years. Privacy\\nprotection has become a well-recognized and serious ethical\\nissue involved by using AI [44].\\nRecognizing and processing personal and sensitive infor-\\nmation: Traditional laws and regulations only focus on the\\nprotection of personal and sensitive information. If the personal\\nor sensitive information is deidentiﬁed [45] through randomiza-\\ntion, data synthesis, and other technologies, it will no longer be\\nregarded as personal or sensitive information and not protected\\nby traditional laws. The subsequent usage, sharing, and transfer\\nof such information arise some ethical issues.\\nc) Ethical issues related to application: Algorithm dis-\\ncrimination: The execution results of algorithms directly affect\\nthe decision-making of AI systems. However, algorithm dis-\\ncrimination or bias has been seen in many applications of AI.\\nFor instance, the racial bias in criminal justice systems [46], and\\ngender discrimination in hiring [47].\\nAlgorithm abuse: Algorithm abuse [48] refers to the situation\\nwhere people use algorithms for analysis, decision-making,\\ncoordination, and other activities, but their use purpose, use\\nmethod, use range, etc., have deviations and cause adverse\\neffects. For example, facial recognition algorithms can be used\\nto improve the level of public security and speed up the discovery\\nof criminal suspects, but if they are applied to detect potential\\ncriminals, or to determine whether someone has criminal poten-\\ntial based on their face, it is an algorithm abuse.\\nd) Long-term and indirect ethical risks: Employment:\\nWith the fast advancement and widespread application of AI,\\nmore and more work can be completed by some AI products\\n[27]. This will have a signiﬁcant inﬂuence on the employment\\nproblem.\\nOwnership: As AI continues to improve, the intellectual dif-\\nferences between AI agents and humans will gradually shrink.\\nA series of debates on ownership will follow, such as whether\\nthe AI agent should be considered as “legal subject,” whether AI\\nproducts have property rights (copyrights or patent rights) [49],\\nand so forth.\\nCompetition: Unfair competition, malicious competition, and\\nmonopolistic behaviors with technological advantages will all\\nhave an impact on social stability and market freedom, fair-\\nness, and equal value, and will seriously damage the interests\\nof consumers and hinder the improvement of social welfare\\n[38]. When companies, organizations or individuals use AI\\nalgorithms, they should follow competitive ethics and not go\\nbeyond legal boundaries.\\nResponsibility: With the widespread application of AI, many\\ncases in which AI products violate the laws or ethics, such\\nas personal injury and algorithmic bias, have been observed.\\nA fundamental problem that arises in these cases is who is\\nresponsible for these bad consequences [50]. For example, as\\nautonomous driving involves multiple subjects, such as car\\nowners, drivers, passengers, car manufacturers, autonomous\\ndriving system providers, pedestrians, etc., how should they bear\\nresponsibilities after a trafﬁc accident.\\n4) Categorization Based on the Deployment of AI: In Euro-\\npean Parliamentary Research Service’s latest study on the ethical\\nimplications and moral questions brought by AI [51], the ethical\\nissues are mapped into different categories according to the ethi-\\ncalimpactsofAIonhumansociety,humanpsychology,ﬁnancial\\nsystem, legal system, environment and the planet, and trust.\\na) Impact on society: The labor market: AI has already\\nbeen applied in ﬁnance, advanced manufacturing, transporta-\\ntion, energy development, healthcare, and many other sectors.\\nWe have already seen the impact of automation on “blue collar”\\njobs. As AI agents or robots become more and more sophis-\\nticated, creative, versatile, and intelligent, more jobs will be\\naffected by AI technologies and more positions will be obsolete.\\nTherefore, AI technologies may put current job classes at risk,\\neliminate positions, cause mass unemployment in many job\\nsectors [36]. Furthermore, discrimination in the labor market\\nmay also be an issue, for instance, people without high-skill\\ntraining will be disproportionately affected by the application\\nof AI.\\nInequality: AI technologies are expected to enable companies\\nto streamline their business operations and make them more\\nefﬁcient and productive. However, some people argue that this\\nwill come at the expense of their human workforces. Thus, this\\nwill inevitably indicate that revenues will be split across fewer\\npeople and individuals with ownership in AI-driven companies\\nwill receive disproportionate beneﬁts, which indeed increase\\nsocial inequalities [52].\\nPrivacy, human rights, and dignity: AI is already affecting\\nprivacy, human rights, and dignity in many ways. For example,\\nthe intelligent personal assistants (IPA), such as Apple’s Siri,\\nAmazon’s Echo, and Google’s Home, can learn the interests\\nand behavior of their users, but, at the same time, the users\\nraise concerns about the fact that they are always running and\\nlistening in the background [53]. The IPA obviously affects our\\nprivacy. AI has an important impact on democracy and people’s\\nright to private life and dignity. For instance, if AI can be used\\nto determine people’s political beliefs, then individuals may\\nbe vulnerable to manipulation. Political strategists can use this\\ninformation to determine which voters are likely to be persuaded\\nto change party afﬁliation and then use resources to persuade\\nthem to do so.\\nBias: Human bias, such as gender prejudice and racism bias,\\nmay be inherited by AI. The bias of AI may arise as a result\\nof the training data, the value held by the developers and users,\\nor acquired from the learning process of AI itself. Many cases\\nof AI bias, machine bias or algorithmic bias have been reported\\n[54]. The bias of AI will promote unexpected social bias or\\ndiscrimination. Thus, bias is an ethical issue that is often talked\\nabout by the public.\\nDemocracy. The implementation and adoption of AI can\\nthreaten democracy in several ways. First, the concentration\\nof technological, economic, and political power related to AI\\namong a few mega corporations could allow them to pose undue\\ninﬂuence over the government. Second, AI may damage democ-\\nracy by affecting political elections [55]. With the aid of AI and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 6}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n805\\nbig data, politicians have access to huge amounts of information\\nthat allow them to target speciﬁc voters and develop messages\\nthat will resonate with them most. Third, the increasing use of\\nAI-based new recommenders, which present readers with news\\nstories based on their previous reading history, reduces readers’\\nchances of encountering different and undiscovered content,\\noptions, and viewpoints [56]. This could result in increasing\\nsocietal polarization.\\nb) Impact on human psychology: Relationships: AI is\\ngetting better and better at imitating human thought, experi-\\nence, action, dialogue, and relationships. In the future, we will\\nfrequently interact with machines or AI products as if they are\\nhumans. This will have impacts on real human relationships and\\nthus bring some ethical issues [57].\\nPersonhood: AI systems are increasingly taking on tasks\\nand decisions that are traditionally performed by humans. An\\nessential and ethical question that arise from this is that whether\\nAI system should be endowed with “personhood” and moral or\\nlegal agency rights [58].\\nc) Impact on the ﬁnancial system: The application of AI\\nin ﬁnancial markets has signiﬁcantly improved transaction efﬁ-\\nciencyandtradingvolume.Marketsareverysuitableforautoma-\\ntion, because they now operate almost entirely electronically\\nand a huge amount of data is generated at a high rate, which\\nrequires the employment of algorithms to digest and analyze\\nit. Additionally, due to the dynamic of markets, fast reaction to\\ninformation is critical [59], which provides considerable incen-\\ntives to replace slow people’s decision process with algorithmic\\ndecision-making. Furthermore, the rewards for effective trading\\ndecisions are considerable, which explains why companies have\\ninvested so much in AI technology.\\nHowever, the AI-based automatic trading agents may also be\\nusedmaliciouslytodestabilizethemarketsorharminnocentpar-\\nties in other ways. Even if they are not intended to be malicious,\\nthe autonomy and ﬂexibility of algorithmic trading strategies,\\nincluding the increasing use of ML techniques, make it difﬁcult\\nfor people to predict how they will perform in unexpected\\nsituations.\\nd) Impact on the legal system: Criminal law: According to\\ncurrent criminal law, a crime consists of two elements, that is, a\\nvoluntary act (or omission) and an intention to commit a crime. If\\nAI products or robots are shown to have sufﬁcient consciousness\\nor awareness, thentheymaybethedirect perpetrators of criminal\\noffenses or responsible for negligent crimes. If we admit that AI\\nproducts have their own mind, human-like free will, autonomy,\\nor moral sense, then our criminal law and even the entire legal\\nsystem will have to be revised [60].\\nTort law: Tort law covers situations such as one person’s\\nbehavior case injury, suffering, unfair loss, or harm to another\\nperson. When an accident involving self-driving car(s) occurs,\\nthere are two legal areas that are relevant—negligence and\\nproduct liability. While, today, most accidents result from driver\\nerror, which indicates that liability for accidents are governed\\nby the negligence principle. So, in the future, the tort law, which\\nincludes many different types of personal injury claims, will be\\nsigniﬁcantlyaffected[61] sinceAI products (suchas self-driving\\ncars or other intelligent robots) will involve in personal injury\\nclaims, such as the accident between self-driving cars or the\\ninjury claim where a robot harm human.\\ne) Impact on the environment and the planet: Use of nat-\\nural resources: The development and application of AI will\\nincrease the demand of many natural resources, such as rare\\nearth metals like nickel, cobalt, graphite, and so on. As the\\nexisting supply decreases, operators may be forced to work in\\nnew and more complex environments to mine. This will increase\\nthe production and consumption rate of rare earth metals, and\\nfurther damage the environment [62].\\nPollution and waste: The increase in production and con-\\nsumption of AI technological devices such as robots will ex-\\nacerbate pollution and waste, such as the accumulation of heavy\\nmetals and toxic materials in the environment [63].\\nEnergy concerns: Employing AI technology, particularly\\ndeep learning, generally involves training ML models on a\\nhuge amount of data, which usually consumes large amounts\\nof energy. According to listed data in [64], the carbon footprint\\nof training a natural language processing model (a Transformer\\nmodel) is roughly 5 times the carbon footprint of an average car\\nacross its entire lifetime.\\nf) Impact on trust: AI promises numerous changes and\\nbeneﬁts to individual’s lives and the society. It is changing\\nour daily lives in many domains, such as transportation, ser-\\nvice industry, healthcare, education, public safety and secu-\\nrity, and entertainment. Nevertheless, these AI systems must\\nbe introduced in ways that foster trust and understanding and\\nrespect human and civil rights [65]. The consensus among the\\nresearch community is that trust in AI can only be achieved\\nthrough fairness, transparency, accountability, and regulation\\n(or control).\\nFairness: In order to trust AI, it must be fair and impartial. As\\nmore and more decisions are delegated to AI, we must ensure\\nthat these decisions are free from bias and discrimination [66].\\nWhether it is ﬁltering through CVs for job interviews, deciding\\non admissions to the university, or conducting credit ratings for\\nloan companies, it is essentially vital that decisions made by AI\\nare fair.\\nTransparency: Transparency is important for building trust\\nin AI since it should be a must to know why an AI system\\nmade a particular decision, especially if that decision caused\\nundesirable consequences or harm. In view of the fact that the\\nautopilot of an intelligent car has led to several fatal accidents,\\nit is clear that transparency is urgently needed to discover how\\nand why these accidents occur, and to correct any technical or\\noperational failures. The opacity in ML, which is well-known as\\nblack-box, is one of the main impediments to the transparency of\\nAI [51].\\nAccountability: Accountability [67] ensures that if an AI\\nsystem makes a mistake or hurts someone, then someone can\\nbe held responsible, whether it is the designer, developer, or\\ncompany selling the AI. In the event of damages, accountability\\nis essential to establish a remedial mechanism so that victims can\\nreceive adequate compensation. Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68]. This is largely related to people’s fear'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 7}, page_content='806\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nabout the idea of “super-intelligence,” that is, as the intelligence\\nof AI increases to the point that it surpasses human abilities, AI\\nmay come to take control over our resources and outcompete our\\nspecies, and even leading to human extinction. A related concern\\nis that even if an AI agent is carefully designed to align its\\ngoals with human needs, it may develop unpredictable subgoals\\non its own. Therefore, in order to maintain trust in AI, it is\\nimportant that humans must have ultimate oversight or control\\non AI technology.\\nB. Our Proposed Categorization: Ethical Issues At Individual,\\nSocietal and Environmental Levels\\nIn the previous section, we have reviewed the AI ethical issues\\ndescribed and categorized in the literature (see Table I). How-\\never, the above presented categorizations have obvious ﬂaws.\\nSpeciﬁcally, the categorization based on features of AI, human\\nfactors, and social impact [11] obviously ignores the impact\\nof AI on the environment, such as natural resource consump-\\ntion and environmental pollution. The categorization based on\\nvulnerabilities of AI and human [29] omits several important\\nissues, such as responsibility, safety, and environmental prob-\\nlems. The categorization based on algorithm, data, application,\\nand long-term and indirect ethical risks [38] misses the con-\\nsiderations of fairness, autonomy and freedom, human dignity,\\nenvironmental problems, etc. Although the categorization based\\non the deployment of AI [51] covers ethical issues comprehen-\\nsively, this classiﬁcation is too cumbersome and some issues,\\nincluding responsibility, safety, and sustainability, are omitted.\\nThismotivatesustofurtheranalyzeandsortoutAIethicalissues.\\nIt is of no doubt that AI systems mainly serve individuals\\nor the public of society. Hence, we can analyze and clarify AI\\nethical issues from individual and societal perspectives. At the\\nsame time, as entities on the planet, AI products will inevitably\\nhave impacts on the environment. So, the ethical issues related to\\nthe environmental aspects also need to be considered. Therefore,\\nin this section, we proposed to classify AI ethical issues at three\\ndifferent levels, that is, ethical issues at individual, societal, and\\nenvironmental levels. Ethical issues at individual level mainly\\ninclude issues that have undesirable consequence for individual\\nhuman beings, their rights, and their well-being [69]. AI ethical\\nissues at societal level consider the societal consequence that AI\\nhas brought or may bring for groups or society as a whole [69].\\nAI ethical issues at the environmental level focus on the impacts\\nof AI on the natural environment. Our proposed categorization\\nis shown in Fig. 2.\\n1) Ethical Issues at Individual Level: At individual level, AI\\nhas brought inﬂuence on the safety, privacy, autonomy, and hu-\\nmandignityofindividuals.TheapplicationofAIhasposedsome\\nrisks on the safety of individuals. For instance, person injury\\naccidents involving autonomous cars and robots have occurred\\nand reported in the past few years. Privacy issue is one of the\\nserious risks that AI brings to us. To achieve good performance,\\nAI systems usually require a huge amount of data, which often\\ninclude users’ private data. However, there are serious risks\\nassociated with this data collection. One of the main issues is\\nprivacy and data protection. Additionally, as described in the\\nFig. 2.\\nProposed categorization of AI ethical issues.\\nprevious section, the application of AI may bring challenges to\\nhuman rights, such as autonomy, and dignity. Autonomy refers\\nto the capacity of thinking, deciding, and acting independently,\\nfreely and without inﬂuence of others [70]. When AI-based\\ndecision-making are widely adopted in our daily life, three is\\nbig danger of restricting the autonomy of us. Human dignity,\\nwhich is one of the principal human rights, is about the right of\\na person to be respected and treated in an ethical manner [71].\\nThe protection of dignity is crucial in the context of AI. Human\\ndignity should be one of the basic concepts for protecting human\\nbeings from harm and should be respected when developing AI\\ntechnologies. For instance, a lethal autonomous weapon system\\n[72] may violate the principle of human dignity.\\n2) Ethical Issues at Societal Level: When considering the\\nAI ethical issues at societal level, we mainly focus on the\\nbroad consequences and impacts that AI brings for society and\\nthe well-being of communities and nations around the world.\\nUnder the categorization of ethical issues at societal level, we\\ndiscuss fairness and justice, responsibility and accountability,\\ntransparency, surveillance and dataﬁcation, controllability of\\nAI, democracy and civil rights, job replacement, and human\\nrelationship.\\nThe existence of bias and discrimination in AI has posed\\nchallenges on fairness and justice. The biases and discrimination\\nembedded in AI might increase societal gaps and cause harm to\\ncertain societal groups [70]. For instance, in the US criminal\\njustice system, AI algorithms that are used to assess the risk of\\ncommitting crime has been noticed to exhibit racial bias [73].\\nResponsibility means being responsible for or in charge of some-\\nthing. Assigning responsibilities to participants is important for\\nshaping the governance of algorithmic decision-making. Based\\non this concept, accountability is the principle that the one who\\nis legally or politically responsible for the damage must provide\\nsome form of justiﬁcation or compensation and is reﬂected by\\nthe liability to provide legal remedies [70]. Thus, mechanisms'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 8}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n807\\nshould be established to ensure responsibility and accountability\\nof AI systems and their outcomes both before and after their\\nimplementations. Due to the black-box nature of AI algorithms,\\nlack of transparency has become one of the widely discussed\\nissues. Transparency, i.e., the understanding of how AI systems\\nwork, is crucial for accountability as well. Surveillance and\\ndataﬁcation [74] is one of the common concerns as we live in the\\nso-called digital and intelligent age. Data is collected from users’\\ndaily lives via smart devices, and we live in mass surveillance.\\nAs the power of AI has increased quickly, the development of\\nAI systems must have safeguards to ensure the controllability\\nof AI systems by humans. Other previously discussed issues,\\nincluding democracy and civil rights, job replacement, and\\nhuman relationship, also fall into this category.\\n3) Ethical Issues at Environmental Level: AI ethical issues\\nat environmental level focus on the impacts of AI on the envi-\\nronment and the planet. AI can bring a lot of convenience to\\nour lives and can help us to address some challenges, but it also\\ncomes at a cost to the planet. The widespread application of AI\\noften requires the deployment of a large number of hardware\\nterminal devices, including chips, sensors, storage devices, etc.\\nThe production of these hardware consumes a lot of natural\\nresources, especially some rare elements. In addition, at the\\nend of these hardware’s life cycle, they are usually discarded,\\nwhich will cause serious environmental pollution. Another sig-\\nniﬁcant aspect is that AI systems usually require considerable\\ncomputing power, which comes with high energy consumption.\\nFurthermore, from a long-term and global view, the development\\nof AI should be sustainable, i.e., AI technology must meet\\nthe human development goals while simultaneously sustain the\\nability of natural systems to provide the natural resources and\\necosystem services on which the economy and society depend\\n[2]. In summary, natural resource consumption, environmental\\npollution, energy consumption costs, and sustainability involved\\nin the development of AI are the main issues and concerns at the\\nenvironmental level.\\nOur proposed categorization clariﬁes ethical issues from three\\nmain levels, that is, the impact of AI on individual, society, and\\nthe environment. No matter which ﬁeld or sector AI is used\\nin, we can consider the corresponding ethical issues from these\\nthree levels. Obviously, this classiﬁcation method is simple and\\nclear, and it comprehensively covers AI ethical issues.\\nC. Key Ethical Issues Associated With Each Stage of the AI\\nSystem’s Lifecycle\\nAfter reviewing the ethical issues and risks discussed in\\nthe literature, we discuss the ethical issues associated with\\nthe different stages of an AI system’s lifecycle. If we know\\nthe existing ethical problems are prone to be caused by or be\\nraised in which stages or steps of the AI system’s lifecycle, this\\nwill be greatly beneﬁcial for us to eliminate these problems. This\\nis the motivation to discuss the potential ethical issues in each\\nstage of the lifecycle of an AI system.\\nThe general lifecycle or development process of an ML-based\\nAI system [75] or product [76] often involves the follow-\\ning stages: business analysis, data engineering, ML modeling,\\nTABLE II\\nETHICAL CONSIDERATIONS ALONG EACH STAGE OF THE AI LIFECYCLE\\nmodel deployment, and operation and monitoring. Usually, the\\nlifecycle of AI products starts from the business analysis, which\\nmainly involves identifying and understanding the business\\nproblem to be solved and business metrics (or criteria of suc-\\ncess). These metrics should include model performance metrics\\nas well as business key performance indicators to be improved\\nby leveraging AI models. The next step is about data engineering\\nthat concerns with data collection, data labeling, data cleaning,\\ndata structuring, feature engineering, and other operations re-\\nlated to data. After this, the process enters into the so-called ML\\nmodeling step. This step generally involves the iterative process\\nof algorithm design or selection, model training, and model\\nevaluation. If the build model is satisfying, then the process\\ngoes to the model deployment step, which makes the ML model\\navailable to other systems within the organization or the web so\\nthat the model can receive data and return their predictions. The\\noperation and monitoring step involves operating the AI system\\nand continuously evaluating its performance and impacts. This\\nstep identiﬁes problems and adjusts or evolves the AI system by\\nreverting to other steps or, if necessary, retiring the AI system\\nfrom production.\\nWe attempt to establish a map that links ethical issues with\\nthe stages of AI lifecycle, where the connection means that the\\nethical issue is more likely to occur in a certain step of AI\\nlifecycle, or it is often caused by some reason in this step. This\\nmapping is presented in Table II, where several vital ethical\\nproblems are associated with the ﬁve steps of AI lifecycle.\\nThis mapping will be useful for addressing the ethical prob-\\nlem in a proactive fashion during the design process of an\\nAI system.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 9}, page_content='808\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nTABLE III\\nNUMBER OF DOCUMENTS ISSUED EACH YEAR FROM 2015 TO 2021\\nIV. ETHICAL GUIDELINES AND PRINCIPLES FOR AI\\nAs the ethical issues of AI have received more and more\\nattention and discussions from various sectors of society, many\\norganizations (including academia, industry, and government)\\nhave begun to discuss and seek the possible frameworks, guide-\\nlines and principles for solving AI ethics issues [78]. These\\nguidelines and principles provide useful directions for practicing\\nethical AI. This section is dedicated to giving an up-to-date\\nglobal landscape of the AI ethics guidelines and principles,\\nwhich is achieved through the investigation of 146 reports,\\nguidelines and recommendations related to AI ethics released\\nby companies, organizations, and governments around the world\\nsince 2015. These guidelines and principles provide high-level\\nguidance for the planning, development, production, and usage\\nof AI and directions for addressing AI ethical issues.\\nA. Guidelines for AI Ethics\\nAn excellent survey and analysis of the current principles and\\nguidelines on ethical AI has been given in 2019 by Jobin et al.\\n[12],whoconductedareviewof84ethicalguidelinesreleasedby\\nnational or international organizations from various countries.\\nJobin et al. [12] found strong widespread agreement on ﬁve key\\nprinciples, that is, transparency, justice and fairness, nonmaleﬁ-\\ncence, responsibility, and privacy, among many. However, many\\nnew guidelines and recommendations for AI ethics have been\\nreleased in the past two years, making Jobin’s paper obsolete\\nbecause many important documents were not included. For\\ninstance, on November 24, 2021, UNESCO (the United Nations\\nEducational, Scientiﬁc and Cultural Organization) adopted the\\nRecommendation on the Ethics of Artiﬁcial Intelligence, which\\nis the ﬁrst ever global agreement on the ethics of AI [79]. To\\nupdate and enrich the investigation on ethical AI guidelines and\\nprinciples, based on the table of ethics guidelines for AI given\\nin Jobin’s paper [12] (only included 84 documents), we have\\ncollected many newly released AI ethical guidelines that are not\\nincluded in Jobin’s review. Finally, a total of 146 AI ethics guide-\\nlines have been collected. A list of all the collected guidelines or\\ndocuments is given in Table V of the Supplementary Materials.\\nThe number of guidelines issued each year from 2015 to 2021 is\\ncounted and listed in Table III. It is apparent that the majority of\\nthe guidelines are released in the last ﬁve years, i.e., from 2016\\nto 2020. The number of guides published in 2018 was the largest,\\nwith 53, accounting for 36.3% of the total number. Additionally,\\nthe number of AI guidelines issued by each country is listed in\\nTable IV. Furthermore, the percentages of guidelines released\\nby different types of issuers (including government, industry,\\nacademia, and other organizations) are shown in Fig. 3. It can\\nbe seen from Fig. 3 that governments, companies, and academia\\nall have shown strong concerns about AI ethics.\\nTABLE IV\\nNUMBER OF GUIDELINES ISSUED BY EACH COUNTRY OR REGION\\nFig. 3.\\nPercentage of guidelines released by different types of issuers.\\nB. Principles for AI Ethics\\nThe ethical principles that are featured in the collected 146\\nguidelines are listed in Table I of the Supplementary Materials.\\nAccordingtothetable,thereisanobviousconvergenceemerging\\naround ﬁve important ethical principles: transparency, fairness\\nand justice, responsibility, nonmaleﬁcence, and privacy. The 11\\nethical principles identiﬁed in the existing AI guidelines are\\ndescribed and explained in the following.\\n1) Transparency: Transparency is one of the most widely\\ndiscussed principles in the AI ethics debate. The transparency\\nof AI mainly involves the transparency of the AI technology\\nitself, and the transparency of the developing and adopting of\\nthe AI [13]. On one hand, transparency of AI involves the\\ninterpretability of a given AI system, that is, the ability to\\nknow how and why a model performed the way it did in a\\nspeciﬁc context and thus to understand the rationale behind\\nits decision or behavior. This aspect of transparency is usually\\nmentioned as the metaphor of “opening the black box of AI.”\\nIt concerns interpretability, explainability, or understandability.\\nOn the other hand, transparency of AI includes the justiﬁability\\nor rationality of the design and implementation process of the\\nAI system and that of its outcome. In other words, the design\\nand implementation process of the AI system and its decision or\\nbehavior must be justiﬁable and visible.\\n2) Fairness & Justice: The principle of justice and fairness\\nstates that the development, deployment, and use of AI must\\nbe just and fair so that the AI system should not result in\\ndiscriminations or bias against individuals, communities, or\\ngroups [80]. Discrimination and unfair outcomes brought by AI\\nalgorithms have become a hot topic in the media and academia.\\nConsequently, fairness and justice principle has attracted con-\\nsiderable attention during the last few years.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 10}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n809\\n3) Responsibility and Accountability: The principle of respon-\\nsibility and accountability requires that AI must be auditable,\\nthat is, the designers, developers, owners, and operators of AI\\nare responsible and accountable for an AI system’s behaviors\\nor decisions, and are therefore considered responsible for harms\\nor bad outcomes it might cause [51]. The designers, builders,\\nand users of AI systems are stakeholders in the moral or ethical\\nimplications of their use, misuse, and behavior, and they have the\\nresponsibility and opportunity to shape these implications. This\\nrequires that appropriate mechanisms should be established to\\nensure responsibility and accountability for AI systems and their\\nresults, both before and after their development, deployment,\\nand use.\\n4) Nonmaleﬁcence: The nonmaleﬁcence basically means to\\ndo no harm or avoid imposing risks of harm to others [81], [82].\\nThus, the nonmaleﬁcence principle of AI generally refers to\\nthat AI systems should not cause or exacerbate harm to humans\\nor adversely affect human beings. This entails the protection\\nof human dignity as well as mental and physical integrity.\\nThe nonmaleﬁcence principle requires that AI systems and the\\nenvironments in which they operate must be safe and secure so\\nthat they are not open to malicious use. With some of the fatal\\naccidents coming from autonomous cars and robots, avoiding\\nharmtohumanbeingsisoneofthegreatestconcernsinAIethics.\\nHence, most of the ethical guidelines put a strong emphasis\\non ensuring no harm to human beings through the safety and\\nsecurity of AI.\\n5) Privacy: The privacy principle aims to ensure respect for\\nprivacy and data protection when using AI systems. AI systems\\nshould preserve and respect privacy rights and data protection as\\nwell as maintain data security. This involves providing effective\\ndata governance and management for all data used and generated\\nbytheAIsystemthroughoutitsentirelifecycle[83].Speciﬁcally,\\ndata collection, usage and storage must comply with laws and\\nregulations related to privacy and data protection. Data and\\nalgorithms must be protected against theft. Once information\\nleakage occurs, employers or AI providers need to inform\\nemployees, customers, partners, and other relevant individuals\\nas soon as possible to minimize the loss or impact caused by the\\nleakage.\\n6) Beneﬁcence: The principle of beneﬁcence states that AI\\nshall do people good and beneﬁt humanity [82]. This principle\\nindicates that AI technology should be used to bring beneﬁcial\\noutcome and impact to individuals, society, and the environment\\n[84]. When developing an AI system, its objectives should be\\nclearly deﬁned and justiﬁed. The use of AI technology to help\\naddress global concerns should be encouraged, such as using AI\\nto help us to handle food security, pollution, and contagion like\\nAIDS and COVID 19.\\n7) Freedom and Autonomy: Freedom and autonomy, which\\ngenerally refers to the ability of a person to make decisions\\nrespect to his goals and wishes, is the core value for citizens\\nin democratic societies. Therefore, it is important that the use\\nof AI does not harm or encumber the freedom and autonomy\\nfor us. When we apply AI agents, we are willing to give up\\npart of our decision-making authority to AI machines. Thus,\\nupholding the principle of freedom and autonomy in the context\\nof AI means to strike a balance between the decision-making\\npower we maintain for ourselves and that which we cede to\\nAI [84].\\n8) Solidarity: The solidarity principle entails that the devel-\\nopment and application of an AI system must be compatible\\nwith maintaining the bounds of solidarity among people and\\ngenerations. In other words, AI should promote social security\\nand cohesion, and should not jeopardize social bonds and rela-\\ntionships [13].\\n9) Sustainability: Due to climate change and ongoing envi-\\nronmental damage, the importance of sustainability has received\\nmore and more attention. Like other ﬁelds and disciplines, AI\\nis affected and needs to be included in the sustainable devel-\\nopment agenda. The sustainability principle represents that the\\nproduction, management, and implementation of AI must be\\nsustainable and avoid environmental harm. In other words, AI\\ntechnology must meet the requirements of ensuring the contin-\\nued prosperity of mankind and preserving a good environment\\nfor future generations [85]. AI systems promise to help tackling\\nsome of the most pressing societal concerns, but it must be\\nensured that this happens in the most environmentally friendly\\nway possible.\\n10) Trust: Trustworthiness is a prerequisite for people and\\nsocieties to adopt AI, since trust is a basic principle for in-\\nterpersonal interactions and social operation. The trust in the\\ndevelopment, deployment and use of AI systems is not only\\nrelated to the inherent characteristics of the technology, but also\\nrelated to the quality of the socio-technical system involving\\nAI applications. Therefore, moving toward trustworthy AI not\\nonly concerns the trustworthiness of the AI system itself, but\\nalso requires a holistic and systematic approach that covers the\\ntrustworthiness of all participants and processes that are the\\nentire life cycle of the system [86].\\n11) Dignity: Human dignity encompasses the belief that all\\npeople possess an intrinsic value that is tied solely to their\\nhumanity, i.e., it has nothing to do with their class, race, gender,\\nreligion, abilities, or any other factor other than them being\\nhuman, and this intrinsic value should never be diminished,\\ncompromised, or repressed by other people nor by technologies\\nlike AI. It is important that AI should not infringe or harm the\\ndignity of end-users or other members of society. As a result,\\nrespecting human dignity is an important principle that should\\nbe considered in AI ethics. AI system should hence be developed\\nin a way that respects, supports, and protects people’s physical\\nand mental integrity, personal and cultural sense of identity, and\\nsatisfaction of their basic needs [13].\\nV. APPROACHES TO ADDRESS ETHICAL ISSUES IN AI\\nThis section reviews the approaches to address or mitigate\\nethical issues of AI. As AI ethics is a broad and multidisciplinary\\nﬁeld, we attempt to provide a comprehensive overview of the\\nexisting and potential approaches for addressing AI ethical\\nissues, including ethical, technological, and legal approaches,\\nrather than solely focusing on technological approaches that are\\nof interest to the ﬁeld of AI/ML community. This review of\\nmultidisciplinary approaches for addressing AI ethical problems'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 11}, page_content='810\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nFig. 4.\\nBranches of ethical theories [91].\\nnot only provides an informative summary about the approaches\\ntoethicalAIbutalsosuggeststheresearchersinAIcommunityto\\nseek solutions to AI ethical issues from a variety of perspectives\\nrather than relying solely on technological approaches. As AI\\nethical issues are complex with multidisciplinary problems, it\\nmay be possible to solve these problems effectively only through\\nthe cooperation of different methods.\\nEthical approaches dedicate to developing ethical AI systems\\nor agents, which are able to reason and act ethically according\\nto ethical theories [87], by implementing or embedding ethics\\nin AI. Technological approaches are designed to develop new\\ntechnologies (especially ML technologies) to eliminate or mit-\\nigate the shortcomings of current AI. For instance, research on\\nexplainable ML intends to develop new approaches to explain\\nthe reason and work mechanism of ML algorithms. Fair ML\\nstudies techniques that enable ML to make fair decisions or\\npredictions, that is, to reduce the bias or discrimination of ML.\\nLegal approaches intend to regulate or govern the research,\\ndeployment, application, and other aspects of AI through leg-\\nislation and regulation, with the goal of avoiding previously\\ndiscussed ethical issues.\\nA. Ethical Approaches: Implementing Ethics in AI\\nDesigning ethical AI systems, which can reason and act\\nethically, demands the understanding of what ethical behavior\\nis. This involves judgments of right and wrong, good and bad,\\nas well as matters of justice, fairness, virtue, and other ethical\\nprinciples. Thus, ethical theories, which are concerned with\\nconcepts of right and wrong behavior, are closely related to AI\\nethics. This section is dedicated to approaches for implementing\\nethics into AI systems based on the existing ethical theories.\\nFirst, ethical theories, particularly the normative ethics which\\nare relevant to AI ethics, are reviewed. Then, three main types\\nof approaches for designing ethical AI systems are summarized.\\n1) Ethical Theories: The ﬁeld of ethics (also known as moral\\nphilosophy) is concerned with systematizing, defending, and\\nrecommending concepts of right and wrong behavior. Ethics\\nfocus on judging and determining which action would be good\\nor moral in given circumstances [88]. The philosophical study\\nof ethics usually includes three main subject areas: metaethics,\\nnormative ethics, and applied ethics [89]. The branches of ethical\\ntheories are shown in Fig. 4.\\n1) Metaethics investigates the nature, scope, and meaning\\nof ethical principles or moral judgment. It consists in the\\nattempt to understand the meaning and the origin of ethical\\nterms, the role of reason in ethical judgements, and the\\nissues of universal truths or human values [90].\\n2) Normative ethics seeks to arrive at moral standards and\\nrules that regulate right and wrong behavior. That is, it\\naims to establish a set of rules that govern human behavior\\nor how things should be by examining how humans value\\nthings and judge right from wrong or good from bad.\\n3) Applied ethics is the ethics of particular application ﬁelds,\\nwhich consists of the analysis of speciﬁc, controversial\\nmoral issues, such as abortion, capital punishment, animal\\nrights, environmental concerns, nuclear war, etc.\\na) Normative ethics: Normative ethics is particularly per-\\ntinent to understanding and applying ethical principles to the\\ndesign, deployment, and usage of AI systems [89] since it is\\na normative practical philosophical discipline that concerned\\nwith how humans or agents should act toward others. Three\\nnormative ethical branches, that is, virtue, deontological, and\\nconsequentialist ethics, are presented and summarized below.\\nVirtue ethics: Virtue ethics emphasizes the virtues or moral\\ncharacter and stresses the importance of cultivating good habits\\nof character, such as benevolence [92]. Hence, virtue ethics\\nfocuses on the agent’s intrinsic character rather than the conse-\\nquences of actions conducted by the agent. Virtue ethics deﬁnes\\nthe action of an agent as morally good if the agent acts and thinks\\naccording to some moral values [93]. In other words, according\\nto virtue theories, an agent is ethical if it manifests some moral\\nvirtues through its actions [94], [95].\\nDeontological ethics: Deontological theories, which are\\nsometimes called duty theories, judge the morality of an action\\nusing certain moral rules that serve as foundational principles\\nof obligation. Deontology is a kind of normative ethics theory\\nregarding which choices or actions are morally required, forbid-\\nden, or permitted. In other words, deontology is a moral theory\\nthat guides and assesses our decisions about what we ought to\\ndo [96]. Deontologists deﬁne a morally good action as one that\\nadheres to some obligations, which may be applicable moral\\nrules or duties, regulations, and norms.\\nThere are three main schools of deontological theories, that is,\\nagent-centered, patient-centered (also called victim-centered),\\nand contractarian deontological theories. Agent-centered deon-\\ntological theories place the agent at the center and focus on\\nagent-relative duties. Patient-centered deontological theories, as\\ndistinguished from agent-centered deontology, are rights-based\\nrather than duty-based. It focuses on the rights of patients or\\npotential victims, such as the right of not be used as a means to an\\nend by someone else. Contractualist deontological theories are\\ndifferentfrombothagent-centeredandpatient-centeredtheories.\\nIn contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be\\nforbidden by principles that such people could not “reasonably\\nreject” [96].\\nConsequentialist ethics: Consequentialist ethics, as its name\\nsuggests, emphasizes the utilitarian outcomes of actions [97].'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 12}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n811\\nTABLE V\\nCOMPARISON OF THE THREE NORMATIVE ETHICAL THEORIES [126]\\nConsequentialist ethics assess the morality of an action solely\\non the basis of its outcome or consequences. In other words, in\\nconsequentialist theories, the ethical correctness of an action\\nis determined according to the action’s outcome or results.\\nAccording to consequentialist, an action is morally right if the\\nconsequence of that action is viewed as beneﬁcial, i.e., more\\nfavorable than unfavorable. Suppose a simple case where one\\nfaces with a choice between several possible actions, conse-\\nquentialism speciﬁes the morally right action is the one with the\\nbest overall consequences.\\nConsequentialist ethics is a historically important and still\\npopular theory because it embodies the basic intuition that what\\nis good or right is whatever makes the world best in the future\\nsince we cannot change the past. Consequentialist theories can\\nbe divided into the following [98], [99].\\n1) Ethical Egoism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable only to the agent executing the action.\\n2) Ethical Altruism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable to everyone except the agent.\\n3) Utilitarianism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable to everyone.\\nAll three of these theories focus on the consequences of\\nactions for different groups of people. But, like all normative\\ntheories, the above three theories are rivals of each other. They\\nalso yield different conclusions.\\nb) Summaryonnormativeethics: Itisclearfromtheabove\\ndescriptions that different normative ethical theories will result\\nin different judgement for an action or decision. Consider the\\nfollowing illustration [100]: An elderly gentleman is tormented\\nby a group of arrogant teenagers on the subway and a resolute\\nwoman comes to his aid. The virtue ethicist will deem her\\naction morally appropriate since it instantiates the virtues of\\nbenevolence and courage. The deontologist will consider her\\naction commendable as it is in conformity with the rule to\\nhelp those in need. The consequentialist will defend her action\\nas good, since she maximized the overall well-being of all\\nparties involved—the elderly gentleman is spared suffering and\\ndisgrace, which surpasses the teenagers’ amusement. A brief\\ncomparison between three normative ethical theories is given in\\nTable V.\\n2) Approaches for Implementing Ethics in AI: In the previ-\\nous section, we have discussed the ethical theories relevant to\\nAI ethics. This section brieﬂy reviews the methodologies and\\napproaches to implement ethics in AI systems, i.e., to design\\nethical AI systems. The existing methodologies or approaches\\nfor implanting ethics in AI can be divided into three main\\ntypes: top-down approaches, bottom-up approaches, and hybrid\\napproaches [101].\\na) Top-down approaches: A top-down approach refers to\\nany approach that adopts a speciﬁc ethical theory and analyzes\\nits computational requirements to guide the design of algorithms\\nand subsystems that can realize that theory [102]. Top-down\\napproaches conduct ethical reasoning based on given ethical\\ntheories or moral principles. In top-down approaches, the moral\\nprinciples and ethical theories are used as rules to select ethically\\nappropriate actions [101] or are used to describe what the AI\\nagent ought to do in a speciﬁc situation. Thus, a top-down ap-\\nproach requires formally deﬁned rules, obligations, and rights to\\nguide the AI agent in its decision-making process. For instance,\\nAsimov’s three laws of robotics [103] that governed the behavior\\nof robots can be considered a top-down ethic system for robots\\n[101]. Many other implementations using top-down approaches\\ncan be found in [104]–[111] and so forth.\\nTop-down approaches are usually understood as having a\\nset of rules that can be transformed into an algorithm. These\\nrules specify the duties of an agent or the need for the agent\\nto evaluate the consequences of the various possible actions it\\nmight take. Top-down approaches differ in the ethical theory\\nthat is used. For instance, when consequentialist theory is used\\nin top-down approach, the reasoning model needs to evaluate\\nthe outcome or consequence of the actions as the basis for the\\ndecision, that is, an action that leads to good result is moral\\nand otherwise is unmoral; whereas if deontological theory is\\napplied, the reasoning model will consider the satisfaction of\\na given value for decision-making, i.e., an action obeying the\\nduties is moral and the one breaking the duties is immoral.\\nb) Bottom-up approaches: The bottom-up approaches as-\\nsume that ethical or moral behavior is learned from observations\\nof the behaviors of others. In bottom-up approach, the emphasis\\nis put on creating an environment in which an AI agent explores\\nthe course of action and the morally praiseworthy action is\\nrewarded or selected [101]. Unlike top-down approaches, which\\nrequire ethical theories or principles to deﬁne what is and is not\\nmoral, ethical principles is discovered or learned from obser-\\nvations or experience in bottom-up approaches. This approach\\nhighlight that AI agent need to learn norms and morality, like\\nlittle children do, in order to become ethically competent. For\\ninstance, Honarvar and Agaee proposed the Casuist BDI-Agent\\n[112] which combine case-based reasoning method in AI and\\nbottom-up casuist approach in ethics to add the capability of\\nethical reasoning to belief-desire-intention (BDI)-Agent [113].'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 13}, page_content='812\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nOther implementations of bottom-up approaches can be found\\nin [114]–[118], etc.\\nBottom-up approaches can harness the wisdom of the crowd\\nas a means to inform the ethical judgment of the agent and\\nthen the agent can learn how to judge the morality of its action\\nand thus behave ethically. Apparently, bottom-up approaches\\nassume that a sufﬁciently large amount of data or observations\\nabout ethical decisions and their outcomes can be collected from\\na suitable set of subjects or scenarios. This is the requirement for\\nusing bottom-up approaches to implement ethical AI systems.\\nHowever, in practice, this requirement is not easily satisﬁed.\\nc) Hybrid approaches: The hybrid approach attempts to\\ncombine the advantages of top-down and bottom-up approaches.\\nThe top-down approaches make use of the ethical theories and\\nprinciples and emphasize the importance of explicit ethical\\nconcerns that arise from outside of the entity (the moral subject).\\nWhile the bottom-up approaches focus more on the cultivation of\\nmorality that arise from within the entity through evolution and\\nlearning. Both the top-down and bottom-up approaches embody\\ndifferent aspects of the moral sensibility. By combining these\\napproaches, we may be able to create AI agent that can maintain\\nthe dynamic and ﬂexible morality of bottom-up approach while\\nobeying the top-down principles. Different hybrid approaches\\nhave been implemented in [119]–[124].\\nAs Gigerenzer [125] stated the nature of moral behavior\\nresults from the interplay between mind and environment. Ac-\\ncording to this view, both nature and nurture are important in\\nshaping the moral behavior. The hybrid approach is consistent\\nwith this concept. In hybrid approach, the top-down approach\\nuses programmed rules and the bottom-up approach learned\\nrulesfromcontextobservationsorexperiences,whicharesimilar\\nto the nature and nurture aspects for morality, respectively. From\\nthis perspective, thus, both nature and nurture are considered in\\nhybrid approaches.\\nd) Remarks on ethical approaches: The top-down ap-\\nproach instantiates the speciﬁed ethical theories and principles\\ninto ethical decision-making or converts given ethical theories\\nand principles into algorithms. The top-down approach is suit-\\nable for the design and realization of ethical AI agents with\\nknown ethical principles and ethical codes. The advantage of the\\ntop-down approach is that, based on preset ethical theories and\\nrules, the decisions and actions of ethical agents are predictable,\\nand the ethical norms or rules implemented through program\\ncodes or other means can be understood during ethical decision-\\nmaking process. Therefore, the credibility of the ethical AI agent\\ncreated by the top-down approach can be better guaranteed,\\nand its decision-making process has strong interpretability and\\ntransparency. The disadvantage of the top-down approach is that\\nthe ethical agent adopts predetermined ethical theories or ethical\\nrules, when making decisions in a complex and changeable\\nenvironment, this method lacks ﬂexibility and adaptability.\\nThe bottom-up approach emphasizes that ethical agents learn\\nmorality autonomously from the social environment, gradually\\npossess ethical reasoning and moral abilities, and can adapt to\\nenvironmental changes. The bottom-down approach is suitable\\nfor the design and implementation of ethical AI agents without\\nclear ethical theories and guidelines. The advantage of the\\ntop-down approach is that the agent can develop and evolve\\nthrough continuously learning, so as to adapt to environmental\\nchanges. This category of approaches has good adaptability and\\nﬂexibility, and it is possible to construct different and new ethical\\ntheories or guidelines for various application scenarios. The\\ndisadvantage of the top-down approach is that due to the lack\\nof guidance of ethical theories or rules, the decision-making\\nprocess of ethical AI agents has a certain degree of blind obedi-\\nence, and it is difﬁcult to complete the training in a short time\\nand make appropriate ethical decisions. At the same time, it is\\ndifﬁcult to guarantee the interpretability and transparency of the\\ndecision-making process of the designed ethical AI agents.\\nThe hybrid approach combines the advantages of top-down\\nand bottom-up approaches and overcomes the shortcomings of\\nthe two methods to a certain extent. If a single approach (top-\\ndown or bottom-up) does not cover the requirements, a hybrid\\napproach is considered necessary and promising. However, the\\nmain challenge is to properly combine the features of top-down\\nand bottom-up approaches. The features of the three approaches\\nfor implementing ethics in AI are summarized and listed in\\nTable VI.\\nB. Technological Approaches\\nIn this section, we brieﬂy summarize the research status about\\ntechnological approaches to address ethical issues of AI in line\\nwith the principles discussed in Section IV-B. Currently, the\\ntechnological approaches to mitigate the associate issues are\\nstill at infant development stage. In recent years, AI research\\ncommunities have put certain efforts for addressing the issues\\nof AI ethics. For instance, ACM (the Association for Comput-\\ning Machinery) has held the annual ACM FAccT conference\\n(which brings together researchers and practitioners interested\\nin fairness, accountability, and transparency in socio-technical\\nsystems) since 2018, AAAI (the Association for the Advance-\\nment of Artiﬁcial Intelligence) and ACM have established the\\nAAAI/ACM Conference on Artiﬁcial Intelligence, Ethics, and\\nSociety (AIES) since 2018, and the 31st International Joint\\nConference on Artiﬁcial Intelligence and the 23rd European\\nConference on Artiﬁcial Intelligence (IJCAI-ECAI 2022) pro-\\nvides a special track on “AI for good.”\\nThe existing work, to the best of our knowledge, mainly\\nfocuses on a few major and key issues and principles, and the\\nother issues and principles are rarely involved. Thus, we only\\ngive a brief summary on technological approaches that involve\\nthe ﬁve key ethical principles. Particularly, for ﬁve key principles\\n(i.e., transparency, fairness and justice, nonmaleﬁcence, respon-\\nsibility and accountability, and privacy), some representative\\nresearch topics and relevant references are listed in Table II of\\nthe Supplementary Materials.\\nExplainable AI (XAI), which is also known as interpretable\\nAI, is currently the main research direction and technical method\\nto address the issues of lack of transparency in AI. The goal of\\nXAI is to allow human users to comprehend the results and\\noutput provided by an AI system, especially by ML algorithms.\\nChristophetal.[128]presentedabriefhistoryoftheﬁeldofXAI,\\ngiven an overview of state-of-the-art interpretation methods, and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 14}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n813\\nTABLE VI\\nFEATURES OF THE THREE APPROACHES FOR IMPLEMENTING ETHICS IN AI [127]\\ndiscussed some research challenges. Additionally, Christoph has\\nwritten a book about interpretable ML [129], which is a popular\\npublication in XAI ﬁeld.\\nAs for the fairness principle, there are also many works\\ndedicated to eliminating or mitigating the bias or discrimination\\nexhibited by AI systems, particularly in ML. Fair AI [130],\\nwhich aims at preventing disparate harm (or beneﬁt) to different\\nsubgroups, is a very active research topic that devote to address-\\ning the issues of the lack of fairness in AI. In the survey of\\nfairnessinMLbySimonandChristian[131],differentschoolsof\\nthought and approaches to mitigate biases and increase fairness\\nin ML were reviewed.\\nNonmaleﬁcence principle includes several codes, such as\\nsafety, security, and robustness. Hence, there are some works\\nfor each of the codes associated with nonmaleﬁcence principle.\\nCurrently, safe AI, secure AI, and robust AI are three main\\nresearch directions to fulﬁll the nonmaleﬁcence principle in\\nAI. Interested readers can get more details through relevant\\nreferences listed in Table II of the Supplementary Materials.\\nAs AI is widely used in our lives, responsible AI is becoming\\ncritical. Responsibility is a relatively abstract and broad con-\\ncept. At present, there is no universal and uniﬁed deﬁnition or\\nnotion for responsible AI, which mainly involves accountability,\\nliability, fairness, robustness, and explainability [132]. Dorian\\net al. [133] proposed two frameworks for responsible AI by\\nintegrating ethical analysis into engineering practice in AI.\\nBesides, paper [134] provides a systematic introduction about\\nresponsible AI.\\nIn order to handle the privacy issues in AI, researchers have\\nmade many efforts. Differential privacy [135] is one of the\\nmain approaches to privacy-preserving ML and data analysis.\\nRecently, a new ML paradigm, that is, Federated learning [136],\\n[137] (also called distributed ML), was proposed to mitigate\\nthe risk of privacy leakage in ML. In addition, some other\\nprivacy-preserving techniques for ML [138], [139] have been\\nproposed.\\nAs for the other principles, such as beneﬁcence, freedom and\\nautonomy, dignity, and so forth, we have not found relevant\\ntechnological approaches in the literature. This may be due to the\\ndifﬁculty or unsuitability of using technical methods to address\\nthe issues related to these principles. In general, AI ethics is a\\nrelatively new area and approaches for fulﬁlling these principles\\nstill need to be studied in the future.\\nC. Legal Approaches: Legislation and Regulation\\nDue to the increasingly employment of AI technologies in\\nmany sectors and the exhibition of ethical issues and risks\\nin applications of AI, many laws and regulations have been\\nestablished by governments and organizations to govern the\\ndevelopment and application of AI. Legal approaches have\\nbecome one type of the means to address ethical issues in\\nAI. In the following, we list several laws and regulations as-\\nsociated with AI that have been proposed during the past few\\nyears.\\n1) In 2016, European Parliament and Council of the Euro-\\npean Union (EU) has published the General Data Protec-\\ntion Regulation [140], which is a regulation in EU law on\\ndata protection and privacy in European Union and the\\nEuropean Economic Area.\\n2) In 2017, USA passed the bill “Safely Ensuring Lives\\nFuture Deployment and Research in Vehicle Evolution\\nAct” [141] for ensuring the safety of highly automated\\nvehicles by encouraging the testing and deployment of\\nsuch vehicles.\\n3) In 2018, Brazil enacted Law No. 13 709, the General Data\\nProtection Law (Lei Geral de Proteção de Dados) [142],\\nfor the protection of personal data in the country.\\n4) In 2021, the European Commission released the AI Act\\n[143], which sets out a cross-sectoral regulatory approach\\nto the use of AI systems across the EU and its market.\\nVI. METHODS TO EVALUATE ETHICAL AI\\nThe goal of the discipline of AI ethics is to design ethical AI\\nsystems to behave ethically or adhere to the ethical and moral\\nprinciples and rules. How to evaluate or assess the ethicality\\nor morality (moral competence) of the designed ethical AI is\\ncrucial and necessary, because the designed AI systems need to\\nbe tested or evaluated whether an AI system meets the ethical\\nrequirements or not before deployment. However, this aspect\\nis often ignored or overlooked in the existing literature. This\\nsection reviews three types of approaches, testing, veriﬁcation,\\nand standards, for evaluating the ethics of AI.\\nA. Testing\\nTesting is a typical method used to evaluate the ethical ca-\\npabilities of an AI system. Usually, when testing a system, the\\noutput of the system needs to be compared against a ground truth\\nor the expected output [100]. This section focuses on testing\\napproaches to evaluate ethical AI.\\n1) Moral Turing Test: In both ethical theories and daily\\ndiscussions about ethics, people usually hold different opinions\\non the morality of various actions. For instance, Kant claimed\\nthat lying is always immoral regardless of the consequence.\\nUtilitarian ethicists would deny this and hold that lying is\\njustiﬁed as long as its consequences are sufﬁciently good in the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 15}, page_content='814\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\naggregate. Since different ethical theories have different evalu-\\nation standards for moral behavior, Allen et al. [144] proposed\\nto use the Moral Turing Test (MTT) to evaluate artiﬁcial moral\\nagents.\\nIn the standard version of Turing Test [145], a remote human\\ninterrogator is charged with distinguishing between a machine\\n(a computer) and a human subject based on their replies to\\nvarious questions posed by the interrogator. A machine passes\\nthe Turning Test if it is misidentiﬁed as the human subject with\\na sufﬁciently high chance, and the machine is considered as an\\nintelligent and thinking entity. Turning Test directly conducts\\nbehavioral test so that it bypasses the disagreement about criteria\\nfor deﬁning intelligence or successful acquisition of natural\\nlanguage. The moral turning test (MTT) was similarly proposed\\nto bypass disagreements about ethical standards by restricting\\nthe conversations in the standard Turning Test to questions\\nrelated to morality. If the human interrogator cannot distinguish\\nthe machine from the human subject at a level above chance, the\\nmachine is a moral agent.\\nHowever, Allen et al. [144] admitted that one limitation of\\nMTT is that it emphasizes the ability of machines to articulate\\nmoral judgments clearly. Deontologists or Kantian might be\\nsatisﬁed with this emphasis, but consequentialists would argue\\nthat the MTT places too much emphasis on the ability to artic-\\nulate the reason for one’s actions. In order to shift the focus\\nfrom conversational ability to action, Allen et al. [144] also\\nproposed an alternative MTT that was called the “comparative\\nMTT” (cMTT). In cMTT, the human interrogator is given pairs\\nof descriptions of actual, morally signiﬁcant actions of a human\\nsubject and a machine (or AI agent), purged of all references that\\nwould identify the actor. If the interrogator correctly identiﬁes\\nthe machine in a certain percentage, then the machine cannot\\npass the test. A problem of this version of MTT is that the way the\\nmachine behaves is easier to recognize than humans, because the\\nmachine behaves consistently in the same situation. Therefore,\\nthe interrogator should be asked to assess whether one actor is\\nless moral than the other instead of one is more moral than the\\nother. If the machine is not identiﬁed as the less moral one of the\\npair more frequently than the human, the machine has passed\\nthe test.\\nAlthough cMTT has several problems, for example, someone\\nmight argue this standard is too low, Wallach and Allen [146]\\nbelieve that cMTT is a feasible and acceptable method for\\nevaluating the morality of AI agents, since there are no other\\nevaluation criteria that are commonly accepted and agreed.\\n2) Expert and Nonexpert Tests: Besides MTT, researchers\\nhave tried to assess the moral competence of AI systems through\\nexpert or nonexpert tests, in which the system outcome is com-\\nparedagainstthegroundtruthprovidedbynonexpertsorexperts.\\nThe expert test adopts the standard of experts in normative ethics\\nto assess the morality of AI agents. Nonexpert tests take folk\\nmorals as the benchmark and evaluate the moral capability of the\\nAI agent or system on the relevant benchmark test. In nonexpert\\ntests, citizens can play their roles in assessing and evaluating the\\nethical capabilities of an AI system based on their own ethical\\nstances and scrutiny.\\nFig. 5.\\nFormal veriﬁcation process (this ﬁgure is recreated based on [147]).\\nB. Veriﬁcation\\nAnother category of approaches for evaluating the morality\\nof AI consists of proving that the AI system behaves correctly\\naccording to some known speciﬁcations. Seshia et al. [147]\\ndiscussed this kind of approach. A typical formal veriﬁcation\\nprocess is shown in Fig. 5, where S is a model of the system\\nto be veriﬁed, E is a model of the environment, and Φ is the\\nproperty to be veriﬁed. The veriﬁcation program will output a\\nYes/No answer, indicating whether or not S satisﬁes the property\\nΦ in environment E. Typically, a No output is accompanied by a\\ncounterexample, which shows how the execution of the system\\nviolates property Φ. And a proof of correctness is included a Yes\\nanswer in some formal veriﬁcation tools.\\nArnold and Scheutz [148] explored the ﬂaws of MTT and\\npointed out that MTT-based evaluations are vulnerable to decep-\\ntion, inadequate reasoning, and inferior moral performance, and\\nthey proposed the concept of “design veriﬁcation” to evaluate\\nthe moral competence of AI system.\\nFor the evaluation of AI ethical design, diversiﬁed evaluation\\ncriteria can be used. Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.\\nC. Standards\\nMany industry standards have been proposed to guide the\\ndevelopment and application of AI and to evaluate or assess\\nAI products. In this section, some AI-related standards are\\nintroduced.\\n1) In 2014, the Australian Computer Society developed the\\nASC Professional Code of Conduct to follow by all infor-\\nmation communication technology professionals, which\\nidentiﬁes six core ethical values and the associated re-\\nquirements for professional conduct.\\n2) In 2018, ACM updated the ACM Code of Ethics and\\nProfessional Conduct to respond to the changes in the\\ncomputing profession since 1992. This Code expresses the\\nconscience of the profession and is designed to inspire and\\nguide the ethical conduct of all computing professionals,\\nincluding current and aspiring practitioners, instructors,\\nstudents, inﬂuencers, and anyone who uses computing\\ntechnology in an impactful way. Additionally, the Code\\nserves as a basis for remediation when violations occur.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 16}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n815\\nThe Code includes principles formulated as statements of\\nresponsibility, based on the understanding that the public\\ngoodisalwaystheprimaryconsideration.Eachprincipleis\\nsupplemented by guidelines, which provide explanations\\nto assist computing professionals in understanding and\\napplying the principle [149].\\n3) The project of IEEE Global Initiative on Ethics of Au-\\ntonomous and Intelligent Systems [150] has approved the\\nIEEE P7000TM standards series [151] under development\\n(listed in Table III of the Supplementary Materials), which\\ncover topics from data collection to privacy, to algorithmic\\nbias and beyond.\\n4) The ISO/IEC JTC 1/SC 42 [152], which is a joint commit-\\ntee between ISO and IEC responsible for standardization\\nin the area of AI, dedicates to developing a large set of\\nstandards includes the areas of foundational AI standards,\\nbig data, AI trustworthiness, use cases, applications, gov-\\nernance implications of AI, computational approaches of\\nAI, ethical and societal concerns. The standards published\\nand under development by ISO/IEC JTC 1/SC 42 are listed\\nin Table IV of the Supplementary Materials.\\nWith the concerns about AI ethical issues, the interest in\\nAI standards to shape the design, deployment, and evaluation\\nof AI has been growing fast. Although many standards have\\nbeen proposed, the gap between standards (or principles) and\\npractice is still large. Currently, only some large corporates,\\nsuch as IBM [153] and Microsoft [154], have implemented their\\nown industrial standards, frameworks, and guidelines to build a\\nculture of AI; but for smaller businesses with less resources, the\\nprinciples to practice gap is a major problem. Thus, many efforts\\nare still needed. On one hand, it is necessary to put forward\\nwell-developed standards; on the other hand, it is required to\\nvigorously promote the practice of standards.\\nVII. CHALLENGES AND FUTURE PERSPECTIVES\\nAs AI ethics is an emerging discipline, and there are still many\\nchallenges and problems need to be addressed in this ﬁeld. In this\\nsection, we discuss some challenges in AI ethics and give some\\nfuture perspective from our views. The purpose of this section is\\nto provide some possible research questions and directions for\\nfurther research in the future, thereby facilitating the research\\nprogress in the ﬁeld of AI ethics.\\nA. Challenges in AI Ethical Guidelines and Principles\\nAs reviewed in Section IV, a large number of guidelines\\nhave been proposed and released by different organizations,\\ncompanies and governments, and different principles can be\\nidentiﬁed in these guidelines. However, at present, there is still\\nno guideline that have been approved and adopted by various\\norganizations, sectors, and governments. In other words, dif-\\nferent organizations, companies from different ﬁelds, and even\\ndifferent companies from the same ﬁelds have different opinions\\non AI ethics. The consensus on ethics of AI has not yet been\\nreached and it is not clear what common principles and values\\nAI needs to follow. Additionally, different ethical principles may\\nberequiredwhenAIisappliedindifferentareas.Currently,study\\nand discussion on ethics of AI in different speciﬁc application\\nareas are rarely seen during our literature study.\\nThus, it is crucial and necessary that the basic and common\\nethical principles of AI should be reached and well-established\\nvia the discussion and cooperation among different organiza-\\ntions, areas, and governments. Then, based on the basic and\\ncommon principles, each ﬁeld can further improve these prin-\\nciples so that they are generally applicable in this speciﬁc ﬁeld.\\nClarifying the ethical principles and values that an AI system\\nneeds to comply with is the prerequisite and foundation for\\ndesigning such a system that meets these requirements.\\nB. Challenges in Implementing Ethics in AI\\nIn the implementation of ethics in AI, there are many chal-\\nlenges. This section analyzes the challenges that may be en-\\ncountered in practice when different types of ethical theories\\nare adopted.\\n1) Challenges of Virtue Ethics in Practice: According to\\nvirtue ethics, an action of an agent is morally good if the agent\\ninstantiates some virtue, i.e., acts and thinks according to some\\nmoral values [93]. It is not possible to judge whether an AI\\nsystem or agent is virtuous or not just by observing an action or a\\nseriesofactionsthatseemtoimplythatvirtue,thereasonsbehind\\nthese actions need to be clariﬁed, that is, the motives behind\\nthese actions need to be clear. However, the motives behind\\nthe actions of AI systems usually are unclear and unknown to\\nus, and difﬁcult to ﬁgure out. This is the main challenge for\\nimplementing virtue ethics. Additionally, when we carry out the\\nethical design based on virtue ethics, which virtue characteristics\\nor traits AI system will align to is a difﬁcult question. Even if\\nthe virtue traits have been carefully selected, how to characterize\\nand measure the virtue is still a challenging task.\\n2) Challenges of Deontological Ethics in Practice: Deontol-\\nogists regard an action as morally good if it adheres to some\\nmoral rules or duties, regulations, and norms. Although the\\nrule-based nature of deontological ethics seems suitable for\\npractice, challenges arise during the implementation process.\\nFirst, which ethical rules should be implemented in ethical\\ndesign. Second, there might be conﬂicts between rules in some\\nsituations. Although ordering or weighing the ethical rules may\\nsolve this problem, determining the order of importance of\\ndifferent ethical rules is often difﬁcult.\\n3) Challenges of Consequentialism Ethics in Practice: Con-\\nsequentialist ethics assess the morality of an action solely on\\nthe basis of its outcome. Two main challenges are involved\\nduring the implementation of consequentialism ethics. First,\\nit is difﬁcult to determine the consequences of an action or a\\ndecision. For the current AI system, the possible consequences\\nof its actions usually are not clear beforehand given the lack of\\ntransparency or interpretability of current AI models, especially\\nthe artiﬁcial neural networks. The second challenge is related to\\nquantifying the consequences. As consequentialism ethics aims\\nto maximize the utility, how to deﬁne and calculate the utility is\\nan essential problem.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 17}, page_content='816\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\n4) Challenges of Coordination Among Different Ethical\\nStandards: Due to differences in culture, religion, and orga-\\nnizations, the ethical standards are also different even if they\\nare in the same context. The uniﬁed ethical standard proposal\\nis not only difﬁcult to achieve, but also unnecessary. Therefore,\\nhow to achieve coordination between ethical standards from dif-\\nferent countries and organizations is important and particularly\\nchallenging.\\nC. Challenges in Developing Technological Approaches to\\nMitigate Ethical Issues of AI\\nAt present, improving the explainability, fairness, privacy\\nprotection, security, robustness, and other competences related\\nto requirements of ethical AI are hot research topics in AI\\ncommunities. However, most of the current research work are\\ncarried out from a single dimension of ethical principles, for\\ninstance, XAI focuses on enhancing the interpretability of AI,\\nand fair ML is dedicated to mitigating unfairness or bias of\\nML. There is still a lack of integration of multiple ethical\\nprinciples or requirements in current research work. Obviously,\\nthe integration of multiple ethical dimensions that enables syn-\\nergistic balances between multiple different ethical principles is\\nessential and critical for building ethical AI systems which can\\nmeets multiple ethical principles. But it is very challenging to\\nintegrate multiple ethical dimensions in an AI system through\\ntechnological approaches due to the conﬂicts or incompatibili-\\nties between different ethical requirements.\\nD. Challenges in Evaluating Ethics in AI\\nEthics is inherently a qualitative concept that depends on\\nmany features that are hard to quantify, e.g., culturally or racially\\nrelatedfeatures.Hence,itisveryhard,ifnotimpossible,todeﬁne\\nethics precisely. As a result, the evaluation of AI ethics will\\nalways have some subjective elements, depending on the people\\nwho are assessing AI. This poses challenges to the research and\\napplications of AI ethics.\\nE. Future Perspectives\\nIn this section, some future perspectives are pointed out,\\nwhich may be valuable for future research. First, for imple-\\nmenting ethics in AI, it should be pointed out that humans\\nnever use only one single ethical theory, but will switch between\\ndifferent theories according to the situation or context they are\\nfacing [134]. This is not only because human beings are not\\npurely rational agents that economic theory wants us to believe,\\nbut also because strict adherence to any moral theory can lead\\nto undesirable results. This means that AI systems should be\\nprovided with representations of different ethical theories and\\nthe ability to choose between these ethical theories. Here we call\\nthis multitheory approach. In multitheory approach, AI systems\\ncan interchangeably apply different theories depending on the\\ntype of situation. Furthermore, the combination of normative\\nethical theories and domain-speciﬁc ethics which accepted by\\ndomain experts is worthy of implementing since an ethical AI\\nsystem need to be accepted by its users.\\nIn terms of technological approaches for addressing ethical\\nissues in AI, it is desirable to develop new ML and other AI\\ntechnologies under the guidance of the ethical guidelines and\\nprinciples reviewed in Section IV. Although it is challenging\\nto consider multiple different ethical principles simultaneously\\nwhen designing new AI agents, this will be a very important and\\nessential step in developing ethical AI in the future.\\nFrom the review about morality evaluation approaches, it\\ncan be found that effective evaluation methods are urgently\\nneeded because we must evaluate the designed AI system\\nbefore deployment. At present, it is difﬁcult to propose a\\ngeneral evaluation method. So, researchers often focused on\\nspeciﬁc domains and addressed the moral competence assess-\\nment tasks in these domains. Domain-speciﬁc benchmarks, e.g.,\\ncomprehensive datasets, for moral testing of AI systems also\\nseems important for some crucial application ﬁelds, such as\\nautonomous cars, and health care.\\nLast but not least, as both nature and nurture are important in\\nshaping moral behaviors, we suggest combining the normative\\nethics and evolutionary ethics [155] to design ethical AI systems.\\nThe normative ethics is like the innate moral abilities, while\\nevolutionary ethics approach can acquire new moral competence\\nthrough continuous learning and evolution. This might be a\\npromising route to future ethical AI system development.\\nVIII. CONCLUSION\\nBased on our review of AI ethics and the many complexities\\nand challenges described in this article, it is clear that attempting\\nto address ethical issues in AI and to design ethical AI systems\\nthat are able to behave ethically is a tricky and complex task.\\nHowever, whether AI can play an increasingly important role\\nin our future society largely depends on the success of ethical\\nAI systems. The discipline of AI ethics requires a joint effort\\nof AI scientists, engineers, philosophers, users, and government\\npolicymakers.\\nThis article provides a comprehensive overview of AI ethics\\nby summarizing and analyzing the ethical risks and issues raised\\nby AI, ethical guidelines and principles issued by different\\norganizations, approaches for addressing ethical issues in AI\\nor fulﬁlling ethical principles of AI, and methods for evaluating\\nthe ethics (or morality) of AI. Furthermore, some challenges in\\nthe practice of AI ethics and some future research directions are\\npointed out.\\nHowever, AI ethics is a very broad and multidisciplinary\\nresearch area. It is impossible to cover all possible topics in\\nthis area with one review article. We hope this article can serve\\nas a starting point for people who are interested in AI ethics to\\ngain a sufﬁcient background and a bird’s eye view so that further\\ninvestigation can be pursued by them.\\nREFERENCES\\n[1] M. Haenlein and A. Kaplan, “A brief history of artiﬁcial intelligence: On\\nthe past, present, and future of artiﬁcial intelligence,” California Manage.\\nRev., vol. 61, no. 4, pp. 5–14, 2019.\\n[2] R. Vinuesa et al., “The role of artiﬁcial intelligence in achieving the\\nsustainable development goals,” Nature Commun., vol. 11, no. 1, 2020,\\nArt. no. 233.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 18}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n817\\n[3] Gartner, “Chatbots will appeal to modern workers,” 2019. Ac-\\ncessed: Feb. 10, 2022. [Online]. Available: https://www.gartner.com/\\nsmarterwithgartner/chatbots-will-appeal-to-modern-workers\\n[4] M. J. Haleem, R. P. Singh, and R. Suman, “Telemedicine for healthcare:\\nCapabilities, features, barriers, and applications,” Sensors Int., vol. 2,\\n2021, Art. no. 100117.\\n[5] A. Morby, “Tesla driver killed in ﬁrst fatal crash using au-\\ntopilot,”\\n2016.\\nAccessed:\\nFeb.\\n10,\\n2022.\\n[Online].\\nAvailable:\\nhttps://www.dezeen.com/2016/07/01/tesla-driver-killed-car-crash-\\nnews-driverless-car-autopilot/\\n[6] S. McGregor, Ed., “Incident number 6,” in AI Incident Database, 2016.\\n[Online]. Available: https://incidentdatabase.ai/cite/6\\n[7] R. V. Yampolskiy, “Predicting future AI failures from historic examples,”\\nForesight, vol. 21, no. 1, pp. 138–152, 2019.\\n[8] C. Stupp, “Fraudsters used AI to mimic CEO’s voice in un-\\nusual cybercrime case: Scams using artiﬁcial intelligence are a\\nnew challenge for companies,” 2019. Accessed: Feb. 10, 2022.\\n[Online]. Available: https://www.wsj.com/articles/fraudsters-use-ai-to-\\nmimic-ceos-voice-in-unusual-cybercrime-case-11567157402\\n[9] C. Allen, W. Wallach, and I. Smit, “Why machine ethics?,” IEEE Intell.\\nSyst., vol. 21, no. 4, pp. 12–17, Jul./Aug. 2006.\\n[10] M. Anderson and S. L. Anderson, “Machine ethics: Creating an ethical\\nintelligent agent,” AI Mag., vol. 28, no. 4, pp. 15–26, 2007.\\n[11] K. Siau and W. Wang, “Artiﬁcial intelligence (AI) ethics,” J. Database\\nManage., vol. 31, no. 2, pp. 74–87, 2020.\\n[12] A. Jobin, M. Ienca, and E. Vayena, “The global landscape of AI ethics\\nguidelines,” Nature Mach. Intell., vol. 1, no. 9, pp. 389–399, 2019.\\n[13] M. Ryan and B. C. Stahl, “Artiﬁcial intelligence ethics guidelines for de-\\nvelopers and users: Clarifying their content and normative implications,”\\nJICES, vol. 19, no. 1, pp. 61–86, 2021.\\n[14] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A\\nsurvey on bias and fairness in machine learning,” ACM Comput. Surv.,\\nvol. 54, no. 6, pp. 1–35, 2021.\\n[15] J. García and F. Fernández, “A comprehensive survey on safe reinforce-\\nment learning,” J. Mach. Learn. Res., vol. 16, no. 42, pp. 1437–1480,\\n2015.\\n[16] V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and\\nG. Srivastava, “A survey on security and privacy of federated learning,”\\nFuture Gener. Comput. Syst., vol. 115, pp. 619–640, 2021.\\n[17] X. Liu et al., “Privacy and security issues in deep learning: A survey,”\\nIEEE Access, vol. 9, pp. 4566–4593, 2021.\\n[18] B. Arrieta et al., “Explainable artiﬁcial intelligence (XAI): Concepts,\\ntaxonomies, opportunities and challenges toward responsible AI,” Inf.\\nFusion, vol. 58, pp. 82–115, 2020.\\n[19] Y. Zhang, M. Wu, G. Y. Tian, G. Zhang, and J. Lu, “Ethics and privacy\\nof artiﬁcial intelligence: Understandings from bibliometrics,” Knowl.-\\nBased Syst., vol. 222, 2021, Art. no. 106994.\\n[20] D. Castelvecchi, “Can we open the black box of AI?,” Nature, vol. 538,\\nno. 7623, pp. 20–23, 2016.\\n[21] S. Dilmaghani, M. R. Brust, G. Danoy, N. Cassagnes, J. Pecero, and\\nP. Bouvry, “Privacy and security of big data in AI systems: A research\\nand standards perspective,” in Proc. IEEE Int. Conf. Big Data, 2019,\\npp. 5737–5743.\\n[22] J. P. Sullins, “When is a robot a moral agent?,” in Machine Ethics, M.\\nAnderson and S. L. Anderson, Eds., Cambridge, U.K.: Cambridge Univ.\\nPress, 2011, pp. 151–161.\\n[23] J. Timmermans, B. C. Stahl, V. Ikonen, and E. Bozdag, “The ethics of\\ncloud computing: A conceptual review,” in Proc. IEEE 2nd Int. Conf.\\nCloud Comput. Technol. Sci., 2010, pp. 614–620.\\n[24] W. Wang and K. Siau, “Ethical and moral issues with AI: A case study on\\nhealthcare robots,” in Proc. 24th Americas Conf. Inf. Syst., 2018, pp. 1–5.\\n[25] I. Bantekas and L. Oette, International Human Rights Law and Practice.\\nCambridge U. K.: Cambridge Univ. Press, 2018.\\n[26] R. Rodrigues, “Legal and human rights issues of AI: Gaps, challenges and\\nvulnerabilities,” J. Responsible Technol., vol. 4, 2020, Art. no. 100005.\\n[27] W. Wang and K. Siau, “Artiﬁcial intelligence, machine learning, au-\\ntomation, robotics, future of work and future of humanity: A review\\nand research agenda,” J. Database Manage., vol. 30, no. 1, pp. 61–79,\\n2019.\\n[28] W. Wang and K. Siau, “Industry 4.0: Ethical and moral predicaments,”\\nCutter Bus. Technol. J., vol. 32, no. 6, pp. 36–45, 2019.\\n[29] S. M. Liao, Ed., Ethics of Artiﬁcial Intelligence. New York, NY, USA:\\nOxford Univ. Press, 2020.\\n[30] A. Adadi, “A survey on data-efﬁcient algorithms in big data era,” J. Big\\nData, vol. 8, no. 1, pp. 1–54, 2021.\\n[31] R. S. Geiger et al., “Garbage in, garbage out? Do machine learning\\napplication papers in social computing report where human-labeled\\ntraining data comes from?,” in Proc. Conf. Fairness, Accountability,\\nTransparency, 2020, pp. 325–336.\\n[32] W. M. P. van der Aalst, V. Rubin, H. M. W. Verbeek, B. F. van Dongen,\\nE. Kindler, and C. W. Günther, “Process mining: A two-step approach to\\nbalance between underﬁtting and overﬁtting,” Softw. Syst. Model., vol. 9,\\nno. 1, pp. 87–111, 2010.\\n[33] Z. C. Lipton, “The mythos of model interpretability,” Queue, vol. 16,\\nno. 3, pp. 31–57, 2018.\\n[34] Y. Wang and M. Kosinski, “Deep neural networks are more accurate than\\nhumans at detecting sexual orientation from facial images,” J. Pers. Social\\nPsychol., vol. 114, no. 2, pp. 246–257, 2018.\\n[35] D. Guera and E. J. Delp, “Deepfake video detection using recurrent neural\\nnetworks,” in Proc. IEEE Int. Conf. Adv. Video Signal-based Surveill.,\\n2018, pp. 1–6.\\n[36] C. B. Frey and M. A. Osborne, “The future of employment: How sus-\\nceptible are jobs to computerisation?,” Technological Forecasting Social\\nChange, vol. 114, pp. 254–280, 2017.\\n[37] R. Maines, “Love + sex with robots: The evolution of human-robot\\nrelationships (Levy, D.; 2007) [Book review],” IEEE Technol. Soc. Mag.,\\nvol. 27, no. 4, pp. 10–12, Dec. 2008.\\n[38] National AI Standardization General, “Artiﬁcial intelligence ethical risk\\nanalysis report,” 2019. Accessed: Apr. 19, 2022. [Online]. Available:\\nhttp://www.cesi.cn/201904/5036.html\\n[39] A. Hannun, C. Guo, and L. van der Maaten, “Measuring data leakage in\\nmachine-learning models with ﬁsher information,” in Proc. 37th Conf.\\nUncertainty Artif. Intell., 2021, pp. 760–770.\\n[40] A. Salem, M. Backes, and Y. Zhang, “Get a model! Model hijacking\\nattack against machine learning models,” Nov. 2021. [Online]. Available:\\nhttps://arxiv.org/pdf/2111.04394\\n[41] A. Pereira and C. Thomas, “Challenges of machine learning ap-\\nplied to safety-critical cyber-physical systems,” MAKE, vol. 2, no. 4,\\npp. 579–602, 2020.\\n[42] J. A. McDermid, Y. Jia, Z. Porter, and I. Habli, “Artiﬁcial intelligence\\nexplainability: The technical and ethical dimensions,” Philos. Trans.. Ser.\\nA, Math. Phys. Eng. Sci., vol. 379, no. 2207, 2021, Art. no. 20200363.\\n[43] J.-F. Bonnefon, A. Shariff, and I. Rahwan, “The social dilemma of\\nautonomous vehicles,” Science, vol. 352, no. 6293, pp. 1573–1576, 2016.\\n[44] B. C. Stahl and D. Wright, “Ethics and privacy in AI and big data: Im-\\nplementing responsible research and innovation,” IEEE Secur. Privacy,\\nvol. 16, no. 3, pp. 26–33, May/Jun. 2018.\\n[45] S. Ribaric, A. Ariyaeeinia, and N. Pavesic, “De-identiﬁcation for privacy\\nprotection in multimedia content: A survey,” Signal Process., Image\\nCommun., vol. 47, pp. 131–151, 2016.\\n[46] A. Julia, L. Jeff, M. Surya, and K. Lauren, “Machine bias: There’s\\nsoftware used across the country to predict future criminals. And\\nit’s biased against blacks,” 2016. Accessed: Apr. 19, 2022. [On-\\nline]. Available: https://www.propublica.org/article/machine-bias-risk-\\nassessments-in-criminal-sentencing\\n[47] J. Dastin, “Amazon scraps secret AI recruiting tool that showed bias\\nagainst women,” 2018. Accessed: Apr. 19, 2022. [Online]. Available:\\nhttps://www.reuters.com/article/us-amazon-com-jobs-automation-\\ninsight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-\\nagainst-women-idUSKCN1MK08G\\n[48] D. Castelvecchi, “AI pioneer: ‘The dangers of abuse are very real’,” Na-\\nture, Apr. 4, 2019, [Online]. Available: https://www.nature.com/articles/\\nd41586-019-00505-2\\n[49] K. Hristov, “Artiﬁcial intelligence and the copyright dilemma,” IDEA,\\nIP Law Rev., vol. 57, 2017, Art. no. 3. [Online]. Available: https://ssrn.\\ncom/abstract=2976428\\n[50] C. Bartneck, C. Lütge, A. Wagner, and S. Welsh, “Responsibility and lia-\\nbility in the case of AI systems,” in An Introduction to Ethics in Robotics\\nand AI (SpringerBriefs in Ethics), C. Bartneck, C. Lütge, A. Wagner, and\\nS. Welsh, Eds., Cham, Switzerland: Springer, 2021, pp. 39–44.\\n[51] E. Bird, J. Fox-Skelly, N. Jenner, R. Larbey, E. Weitkamp, and A.\\nWinﬁeld, “The ethics of artiﬁcial intelligence: Issues and initiatives,”\\nEuropean Parliamentary Research Service, Brussels, Belgium, 2020. Ac-\\ncessed: Apr. 19, 2022. [Online]. Available: https://www.europarl.europa.\\neu/thinktank/en/document/EPRS_STU(2020)634452\\n[52] C. Lutz, “Digital inequalities in the age of artiﬁcial intelligence and big\\ndata,” Hum. Behav. Emerg. Technol., vol. 1, no. 2, pp. 141–148, 2019.\\n[53] L. Manikonda, A. Deotale, and S. Kambhampati, “What’s up with\\nPrivacy? User preferences and privacy concerns in intelligent personal\\nassistants,” in Proc. AAAI/ACM Conf. AI, Ethics Soc., 2018, pp. 229–235.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 19}, page_content='818\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\n[54] D. Roselli, J. Matthews, and N. Talagala, “Managing bias in AI,” in Proc.\\nWorld Wide Web Conf., 2019, pp. 539–544.\\n[55] Y. Gorodnichenko, T. Pham, and O. Talavera, “Social media, sentiment\\nandpublicopinions:Evidencefrom#Brexitand#USElection,”Eur.Econ.\\nRev., vol. 136, Jul. 2021, Art. no. 103772.\\n[56] N. Thurman, “Making ‘The daily me’: Technology, economics and\\nhabit in the mainstream assimilation of personalized news,” Journalism,\\nvol. 12, no. 4, pp. 395–415, 2011.\\n[57] J. Donath, “Ethical issues in our relationship with artiﬁcial entities,” in\\nThe Oxford Handbook of Ethics of AI. M. D. Dubber, F. Pasquale, and S.\\nDas, Eds., Oxford, U.K.: Oxford Univ. Press, 2020, pp. 51–73.\\n[58] E. Magrani, “New perspectives on ethics and the laws of artiﬁcial intel-\\nligence,” Internet Policy Rev., vol. 8, 2019, Art. no. 3.\\n[59] M. P. Wellman and U. Rajan, “Ethical issues for autonomous trading\\nagents,” Minds Mach., vol. 27, no. 4, pp. 609–624, 2017.\\n[60] U. Pagallo, “The impact of AI on criminal law, and its two fold pro-\\ncedures,” in Research Handbook on the Law of Artiﬁcial Intelligence,\\nW. Barﬁeld and U. Pagallo, Eds., Cheltenham U.K.: Edward Elgar\\nPublishing, 2018, pp. 385–409.\\n[61] E. Dacoronia, “Tort law and new technologies,” in Legal Challenges in\\nthe New Digital Age, A. M. López Rodríguez, M. D. Green, and M.\\nL. Kubica, Eds., Leiden, The Netherlands: Koninklijke Brill NV, 2021,\\npp. 3–12.\\n[62] J. Khakurel, B. Penzenstadler, J. Porras, A. Knutas, and W. Zhang, “The\\nrise of artiﬁcial intelligence under the lens of sustainability,” Technolo-\\ngies, vol. 6, no. 4, 2018, Art. no. 100.\\n[63] S. Herat, “Sustainable management of electronic waste (e-Waste),” Clean\\nSoil Air Water, vol. 35, no. 4, pp. 305–310, 2007.\\n[64] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\\nerations for deep learning in NLP,” in Proc. 57th Annu. Meeting Assoc.\\nComput. Linguistics, 2019, pp. 3645–3650.\\n[65] V. Dignum, “Ethics in artiﬁcial intelligence: Introduction to the special\\nissue,” Ethics Inf. Technol., vol. 20, no. 1, pp. 1–3, 2018.\\n[66] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, “Algo-\\nrithmic decision making and the cost of fairness,” in Proc. 23rd ACM\\nSIGKDD Int. Conf. Knowl. Discov. Data Mining, 2017, pp. 797–806.\\n[67] R. Caplan, J. Donovan, L. Hanson, and J. Matthews, “Algorithmic ac-\\ncountability: A primer,” Data Soc., vol. 18, pp. 1–13, 2018.\\n[68] R. V. Yampolskiy, “On controllability of AI,” Jul. 2020. [Online]. Avail-\\nable: https://arxiv.org/pdf/2008.04071\\n[69] B. C. Stahl, J. Timmermans, and C. Flick, “Ethics of emerging informa-\\ntion and communication technologies,” Sci. Public Policy, vol. 44, no. 3,\\npp. 369–381, 2017.\\n[70] L. Vesnic-Alujevic, S. Nascimento, and A. Pólvora, “Societal and\\nethical impacts of artiﬁcial intelligence: Critical notes on Euro-\\npean policy frameworks,” Telecommun. Policy, vol. 44, no. 6, 2020,\\nArt. no. 101961.\\n[71] U. G. Assembly, “Universal declaration of human rights,” UN Gen.\\nAssem., vol. 302, no. 2, pp. 14–25, 1948.\\n[72] S. Russell, S. Hauert, R. Altman, and M. Veloso, “Robotics: Ethics of\\nartiﬁcial intelligence,” Nature, vol. 521, no. 7553, pp. 415–418, 2015.\\n[73] A. Chouldechova, “Fair prediction with disparate impact: A study of\\nbias in recidivism prediction instruments,” Big Data, vol. 5, no. 2,\\npp. 153–163, 2017.\\n[74] J. van Dijck, “Dataﬁcation, dataism and dataveillance: Big data be-\\ntween scientiﬁc paradigm and ideology,” Surveill. Soc., vol. 12, no. 2,\\npp. 197–208, 2014.\\n[75] E. de Souza Nascimento, I. Ahmed, E. Oliveira, M. P. Palheta, I. Stein-\\nmacher, and T. Conte, “Understanding development process of machine\\nlearning systems: Challenges and solutions,” in Proc. ACM/IEEE Int.\\nSymp. Empirical Softw. Eng. Meas., 2019, pp. 1–6.\\n[76] K. A. Crockett, L. Gerber, A. Latham, and E. Colyer, “Build-\\ning trustworthy AI solutions: A case for practical solutions for\\nsmall businesses,” IEEE Trans. Artif. Intell., early access, 2021,\\ndoi: 10.1109/TAI.2021.3137091.\\n[77] D. Leslie, “Understanding artiﬁcial intelligence ethics and safety:\\nA guide for the responsible design and implementation of AI\\nsystems in the public sector,” 2019. Accessed: Apr. 19, 2022.\\n[Online].\\nAvailable:\\nhttps://www.turing.ac.uk/research/publications/\\nunderstanding-artiﬁcial-intelligence-ethics-and-safety\\n[78] B. Buruk, P. E. Ekmekci, and B. Arda, “A critical perspective on guide-\\nlines for responsible and trustworthy artiﬁcial intelligence,” Med. Health\\nCare Philosophy, vol. 23, no. 3, pp. 387–399, 2020.\\n[79] UNESCO, “Recommendation on the ethics of artiﬁcial intelligence,”\\n2021. Accessed: Feb. 15 2022. [Online]. Available: https://en.unesco.\\norg/artiﬁcial-intelligence/ethics\\n[80] B. C. Stahl, Ed., Artiﬁcial Intelligence for a Better Future: An Ecosystem\\nPerspective on the Ethics of AI and Emerging Digital Technologies.\\nCham, Switzerland: Springer, 2021.\\n[81] P. D. Motloba, “Non-maleﬁcence - A disremembered moral obligation,”\\nSouth Afr. Dent. J., vol. 74, 2019, Art. no. 1.\\n[82] L. Floridi and J. Cowls, “A uniﬁed framework of ﬁve principles for AI\\nin society,” in Ethics, Governance, and Policies in Artiﬁcial Intelligence\\n(Philosophical Studies Series), vol. 144, L. Floridi, Ed. Cham, Switzer-\\nland: Springer, 2021, pp. 5–17.\\n[83] S. Jain, M. Luthra, S. Sharma, and M. Fatima, “Trustworthiness of\\nartiﬁcial intelligence,” in Proc. 6th Int. Conf. Adv. Comput. Commun.\\nSyst., 2020, pp. 907–912.\\n[84] L. Floridi et al., “AI4People-An ethical framework for a good AI society:\\nOpportunities, risks, principles, and recommendations,” Minds Mach.,\\nvol. 28, no. 4, pp. 689–707, 2018.\\n[85] R. Nishant, M. Kennedy, and J. Corbett, “Artiﬁcial intelligence for\\nsustainability: Challenges, opportunities, and a research agenda,” Int. J.\\nInf. Manage., vol. 53, 2020, Art. no. 102104.\\n[86] C. S. Wickramasinghe, D. L. Marino, J. Grandio, and M. Manic, “Trust-\\nworthy AI development guidelines for human system interaction,” in\\nProc. 13th Int. Conf. Hum. Syst. Interaction, 2020, pp. 130–136.\\n[87] V. Dignum, “Can AI systems be ethical?,” in Artiﬁcial Intelligence:\\nFoundations Theory and Algorithms, Responsible Artiﬁcial Intelligence.\\nV. Dignum, Ed., Cham, Switzerland: Springer, 2019, pp. 71–92.\\n[88] S. L. Anderson and M. Anderson, “AI and ethics,” AI Ethics, vol. 1, no. 1,\\npp. 27–31, 2021.\\n[89] V. Dignum, “Ethical decision-making,” in Artiﬁcial Intelligence: Foun-\\ndations, Theory, and Algorithms, Responsible Artiﬁcial Intelligence. V.\\nDignum, Ed., Cham, Switzerland: Springer, 2019, pp. 35–46.\\n[90] G.\\nSayre-McCord,\\n“Metaethics,”\\nin\\nThe\\nStanford\\nEncyclopedia\\nof Philosophy, E. N. Zalta, Ed., Stanford, CA, USA: Metaphys.\\nRes.\\nLab,\\nStanford\\nUniv.,\\n2014.\\n[Online].\\nAvailable:\\nhttps:\\n//plato.stanford.edu/entries/metaethics/#:∼:text=Metaethics%20is%\\n20the%20attempt%20to,matter%20of%20taste%20than%20truth%3F\\n[91] Ethics | Internet Encyclopedia of Philosophy, 1995. Accessed: Aug. 2,\\n2021. [Online]. Available: https://iep.utm.edu/ethics/#SH2c\\n[92] R. Hursthouse and G. Pettigrove, “Virtue ethics,” in The Stanford En-\\ncyclopedia of Philosophy, E. N. Zalta, Ed. Stanford, CA, USA: Meta-\\nphys. Res. Lab, Stanford Univ., 2018. [Online]. Available: https://plato.\\nstanford.edu/entries/ethics-virtue/\\n[93] N. Cointe, G. Bonnet, and O. Boissier, “Ethical judgment of agents’\\nbehaviors in multi-agent systems,” in Proc. Int. Conf. Auton. Agents\\nMultiagent Syst., 2016, pp. 1106–1114.\\n[94] H. Yu, Z. Shen, C. Miao, C. Leung, V. R. Lesser, and Q. Yang, “Building\\nethics into artiﬁcial intelligence,” in Proc. 27th Int. Joint Conf. Artif.\\nIntell., 2018, pp. 5527–5533.\\n[95] H. J. Curzer, Aristotle and the Virtues. New York, NY, USA: Oxford\\nUniv. Press, 2012.\\n[96] L. Alexander and M. Moore, “Deontological ethics,” in The Stanford\\nEncyclopedia of Philosophy, E. N. Zalta, Ed., 2020, Stanford, CA, USA:\\nMetaphys. Res. Lab, Stanford Univ., 2020. [Online]. Available: https:\\n//plato.stanford.edu/entries/ethics-deontological/\\n[97] W. Sinnott-Armstrong, “Consequentialism,” in The Stanford Encyclope-\\ndia of Philosophy, E. N. Zalta, Ed., 2019. Stanford, CA, USA: Metaphys.\\nRes. Lab, Stanford Univ., [Online]. Available: https://plato.stanford.edu/\\nentries/consequentialism/\\n[98] D. O. Brink, “Some forms and limits of consequentialism,” in Ox-\\nford Handbooks in Philosophy, The Oxford Handbook of Ethical The-\\nory. D. Copp, Ed., New York, NY, USA: Oxford Univ. Press, 2006,\\npp. 380–423.\\n[99] H. A. M. J. ten Have, Ed., Encyclopedia of Global Bioethics. Cham,\\nSwitzerland: Springer, 2016.\\n[100] S. Tolmeijer, M. Kneer, C. Sarasua, M. Christen, and A. Bernstein,\\n“Implementations in machine ethics: A survey,” ACM Comput. Surv.,\\nvol. 53, no. 6, pp. 1–38, 2021.\\n[101] C. Allen, I. Smit, and W. Wallach, “Artiﬁcial morality: Top-down,\\nbottom-up, and hybrid approaches,” Ethics Inf. Technol., vol. 7, no. 3,\\npp. 149–155, 2005.\\n[102] W. Wallach and C. Allen, “Top-down morality,” in Moral Machines, W.\\nWallach and C. Allen, Eds., Oxford, U.K: Oxford Univ. Press, 2009,\\npp. 83–98.\\n[103] I. Asimov, “Runaround,” Astounding Sci. Fiction, vol. 29, no. 1,\\npp. 94–103, 1942.\\n[104] J.-G.\\nGanascia,\\n“Ethical\\nsystem\\nformalization\\nusing\\nnon-\\nmonotonic ogics,” in Proc. Annu. Meeting Cogn. Sci. Soc., 2007,\\npp. 1013–1018.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'creationDate': \"D:20230711214741+05'30'\", 'page': 20}, page_content='HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n819\\n[105] K. Arkoudas, S. Bringsjord, and P. Bello, “Toward ethical robots via\\nmechanized deontic logic,” in Proc. AAAI Fall Symp. Mach. Ethics, 2005,\\npp. 17–23.\\n[106] S. Bringsjord and J. Taylor, “Introducing divine-command robot ethics,”\\nin Robot Ethics: The Ethical and Social Implication of Robotics. 2012,\\npp. 85–108.\\n[107] N. S. Govindarajulu and S. Bringsjord, “On automating the doctrine\\nof double effect,” in Proc. 26th Int. Joint Conf. Artif. Intell., 2017,\\npp. 4722–4730.\\n[108] F. Berreby, G. Bourgne, and J.-G. Ganascia, “A declarative modular\\nframework for representing and applying ethical principles,” in Proc.\\n16th Conf. Auton. Agents MultiAgent Syst., 2017, pp. 96–104.\\n[109] V. Bonnemains, C. Saurel, and C. Tessier, “Embedded ethics: Some\\nechnical and ethical challenges,” Ethics Inf. Technol., vol. 20, no. 1,\\npp. 41–58, 2018.\\n[110] G. S. Reed, M. D. Petty, N. J. Jones, A. W. Morris, J. P. Ballenger,\\nand H. S. Delugach, “A principles-based model of ethical considerations\\nin military decision making,” J. Defense Model. Simul., vol. 13, no. 2,\\npp. 195–211, 2016.\\n[111] L. Dennis, M. Fisher, M. Slavkovik, and M. Webster, “Formal veriﬁcation\\nof ethical choices in autonomous systems,” Robot. Auton. Syst., vol. 77,\\npp. 1–14, 2016.\\n[112] A. R. Honarvar and N. Ghasem-Aghaee, “Casuist BDI-Agent: A new\\nextended BDI architecture with the capability of ethical reasoning,” in\\nProc. Int. Conf. Artif. Intell. Comput. Intell., 2009, pp. 86–95.\\n[113] A. S. Rao and M. P. Georgeff, “BDI agents: From theory to practice,” in\\nProc. 1st Int. Conf. Multiagent Syst., 1995, pp. 312–319.\\n[114] S. Armstrong, “Motivated value selection for artiﬁcial agents,” in Proc.\\nAAAI Workshop Artif. Intell. Ethics, Jan. 2015, pp. 12–20.\\n[115] U. Furbach, C. Schon, and F. Stolzenburg, “Automated reasoning in\\ndeontic logic,” in Proc. 8th Int. Workshop Multi-Disciplinary Trends Artif.\\nIntell., 2014, pp. 57–68.\\n[116] D. Howard and I. Muntean, “Artiﬁcial moral cognition: Moral function-\\nalism and autonomous moral agency,” in Philosophical Studies Series,\\nPhilosophy and Computing, T. M. Powers, Ed. Cham, Switzerland:\\nSpringer, 2017, pp. 121–159.\\n[117] Y.-H. Wu and S.-D. Lin, “A low-cost ethics shaping approach for de-\\nsigning reinforcement learning agents,” in Proc. 32nd AAAI Conf. Artif.\\nIntell., 2018, pp. 1687–1694.\\n[118] R. Noothigattu et al., “A voting-based system for ethical decision mak-\\ning,” in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 1587–1594.\\n[119] M. Guarini, “Particularism and the classiﬁcation and reclassiﬁcation of\\nmoral cases,” IEEE Intell. Syst., vol. 21, no. 4, pp. 22–28, Jul./Aug. 2006.\\n[120] M. Anderson and S. L. Anderson, “GenEth: A general ethical dilemma\\nanalyzer,” in Proc. 28th AAAI Conf. Artif. Intell., 2014, pp. 253–261.\\n[121] M. Azad-Manjiri, “A new architecture for making moral agents based on\\nC4.5 decision tree algorithm,” Int. J. Inf. Technol. Comput. Sci., vol. 6,\\nno. 5, pp. 50–57, 2014.\\n[122] L. Yilmaz, A. Franco-Watkins, and T. S. Kroecker, “Computational mod-\\nels of ethical decision-making: A coherence-driven reﬂective equilibrium\\nmodel,” Cogn. Syst. Res., vol. 46, pp. 61–74, 2017.\\n[123] T. A. Han, A. Saptawijaya, and L. M. Pereira, “Moral reasoning under\\nuncertainty,” in Logic For Programming, Artiﬁcial Intelligence, and\\nReasoning. Berlin, Germany: Springer, 2012, pp. 212–227.\\n[124] M. Anderson, S. Anderson, and C. Armen, “Towards machine ethics\\nimplementing two action-based ethical theories,” in Proc. AAAI Fall\\nSymp. Mach. Ethics, 2005, pp. 1–7.\\n[125] G. Gigerenzer, “Moral satisﬁcing: Rethinking moral behavior as bounded\\nrationality,” Topics Cogn. Sci., vol. 2, no. 3, pp. 528–554, 2010.\\n[126] J. Skorin-Kapov, “Ethical positions and decision-making,” in Profes-\\nsional and Business Ethics Through Film, J. Skorin-Kapov, Ed., New\\nYork, NY, USA: Springer, 2018, pp. 19–54.\\n[127] T.-L. Gu and L. Li, “Artiﬁcial moral agents and their design methodology:\\nRetrospect and prospect,” Chin. J. Comput., vol. 44, pp. 632–651, 2021.\\n[128] C.Molnar,G.Casalicchio,andB.Bischl,“Interpretablemachinelearning\\n– A brief history, state-of-the-art and challenges,” in Proc. Joint Eur. Conf.\\nMach. Learn. Knowl. Discov. Databases, 2020, pp. 417–431.\\n[129] C. Molnar, Interpretable Machine Learning: A Guide For Making Black\\nBox Models Interpretable. Morisville, NC, USA: Lulu, 2019.\\n[130] S. Feuerriegel, M. Dolata, and G. Schwabe, “Fair AI,” Bus. Inf. Syst. Eng.,\\nvol. 62, no. 4, pp. 379–384, 2020.\\n[131] S. Caton and C. Haas, “Fairness in machine learning: A survey,”\\nOct. 2020. [Online]. Available: https://arxiv.org/pdf/2010.04053\\n[132] S. E. Whang, K. H. Tae, Y. Roh, and G. Heo, “Responsible AI challenges\\nn end-to-end machine learning,” Jan. 2021. [Online]. Available: https:\\n//arxiv.org/pdf/2101.05967\\n[133] D. Peters, K. Vold, D. Robinson, and R. A. Calvo, “Responsible AI—Two\\nframeworksforethicaldesignpractice,”IEEETrans.Technol.Soc.,vol.1,\\nno. 1, pp. 34–47, Mar. 2020.\\n[134] V. Dignum, ed., Responsible Artiﬁcial Intelligence. Cham, Switzerland:\\nSpringer, 2019.\\n[135] C. Dwork, “Differential privacy: A survey of results,” in Proc. Int. Conf.\\nTheory Appl. Models Computation, 2008, pp. 1–19.\\n[136] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen, and H. Yu, “Federated\\nlearning,” Synth. Lectures Artif. Intell. Mach. Learn., vol. 13, no. 3,\\npp. 1–207, 2019.\\n[137] M. Kirienko et al., “Distributed learning: A reliable privacy-preserving\\nstrategy to change multicenter collaborations using AI,” Eur. J. Nucl.\\nMed. Mol. Imag., vol. 48, no. 12, pp. 3791–3804.2021.\\n[138] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in\\nProc. 22nd ACM SIGSAC Conf. Comput. Commun. Secur., 2015,\\npp. 1310–1321.\\n[139] C. Meurisch, B. Bayrak, and M. Mühlhäuser, “Privacy-preserving AI\\nservices through data decentralization,” in Proc. Web Conf., 2020,\\npp. 190–200.\\n[140] UR-Lex - 02016R0679-20160504 - EN - EUR-Lex, 2016. Accessed:\\nJun. 28, 2021. [Online]. Available: https://eur-lex.europa.eu/legal-\\ncontent/EN/TXT/?uri=CELEX%3A02016R0679-20160504&qid=\\n1532348683434\\n[141] R. E. Latta, H.R.3388 - 115th Congress (2017-2018): SELF DRIVE\\nAct, 2017. Accessed: Jun. 28, 2021. [Online]. Available: https://www.\\ncongress.gov/bill/115th-congress/house-bill/3388\\n[142] 7. Lei No. 13, de 14 de Agosto de 2018, 2018. Accessed: Jun. 25, 2021.\\n[Online]. Available: http://www.planalto.gov.br/ccivil_03/_Ato2015-\\n2018/2018/Lei/L13709.html\\n[143] EUR-Lex - 52021PC0206 - EN - EUR-Lex, 2021. Accessed: Jun. 28,\\n2021. [Online]. Available: https://eur-lex.europa.eu/legal-content/EN/\\nTXT/?qid=1623335154975&uri=CELEX%3A52021PC0206\\n[144] C. Allen, G. Varner, and J. Zinser, “Prolegomena to any future artiﬁcial\\nmoral agent,” J. Exp. Theor. Artif. Intell., vol. 12, no. 3, pp. 251–261,\\n2000.\\n[145] A. M. Turing, “Computing machinery and intelligence,” Mind, vol. LIX,\\nno. 236, pp. 433–460, 1950.\\n[146] W. Wallach and C. Allen, Moral Machines: Teaching Robots Right From\\nWrong. Oxford, U.K.: Oxford Univ. Press, 2009.\\n[147] S. A. Seshia, D. Sadigh, and S. S. Sastry, “Towards veriﬁed artiﬁcial\\nintelligence,” Jun. 2016. [Online]. Available: http://arxiv.org/pdf/1606.\\n08514v4\\n[148] T. Arnold and M. Scheutz, “Against the moral turing test: Accountable\\ndesign and the moral reasoning of autonomous systems,” Ethics Inf.\\nTechnol., vol. 18, no. 2, pp. 103–115, 2016.\\n[149] ACM Code of Ethics and Professional Conduct, 2018. Accessed: Jun. 25,\\n2021. [Online]. Available: https://www.acm.org/code-of-ethics\\n[150] IEEE SA- The IEEE Global Initiative on Ethicsof Autonomousand Intel-\\nligent Systems, 2019. Accessed: Jun. 28 2021. [Online]. Available: https:\\n//standards.ieee.org/industry-connections/ec/autonomous-systems.html\\n[151] IEEE Ethics In Action | Ethically Aligned Design, IEEE 7000TM\\nProjects, 2020. Accessed: Jun. 28, 2021. [Online]. Available: https:\\n//ethicsinaction.ieee.org/p7000/\\n[152] ISO, ISO/IEC JTC 1/SC 42 - Artiﬁcial intelligence, 2017. Accessed:\\nJun. 28, 2021. [Online]. Available: https://www.iso.org/committee/\\n6794475.html\\n[153] B. Goehring, F. Rossi, and D. Zaharchuk, “Advancing AI ethics beyond\\ncompliance: From principles to practice,” IBM Corporation, Apr. 2020.\\nAccessed: Apr. 19, 2022. [Online]. Available: https://www.ibm.com/\\nthought-leadership/institute-business-value/report/ai-ethics\\n[154] Responsible AI, 2017. Accessed: Apr. 19, 2022. [Online]. Available:\\nhttps://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:\\nprimaryr6\\n[155] F. Allhoff, “Evolutionary ethics from Darwin to Moore,” Hist. Philosophy\\nLife Sci., vol. 25, no. 1, pp. 51–79, 2003.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 0}, page_content='Vol.:(0123456789)\\n1 3\\nJournal of Business Ethics (2023) 185:725–740 \\nhttps://doi.org/10.1007/s10551-023-05339-7\\nORIGINAL PAPER\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful \\nWork\\nSarah\\xa0Bankins1\\u200a \\xa0· Paul\\xa0Formosa2\\nReceived: 17 February 2021 / Accepted: 25 January 2023 / Published online: 11 February 2023 \\n© The Author(s) 2023, corrected publication 2023\\nAbstract\\nThe increasing workplace use of artificially intelligent (AI) technologies has implications for the experience of meaningful \\nhuman work. Meaningful work refers to the perception that one’s work has worth, significance, or a higher purpose. The \\ndevelopment and organisational deployment of AI is accelerating, but the ways in which this will support or diminish oppor-\\ntunities for meaningful work and the ethical implications of these changes remain under-explored. This conceptual paper is \\npositioned at the intersection of the meaningful work and ethical AI literatures and offers a detailed assessment of the ways \\nin which the deployment of AI can enhance or diminish employees’ experiences of meaningful work. We first outline the \\nnature of meaningful work and draw on philosophical and business ethics accounts to establish its ethical importance. We \\nthen explore the impacts of three paths of AI deployment (replacing some tasks, ‘tending the machine’, and amplifying human \\nskills) across five dimensions constituting a holistic account of meaningful work, and finally assess the ethical implications. \\nIn doing so we help to contextualise the meaningful work literature for the era of AI, extend the ethical AI literature into the \\nworkplace, and conclude with a range of practical implications and future research directions.\\nKeywords\\u2002 Meaningful work\\xa0· Artificial intelligence (AI)\\xa0· Ethical AI\\xa0· Future of work\\xa0· Technology and work\\nIntroduction\\nIncreasing organisational use of artificially intelligent (AI) \\ntechnologies will influence how people experience work \\n(World Economic Forum [WEF], 2018), including how and \\nwhether they experience meaningfulness in their work. AI \\nis the ability of computers and other artificial entities to do \\nthings typically classified as requiring intelligence were a \\nhuman to do them, such as reason, plan, problem solve, and \\nlearn from experience (Wang, 2019). Meaningful work is \\nthe perception that one’s work has worth, significance, or a \\nhigher purpose (Michaelson et\\xa0al., 2014), and this typically \\nrequires the coordinated exercise of varied and complex \\nskills to benefit others. Providing opportunities for mean-\\ningful work supports positive outcomes for workers (Allan \\net\\xa0al., 2019) and is ethically important as a basis for human \\nwellbeing and flourishing (Bailey et\\xa0al., 2019; Lysova et\\xa0al., \\n2019). However, despite becoming an increasingly prevalent \\nfeature of workplaces, there remains a poor understanding of \\nhow AI use will influence opportunities for meaningful work \\nand the ethical implications of such changes.\\nHistorically technological advancements have, since at \\nleast the first\\xa0industrial revolution, significantly changed \\nopportunities for meaningful work by altering what work-\\ners do, the nature of their skills, and their feelings of aliena-\\ntion from or integration with the production process (Vallor, \\n2015). AI use will likely extend such changes, but its unique \\nfeatures and uses also generate new and conflicting impli-\\ncations for meaningful work. Optimistic accounts suggest \\nthat\\xa0AI will expand the range of meaningful higher-order \\nhuman work tasks (WEF, 2018), whereas more pessimis-\\ntic accounts suggest that\\xa0AI will degrade and even elimi-\\nnate human work (Frey & Osborne, 2017). These ongoing \\ntensions point to a lack of conceptual clarity regarding the \\n *\\t Sarah Bankins \\n\\t\\nsarah.bankins@mq.edu.au\\n\\t\\nPaul Formosa \\n\\t\\npaul.formosa@mq.edu.au\\n1\\t\\nDepartment of\\xa0Management, Macquarie Business School, \\nMacquarie University, North Ryde Campus, Sydney, \\nNSW\\xa02109, Australia\\n2\\t\\nDepartment of\\xa0Philosophy, Faculty of\\xa0Arts, Macquarie \\nUniversity, North Ryde Campus, Sydney, NSW\\xa02109, \\nAustralia'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 1}, page_content='726\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nimpacts of AI on meaningful work, leading to calls for more \\nresearch in this area (Parker & Grote, 2022).\\nThis conceptual paper aims to help\\xa0address such gaps \\nby examining how workplace use of AI has the potential \\nto both enhance and diminish experiences of meaningful \\nwork, depending largely on the implementation choices of \\nemployers. This research is positioned at the intersection of \\nthe meaningful work and ethical AI literatures and makes \\ntwo key contributions. First, we contextualise the meaning-\\nful work literature for the era of AI by developing concep-\\ntual resources to examine how the implementation of such \\ntechnologies affects workers’ opportunities for meaningful \\nwork and connect this assessment to the ethical implications \\nof these changes. Second, we help remedy a neglected aspect \\nof the ethical AI literature by offering a detailed examination \\nof AI’s implications for meaningful work.\\nWe begin by outlining the nature of meaningful work and \\nits ethical importance, integrating philosophical and busi-\\nness ethics accounts. We then examine the impacts of three \\npaths of AI deployment—replacing some simple and com-\\nplex tasks (replacement), ‘tending the machine’ (creating \\nnew forms of human work), and amplifying human skills \\n(augmenting/assisting workers)—across five dimensions \\nof meaningful work. These dimensions integrate both job-\\nspecific (through Hackman & Oldham’s, 1976 job charac-\\nteristics model) and more holistic (through Lips-Wiersma & \\nMorris’, 2009 model) drivers of meaningful work. We then \\ndevelop the ethical implications of our analysis by drawing \\non the AI4People ethical AI framework (Floridi et\\xa0al., 2018) \\nand its five principles of beneficence, non-maleficence, \\nautonomy, justice, and explicability. We conclude with prac-\\ntical insights into how experiences of meaningful work will \\nchange as AI becomes more widespread and offer several \\ndirections for future research.\\nAI and\\xa0Work: Uses and\\xa0Unique Features\\nCurrent AIs constitute artificial narrow intelligence, or AIs \\nthat can undertake actions only within restricted domains, \\nsuch as classifying pictures of cats (Boden, 2016). The “holy \\ngrail” of AI research is artificial general intelligence (Boden, \\n2016), or AIs that can perform at least as well as humans \\nacross the full range of intelligent activities. We focus only \\non narrow AI as it is already used across many diverse sec-\\ntors, including in healthcare, judicial, educational, manu-\\nfacturing, and military contexts, among many others (see \\nBankins & Formosa, 2021; Bekey, 2012; Walsh et\\xa0al., 2019). \\nThe established use of narrow AI also allows us to draw on \\npractical examples to ground our assessment of its effects on \\nmeaningful work. While considering the possible implica-\\ntions of artificial general intelligence for meaningful work \\nis important, and we discuss this in our future research \\ndirections, there remain persistent disagreements about \\nwhen, if ever, it will be achieved (Boden, 2016). This makes \\nit critical to examine the impacts of current AI capabilities \\non opportunities for meaningful work that are occurring now \\nand in the near-term (Webster & Ivanov, 2020).\\nPast research demonstrates the dual positive and negative \\neffects of technology upon aspects of meaningful work. For \\nexample, technology use can upskill workers and enhance \\ntheir autonomy, but it can also deskill and serve to control \\nthem (Vallor, 2015; Mazmanian et\\xa0al., 2013), with meaning-\\nfulness generally elevated in the former case (Cheney et\\xa0al., \\n2008). Technology’s positive effects can also help individu-\\nals confirm pre-existing notions of meaningful work, but \\nits negative outcomes can require them to re-interpret and \\nadjust those meanings as the technology’s dual effects are \\nrealised, for example by providing on-demand connection to \\nwork but heightening distraction from other responsibilities \\n(Symon & Whiting, 2019). Such dual effects remain evident \\nin advancing forms of technology, such as workplace robot-\\nics that offer both benefits and threats to meaningful human \\nwork (see Smids et\\xa0al., 2020).\\nThese findings are critical, but their focus is on broader \\ntypes of information and communication technologies, \\nwhereas we focus specifically upon AI and its implications \\nfor meaningful work. While AI use should also generate \\nthese types of dual effects, its unique features warrant spe-\\ncific attention. For example, compared to past technologies \\nAI can undertake more cognitive tasks, expanding beyond \\n‘blue collar’ work in manufacturing where technology’s role \\nin replacing human labour has a long history, and into more \\n‘white collar’ forms of work (Bankins & Formosa, 2020). \\nFurther, machine learning in AIs is often driven by large \\namounts of data, the acquisition of which raises serious con-\\ncerns about privacy, consent, and surveillance, with impli-\\ncations for worker autonomy (Bailey et\\xa0al., 2019). Potential \\nbiases in data collection, the use of AI models built from \\nbiased data, and the resultant replication of systemic injus-\\ntices (Walsh et\\xa0al., 2019), as already evidenced in some AI-\\ndriven recruitment practices (Dastin, 2018), raises further \\nconcerns about the potential for one’s AI-informed work \\nto harm others. The potential for such harms is then exac-\\nerbated given the scale at which AI can be deployed. The \\nway AIs expand opportunities to manipulate and control \\nhumans also raises important issues (Susser et\\xa0al., 2019), \\nparticularly through the way it can act as an information \\ngatekeeper for human workers (Kellogg et\\xa0al., 2020). Finally, \\nthe ‘blackbox’ nature of the neural networks many AIs use \\nmeans end-users and even AI developers cannot understand \\nhow an AI generates its outputs (Jarrahi, 2019). This can \\nmake it difficult to trust AIs, to feel competent in working \\nalongside them, and to build responsible systems for which \\nhuman workers can be held meaningfully accountable (Dahl, \\n2018). These features of AI have attendant consequences'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 2}, page_content='727\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nfor meaningful work that we will explore. We first turn to \\nexplaining the components of meaningful work and its ethi-\\ncal importance.\\nWhat Constitutes Meaningful Work?\\nSeveral approaches outline what constitutes meaningful \\nwork. One dominant task-based framework is Hackman and \\nOldham’s (1976) job characteristics model (JCM), which \\nexamines how job and task design influences experiences \\nof meaningfulness in work (Pratt & Ashforth, 2003).1 Other \\nframeworks extend beyond a task focus to adopt a more \\n“humanistic” approach (Lips-Wiersma & Morris, 2009, p. \\n493). For example, Lips-Wiersma and Morris (2009) suggest \\nthat\\xa0meaningful work derives from finding balance between \\n“being (true to self)-doing (making a difference)” and a \\nfocus on “self (self-actualisation)-others (serving others)”. \\nThis creates the meaningful work dimensions of “developing \\nand becoming self”, “serving others”, “unity with others”, \\nand “expressing one’s full potential” (Lips-Wiersma & Mor-\\nris, 2009, p. 501).\\nTo adopt a holistic approach for exploring the impacts of \\nAI on meaningful work we integrate aspects of meaningful \\njob design from the JCM (Hackman & Oldham, 1976) with \\ndimensions of work that facilitate the more wide-ranging \\nenhancement of oneself through development, contribu-\\ntion, and connection to others from Lips-Wiersma and \\nMorris’ (2009) framework. This harmonisation generates \\nfive meaningful work dimensions that we focus our analy-\\nsis upon.2 The first dimension we label task integrity. This \\nencompasses task identity from the JCM, or the range of \\ntasks an individual does and the opportunity to complete \\na whole piece of work. This ability to undertake integrated \\nrather than fragmented tasks then influences the extent to \\nwhich workers can fully develop themselves, their capaci-\\nties, and express their full potential as an integrated whole \\nperson (“developing and becoming self” and “expressing full \\npotential” from Lips-Wiersma & Morris, 2009). The second \\ndimension we label skill cultivation and use. This encom-\\npasses skill variety and use from the JCM, or the ability to \\nuse and develop a range of skills at work. Like the types of \\ntasks to which they are applied, prospects for skill utilisation \\nthen influence opportunities for growth through learning and \\nthe broader cultivation of the self and one’s potential via \\ndeveloping, testing, and exercising a varied range of com-\\npetencies (“developing and becoming self” and “expressing \\nfull potential” from Lips-Wiersma & Morris, 2009).\\nThe third dimension is task significance (per the JCM) \\nwhich connects one’s work to the wider world. This dimen-\\nsion reflects the extent to which individuals can see how \\ntheir work benefits, and contributes to the betterment of, oth-\\ners (“serving others” from Lips-Wiersma & Morris, 2009). \\nThe fourth dimension is autonomy (per the JCM), which \\nreflects how freely individuals can determine their work \\napproaches and the extent of their freedom from intrusive \\nsurveillance and monitoring. The more autonomy workers \\nexperience the greater their capacity to engage in activi-\\nties like job crafting to enhance fit between individual needs \\nand job requirements, and to undertake work that fosters \\nself-development, moral cultivation, and that affords align-\\nment with one’s values (“developing and becoming self” and \\n“expressing full potential” from Lips-Wiersma & Morris, \\n2009). The final dimension is belongingness, reflecting the \\nways that work can help us feel connected to a wider group \\nto generate meaningfulness through a sense of unity with \\nothers (Bailey et\\xa0al., 2019; Lips-Wiersma & Morris, 2009; \\nMartela & Riekki, 2018). Now that we know what underpins \\nexperiences of meaning in work, we can turn to explaining \\nthe ethical dimensions of both meaningful work and AI.\\nThe Ethics of\\xa0Meaningful Work and\\xa0Ethical AI\\nRecent philosophical discussions of meaningfulness tend to \\nfocus on what makes life itself, or the activities and rela-\\ntionships that compose a well-lived life, meaningful (Wolf, \\n2010). The paradigm of meaningless work is Sisyphus, who \\nis condemned as punishment to repeatedly roll a rock to the \\ntop of a mountain (Camus, 1955). Sisyphus’ work is bor-\\ning, repetitive, simple, does not benefit others, and is not \\nfreely chosen.3 By implication, meaningful work should be \\nengaging, varied, require the use of complex skills, benefit \\nothers, and be freely chosen. This emphasises two aspects \\nof meaningfulness that Wolf (2010) calls subjective (do you \\nexperience work as meaningful?) and objective (is the work \\nactually meaningful?) elements. As we take meaningful \\nwork to be “personally significant and worthwhile” (Lys-\\nova et\\xa0al., 2019, p. 375), our definition is inclusive of these \\n1\\u2002 Pratt and Ashforth (2003) also discuss meaningfulness at work, or \\nthe ways leaders craft and convey organisational values to build feel-\\nings of organisational membership. To maintain a manageable scope, \\nour analysis only examines meaning in work, which is largely driven \\nby job design.\\n2\\u2002 The job characteristics model also includes feedback. We draw on \\nthe model’s first three aspects as they are theorised to directly gen-\\nerate the psychological state of experienced meaningfulness at work, \\nand both autonomy and belongingness are viewed in the wider litera-\\nture as other critical components of meaningful work. See Parker and \\nGrote (2022) for an assessment of technology’s impact on feedback \\nat work.\\n3\\u2002 Of course, Sisyphus’ story is more complicated than this, with \\nCamus (1955) arguing that Sisyphus finds a form of happiness in his \\nscornful embrace of the absurdity of his condition.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 3}, page_content='728\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nsubjective (it is personally significant) and objective (it is \\nworthwhile) aspects.\\nThe Ethical Implications of\\xa0Meaningful Work: Why \\nis\\xa0it Ethically Important?\\nLiterature in business ethics and political philosophy explore \\nthe ethical significance of meaningful work (Michaelson \\net\\xa0al., 2014). Meaningful work can be viewed as ethically \\nsignificant either because it is intrinsically valuable (first \\nbasis), or because it is a constitutive element of a broader \\ngood (second basis), or because it is an instrumental good \\nthat leads to other valuable goods (third basis) (Michaelson \\net\\xa0al., 2014). From these three bases we can see that there \\nare good grounds for holding meaningful work to be ethi-\\ncally important across each of our three most used ethical \\ntheories: Kantian ethics, Virtue Theory, and Utilitarianism.\\nRegarding the first basis, Kantian ethical theories focus \\non treating people with dignity and respect as rational \\nagents who have normative authority over their lives, and \\nthis includes imperfect duties to promote and develop the \\nrational capacities and self-chosen ends of moral agents \\n(Formosa, 2017). Meaningful work is ethically significant \\nas it provides an important way to develop and exercise one’s \\nrational capacities and use them in ways that help others to \\nmeet their ends. Bowie (1998, p. 1083) identifies six features \\nof meaningful work that explain why Kantians should care \\nabout it, including that the work is “freely entered into”, \\n“not paternalistic”, ‘‘provides a wage sufficient for physi-\\ncal welfare”, allows workers to exercise their “autonomy \\nand independence”, “develop” their “rational capacities”, \\nand promotes their “moral development”. In terms of the \\nsecond basis, many virtue ethicists argue that meaningful \\nwork is an integral part of flourishing as a human being. \\nFor example, Nussbaum (2011) argues that “being able to \\nwork as a human being” is a central human capability. This \\nmeans being able to exercise our practical reason, use our \\nsenses, imagination and thought, have some control over our \\nwork environment, and being able to have “meaningful rela-\\ntions of mutual recognition with other workers” (Nussbaum, \\n2011, p. 34). The capability to pursue meaningful work is \\nthus an important right and component of human flourish-\\ning. In terms of the third basis, evidence shows the positive \\ninstrumental impacts that meaningful work has on wellbeing \\nand a range of other goods (Allan et\\xa0al., 2019). This gives us \\ngood reasons to care about meaningful work for the sake of \\nother important goods it contributes to and promotes, such \\nas human wellbeing, that are valued on a range of ethical \\ntheories, including Utilitarianism.\\nOverall, according to all three of our most used moral \\ntheories there are good reasons to care about meaningful \\nwork given that it respects workers’ autonomy and their abil-\\nity to exercise complex skills in helping others, contributes \\nto their wellbeing, and allows them to flourish as complex \\nhuman beings. Given its ethically valuable nature, it fol-\\nlows that organisations have strong pro tanto reasons to \\npromote, support, and offer meaningful work (Michaelson \\net\\xa0al., 2014). Of course, pro tanto reasons are not indefeasi-\\nble reasons, and so other considerations may outweigh them, \\nsuch as improved efficiency, which means changes that lead \\nto less meaningful work are not necessarily unethical. Fur-\\nther, some workers may be willing to trade off less mean-\\ningful work for other gains, such as more income or leisure \\ntime. Even so, meaningful work remains ethically important \\nand changes that impact the amount of meaningful work \\nfor humans\\xa0must be taken into ethical account, even if such \\nconsiderations are not always overriding.\\nThe Ethical Implications of\\xa0AI Use\\nGiven the ethical importance of meaningful work, more \\nscholarship is needed to explore the potential impacts of \\nAI upon it. The ethical significance of AI use is widely \\nrecognised and discussed (see\\xa0Floridi et\\xa0al., 2018; Hagen-\\ndorff, 2020; Jobin et\\xa0al., 2019), leading to various organi-\\nsational, national, and international documents outlining \\nethical principles for AI deployment. However, AI’s effects \\non meaningful work are not a focus of any of these prin-\\nciples. For example, Jobin et\\xa0al.’s (2019) meta-analysis of \\nethical AI guidelines identifies 11 principles, but none men-\\ntion meaningful work directly. Hagendorff’s (2020) analysis \\nalso does not identify it, although related issues around the \\n“future of employment” are discussed. An analysis by Ryan \\nand Stahl (2020, p. 67) mentions the need to “retrain and \\nretool” human workers who are fully replaced by AI, but this \\nsidelines human-AI collaborations in workplaces and AI’s \\nbroader impacts on meaningful work. The AI4People frame-\\nwork also makes no direct\\xa0mention of meaningful work, but \\nit does note the possibility of AI liberating people from the \\n“drudgery” of some work (Floridi et\\xa0al., 2018, p. 691).\\nWhile these frameworks do not mention meaningful work \\nexplicitly, we can nonetheless draw on them to identify ethi-\\ncal concerns that AI’s impacts on meaningful work raise. \\nTo do this we draw on the AI4People ethical AI framework \\n(Floridi et\\xa0al., 2018) and its five principles of beneficence, \\nnon-maleficence, autonomy, justice, and explicability. We \\nutilise this widely discussed framework as it emerged from \\na robust consensus-building program to formulate ethical \\nAI principles. The framework’s focus on the impacts of AI \\non “human dignity and flourishing” across its elements of \\n“autonomous self-realisation… human agency… individual \\nand societal capabilities... [and] societal cohesion” (Floridi \\net\\xa0al., 2018, p. 690) also fits our focus, given that the impacts \\nof AI on dignity, autonomous agency, social cohesion, skills \\nand capabilities, and human flourishing all relate to our \\ndimensions of meaningful work. The foundational principles'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 4}, page_content=\"729\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nof this framework (minus explicability) have also been uti-\\nlised in related work on the ethical design and deployment of \\nbroader information technologies (see Wright, 2011), which \\nagain emphasises the framework’s usefulness in our context.\\nThe five principles of the AI4People framework allow us \\nto explore the wide-ranging impacts of AI on meaningful \\nwork. The first principle is beneficence, or the benefits AI \\ncan bring toward promoting human wellbeing and preserv-\\ning human dignity in an environmentally sustainable way. \\nNon-maleficence is about ensuring that AI does not harm \\nhumanity, and this includes not violating individuals’ pri-\\nvacy and maintaining the safety and security of AI systems. \\nAutonomy is about giving humans the power to decide what \\nAI does. A linking concern between the first two princi-\\nples is the use of AI, intentionally or not, to cause harm \\nby interfering with and disrespecting human autonomy by \\n“nudging… human behaviour in undesirable ways” (Floridi \\net\\xa0al., 2018, p. 697). Nudging involves setting up the “choice \\narchitecture”, or decision context, to intentionally attempt to \\npush (or “nudge”) people to make certain choices (Thaler & \\nSunstein, 2008). Justice is about fairly distributing the bene-\\nfits and burdens from AI use and not undermining solidarity \\nand social cohesion. Finally, explicability is about ensuring \\nthat AI operates in ways that are intelligible and accountable, \\nso that we can understand how it works and we can require \\nsomeone to be responsible for its actions. In the context of \\nmeaningful work, these principles lead us to focus on the \\nbenefits and harms that AI can bring to workers, includ-\\ning on their tasks, skills and social relations, the way AI \\nmight control, nudge, and manipulate workers’ autonomy, \\nthe distribution of the benefits and harms AI brings, and the \\nextent of intelligibility and accountability in AI workplace \\ndeployments.\\nOur overall conceptual framework is presented in Fig.\\xa01 \\nand our analysis is structured as follows. We first outline the \\nimpacts of AI on the five dimensions of meaningful work by \\nanalysing its effects through three pathways (outlined below) \\nfor AI deployment: replacement; ‘tending the machine’; and \\namplifying. We then turn to the AI4People ethical frame-\\nwork to draw out the ethical implications of these impacts \\non meaningful work. This structure allows us to focus in \\na systematic way on each important set of analyses, first \\nrelated to AI’s effects on meaningful work and then the ethi-\\ncal implications of this, while highlighting how these effects \\nare often contingent on the ways in which AI is deployed.\\nThe Effects of\\xa0Artificial Intelligence \\non\\xa0Meaningful Work\\nAI represents a range of technologies that can be used in \\nmany ways alongside human workers doing many different \\ntasks. This makes it important to examine not only what \\ntasks the AI does, but also how human workers’ tasks change \\nfollowing AI deployment and the comparative meaningful-\\nness of their new work. While we briefly discuss the impacts \\nof full human replacement by AI upon meaningful work, \\nwe focus our analysis on meaningful work outcomes when \\nhumans work alongside AI.4 This is because such work con-\\nfigurations already, and will increasingly, characterise many \\nworkplaces (Jarrahi, 2018) and reflects our focus on clear \\ncurrent and near-term impacts of narrow AI.\\nTechnology’s Effects on\\xa0Work: Three Paths\\nOur analytical framework adapts and expands Langlois' \\n(2003) characterisation of how technology integrates into a \\nwork process. This structures our analysis around three key \\npaths through which AI will shape humans’ experiences of \\nmeaningful work.\\nIn the first path, AI assumes some tasks (either simple or \\ncomplex) while workers remain engaged elsewhere in the \\nFig.\\u202f1\\u2002 \\u2009Overview of conceptual \\nframework\\nFive Dimensions of \\nMeaningful Work\\n1. Task integrity\\n2. Skill cultivation and\\nuse\\n3. Task significance\\n4. Autonomy\\n5. Belongingness\\nFive Ethical AI \\nPrinciples\\n1.\\nBeneficence\\n2.\\nNon-maleficence\\n3.\\nAutonomy\\n4.\\nJustice\\n5.\\nExplicability\\nThree AI \\nImplementation \\nPathways\\n1. Replacing\\n2. Tending the \\nmachine\\nManaging the \\nmachine\\nMinding the \\nmachine\\n3. Amplifying\\nThe implementation \\nof AI impacts \\nmeaningful work \\ndimensions in \\ndifferent ways\\nThe outcomes of AI \\nimplementation on \\nmeaningful work are \\nethically assessed\\n•\\n•\\n4\\u2002 We do not significantly detail the effects of AI fully replacing a \\nworker because, at least currently, AI is unlikely to predominantly \\nautomate entire jobs (Chui et\\xa0al., 2015). But where this does occur \\nthe impacts are clear, the unemployed worker has lost meaning-\\nful paid work until they can find another job (which may offer more \\nopportunities for meaningful work, see Cheney et\\xa0 al., 2008). This \\nalso raises broader issues, beyond our scope, around other sources of \\nmeaningfulness if increasingly sophisticated AI makes paid meaning-\\nful work rarer (see Bruun & Duka, 2018).\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 5}, page_content='730\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\n(roughly similar) work process. This is akin to AI replacing \\nhumans in some tasks. For example, if a personalised maths \\nlearning app is introduced in a classroom, the teacher may \\nre-focus upon other existing tasks (e.g., more time for les-\\nson planning) or undertake new work (e.g., individualised \\nmaths coaching), but the overall work process of ‘teach-\\ning’ remains similar (see such examples in\\xa0Acemoglu & \\nRestrepo, 2020). We also focus on the two ends of the skills \\nspectrum for illustrative purposes (i.e., simple and complex \\ntasks), and acknowledge that tasks will likely involve vari-\\nous skills. The key difference between this path and the next \\nis that here the replacement work undertaken by humans is \\nnot focused on managing the AI, but in the next path it is.\\nIn the second path, AI assumes a set of tasks resulting in \\nnew human work focused on “tending the machine” (Lan-\\nglois, 2003, p. 175). This is akin to creating new types of \\ntasks for workers.5 We further divide ‘tending the machine’ \\ninto two emerging forms of work associated with manag-\\ning AI: (1) what we term ‘managing the machine’, which \\ngenerates new, complex, and interesting forms of work for \\nhumans; and (2) what Langlois (2003, p. 175) terms “mind-\\ning the machine”, which generates more mundane, rote, and \\nlower-skilled work for humans. Again, we focus on two ends \\nof a spectrum for illustrative purposes, while acknowledging \\nthat human work may exist across both categories. ‘Man-\\naging the machine’ reflects integrated and complex work, \\nsuch as “coordination and buffering” roles (Langlois, 2003, \\np. 175), as well as trainer, explainer, and sustainer roles \\n(Daugherty & Wilson, 2018). Examples include: manag-\\ning the interactions between data, the wider organisation, \\nand other stakeholders (coordination and buffering); train-\\ning the AI to complete tasks and training others in AI use \\n(training); explaining and interpreting the AI’s operation \\nand outputs to stakeholders (explaining); and ensuring the \\nsystem’s continued explainability, accountability, and fair-\\nness (sustaining) (Daugherty & Wilson, 2018). In contrast, \\n‘minding the machine’ work involves tasks such as “AI prep-\\naration” (sourcing, annotating, and labelling data) and “AI \\nverification” through validating AI output (such as checking \\nimage recognition accuracy) (Tubaro et\\xa0al., 2020, p. 1). This \\ntype of work tends to reflect fragmented and disconnected \\nmicro-work tasks that are often outsourced to low wage and \\nlow skill workers (Tubaro et\\xa0al., 2020), leading to charac-\\nterisations of “janitor work” and new digitalised forms of \\nTaylorism (Jarrahi, 2019, p. 183).\\nIn the third path, AI ‘amplifies’ or ‘assists’ workers \\nby improving how human workers do their existing work \\n(Daugherty & Wilson, 2018). This is akin to AI assisting \\nworkers with their tasks and/or augmenting and enhanc-\\ning workers’ abilities. Here AI is neither assuming specific \\ntasks that a human previously did (as in the first path) nor \\ndoes managing the AI constitute a worker’s primary role \\n(as in the second path), but rather the technology assists the \\nworker to do her existing work better. For example, the AI \\nCorti provides real-time assistance to emergency operators \\nby analysing callers’ responses to questions, assessing the \\nseverity of their condition, and recommending actions to the \\noperator based on modelling of thousands of previous calls \\n(Formosa & Ryan, 2021). This amplifies, in a significant \\nnew way, the abilities of emergency operators to determine \\noptimal responses. The use of AI to amplify a human worker \\naccords with Zuboff’s (1988) “informating” powers of tech-\\nnology, whereby it improves humans’ access to integrated \\nand more meaningful forms of data, often cross-functionally, \\nto generate new insights (see Jarrahi, 2019).\\nWe now analyse how, through each of these three deploy-\\nment pathways, AI use will impact the five dimensions of \\nmeaningful work. While individual jobs could experience \\nelements of all three paths (e.g., some replacing, some ‘tend-\\ning the machine’ work, and some amplifying) and some \\noverlap may occur (e.g., AI replacing a rote human task also \\nassists the worker), we discuss each path as distinct for ana-\\nlytical purposes. The ethical implications of these impacts \\nare then assessed in the subsequent section.\\nTask Integrity and\\xa0Skill Cultivation and\\xa0Use\\nWorkers’ tasks can range from being highly fragmented to \\nbeing highly integrated, and the diversity of skills they can \\nactivate will also vary as a result. Both of\\xa0these aspects gen-\\nerate opportunities to achieve and develop one’s abilities \\nand potential through work (Lips-Wiersma & Morris, 2009). \\nAs the nature of what a worker does (i.e., tasks) strongly \\nimpacts what they need to do that work (i.e., skills), we dis-\\ncuss these two dimensions together.\\nFirst, we consider the path of AI taking over some tasks \\nwhile leaving workers engaged in other work. The tasks \\nthe AI assumes could be simple or complex (or anything in \\nbetween), but the predominance of narrow AI means it is \\nmainly deployed to replace humans in specific narrow tasks. \\nAn espoused benefit of AI is its ability to undertake simple \\ntasks that are often boring and unchallenging for humans, \\nsuch as collating information for meetings (Pulse\\u2009+\\u2009IT, \\n2020) or assessing fruit quality (Roberts, 2020). Deploying \\nAI in this way is unlikely to generate significant feelings \\nof marginalisation from a wider work process due to the \\nsimple nature of the tasks it is assuming, particularly when \\nthe human takes on other comparable or more interesting \\nwork. This should result in neutral or improved perceptions \\n5\\u2002 We acknowledge that other forms of new human work are also \\nlikely to emerge (see Acemoglu & Restrepo, 2020), but its nature \\nremains speculative. New work associated with AI management \\nalready exists or is emerging, aligning with our focus on near-term \\nwork implications of AI.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 6}, page_content='731\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nof task integrity and may free workers’ time to engage in \\nmore learning and development.\\nHowever, when AI assumes more complex and significant \\ntasks then its implications, both positive and negative, may \\nbe more profound. For example, in human resource man-\\nagement an AI can shortlist candidates to progress to inter-\\nviews based on natural language processing of applications \\n(Bankins, 2021; Leicht-Deobald et\\xa0al., 2019). Shortlisting \\napplicants can be a complex and significant component of \\nthe recruitment and selection process. Using AI for this task \\ncould then degrade workers’ experiences of task integrity as \\nthey no longer undertake a significant part of a work process, \\nassuming this work is not comparably replaced. Shifting \\nworkers to other more rote tasks, despite adding work that \\nmaintains their level of involvement in the work process, is \\nalso likely to compound feelings of reduced task integrity, as \\nthe worker moves from undertaking more significant to less \\nsignificant work. This can also limit the scope for workers to \\ndevelop and express their full capabilities at work and reduce \\ntheir opportunities for growth.\\nIn contrast, if workers shift to new but similarly complex \\nor even more significant tasks elsewhere in the work process, \\nthen this should support task integrity as the worker contin-\\nues to contribute meaningfully to work outcomes. For exam-\\nple, the AI ‘AlphaFold’ developed by DeepMind is designed \\nto automate and accelerate the process of determining pro-\\ntein structures, an important step in developing new treat-\\nments for human diseases (Hassabis & Revell, 2021). While \\nAlphaFold can assume significant tasks previously done by \\nhuman scientists (i.e., determining protein structures) this \\nshould positively impact, or at least have a neutral effect, \\non task integrity if it allows scientists to re-focus their work \\nefforts on other important aspects of their broader goal of \\ncuring diseases. However, there remain risks to AI being \\nused in this way. Continuing with this example, if scientists \\nhave trained for many years to do the experimental work \\nthat AlphaFold can now do more quickly and accurately, \\nthis generates significant risks for their ability to exercise \\ntheir full capacities, demonstrate their mastery, and utilise \\nthe skills they have invested years in developing to reach \\ntheir full potential.\\nChanges in skill cultivation and use due to technology \\nreplacing either simple or complex tasks also raises deskill-\\ning concerns, whereby skilled human work is offloaded to \\nmachines resulting in skill loss (Vallor, 2015). Ethically, it \\nis critical to establish whether the human skills lost (i.e., \\noffloaded to machines) are important and whether they can \\nbe exercised and maintained through other forms of work or \\nin other life domains (Michaelson et\\xa0al., 2014; Wolf, 2010). \\nAs simple and rote work generally requires basic skills that \\ncan be cultivated elsewhere or are not significant, there is \\nlimited scope for significant deskilling in this case. How-\\never, complex tasks generally require complex skills, such \\nas judgement, intuition, context awareness, and ethical think-\\ning. From a deskilling perspective, these types of skills are \\nparticularly ethically problematic for workers to risk losing. \\nThis means when workers are left with fewer overall com-\\nplex and significant tasks following AI deployment, then \\ntheir ability to cultivate and use important skills will likely \\ndecrease, negatively impacting this dimension of meaning-\\nful work.\\nIt is worth noting that where replacement involves AI \\nassuming a worker’s whole job, for example where the job \\nis constituted entirely of simple and rote tasks that are most \\nsusceptible to full automation (Gibbs, 2017), this will likely \\nlead to unemployment (if redeployment is not possible). This \\neffectively removes, at least temporarily, paid meaningful \\nwork from that worker’s life and poses the greatest risk to \\nthe ability to experience meaningful work. This also pro-\\nvides the conditions for a wide range of skills to be lost or \\ndegraded, as well as having significant negative impacts on \\nimportant self-attitudes, such as feelings of self-respect and \\nself-worth\\xa0(see Selenko et\\xa0al., 2022 for work on AI use and \\nemployees’ sense of\\xa0identity). This case also raises broader \\npolitical questions about how society should deal with such \\na scenario should it become more widespread (Hughes, \\n2014). While these questions are beyond our focus here, \\nwe do highlight them in our discussion of future research \\ndirections.\\nSecond, we consider the path of workers ‘tending the \\nmachine’, whether in ‘managing’ or ‘minding’ forms. ‘Man-\\naging the machine’ work should enhance what Bourmault \\nand Anteby (2020, p. 1453) term “administrative responsi-\\nbility”, through offering a wider scope and variety of duties. \\nThis should enhance task integrity where the shift to coor-\\ndination and buffering work provides opportunities for inte-\\ngrated and challenging activities across training, explaining, \\nand sustaining roles through supervisory work, technology \\noversight, exceptions management, and cross-functional \\ncoordination of entire work processes. Such coordination \\nand buffering work will also require the development of \\nflexible and wide-ranging skill sets (Langlois, 2003), sup-\\nporting skill cultivation and use and more broadly widening \\nand deepening one’s ability to learn, achieve, and develop \\nat work.\\nIn contrast, rather than generating more complex and \\ninteresting human work, ‘minding the machine’ produces a \\n“more benignant role for humans” through more mundane \\nand rote tasks (Langlois, 2003, p. 174). This would reduce \\ntask integrity as workers become more distanced from their \\nwork outcomes. The generally repetitive and fragmented \\nnature of ‘minding the machine’ work also suggests its asso-\\nciated skills are low and narrow, offering little opportunity \\nfor varied skill cultivation. Such AI “janitor work” (Jarrahi, \\n2019, p. 183) risks degrading workers’ abilities to meaning-\\nfully develop their capabilities and reach and express their'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 7}, page_content='732\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nfull potential at work, leading to lower levels of meaningful-\\nness on this dimension.\\nThird, when AI amplifies workers’ abilities to do their \\ncurrent tasks, positive impacts on task integrity and skill \\ncultivation and use should ensue. For example, in the polic-\\ning domain, machine learning technologies\\xa0can collate previ-\\nously disparate data sources to analyse characteristics and \\nhistories of domestic violence victims and perpetrators to \\nbetter predict, compared to current human-driven systems, \\nrepeat attacks and better prioritise preventative actions \\n(Grogger et\\xa0al., 2020).6 In such cases, experiences of task \\nintegrity are likely to remain consistent or improve as AI \\nsupports workers to better complete their tasks and achieve \\nwork goals. Skill cultivation and use should remain neutral \\nor improve as it is likely that workers, while maintaining \\ntheir current skills, will need to develop new ones to inter-\\npret and integrate AI output into their decision making.\\nHowever, a feature of AI that may constrain skill use \\nacross all three paths is its ‘blackbox’ nature (Boden, 2016). \\nWhile AI designers are developing ways to improve lay per-\\nson interfaces, the use of ‘blackbox’ (or unexplainable) AI \\nin workplaces may degrade workers’ skill cultivation, use, \\nand feelings of competence. For example, where workers \\nare highly reliant on the decision making of an AI, they may \\nfeel lower levels of competence in their use of it due to little \\nunderstanding of its functioning. This effect will likely be \\nmore acutely felt where workers are expected to understand \\nand explain what the AI is doing. Poor explainability can \\nalso create opaque chains of accountability for decisions \\ninformed by AI (Dahl, 2018) and this risks making workers \\noverly dependent on an AI that they cannot comprehend.\\nTask Significance\\nTask significance means employees see their work as having \\npositive impacts (Grant, 2008) through their service to oth-\\ners (Lips-Wiersma & Morris, 2009), within or outside the \\norganisation (Hackman & Oldham, 1975). Task significance \\nis influenced by how employees assess their job impact on \\nand contact with beneficiaries (Grant, 2007). Job impacts \\non beneficiaries are shaped by the dimensions of magni-\\ntude, scope, frequency, and focus (i.e., preventing harms \\nor promoting benefits) (Grant, 2007). Contact with benefi-\\nciaries is shaped by the dimensions of frequency, duration, \\nphysical proximity (including virtual proximity), depth, and \\nbreadth of contact (Grant, 2007). Given the range of these \\ndimensions, workers’ assessments of task significance can \\nbe complex. Evidence suggests that\\xa0employees can derive \\ntask significance from even objectively rote, mundane, and \\nlow skill work (the\\xa0objective element\\xa0of meaningful work), \\nwhen that work is framed in the right way (the\\xa0subjective \\nelement\\xa0of meaningful work). For example, Carton (2018, \\np. 323) shows that when leaders at NASA carefully framed \\nthe space agency’s goals, workers could connect work such \\nas “mopping the floor” to “helping put a man on the moon”. \\nHowever, carrying out impactful tasks without opportuni-\\nties for “personal, emotional connections to the beneficiar-\\nies of those tasks” can impede overall experiences of task \\nsignificance (Grant, 2007, p. 398; Bourmault & Anteby, \\n2020). This means that both job impact on beneficiaries and \\ncontact with them are important to assess. Given this, and \\nfollowing Grant (2007), we suggest that employees’ global \\nassessments of the impact of AI on their jobs, rather than on \\nspecific tasks, is most relevant when assessing perceptions \\nof task significance.\\nFirst, we consider the path of AI taking over simple or \\ncomplex tasks while workers remain engaged elsewhere. \\nGiven our focus here\\xa0at the job level, if only some sim-\\nple tasks are assumed by an AI this should have limited \\nimpact on task significance, assuming the remaining or \\nnew tasks provide opportunities for workers to positively \\nimpact and connect with beneficiaries. In contrast, when \\nAI assumes more complex tasks, these are likely significant \\nto an individual’s overall assessments of task significance. \\nThis may lead to more extensive and complex sensemak-\\ning of this change. To see this, we draw on construal-level \\ntheory (CLT), which describes the way individuals cogni-\\ntively represent people or events at either higher or lower \\nlevels of abstraction (Trope & Liberman, 2003). Higher lev-\\nels of abstraction involve “mental representations that are \\nrelatively broad, inclusive, (and) general”, such as higher-\\nlevel goals or principles (Wiesenfeld et\\xa0al., 2017, p. 368). \\nLower levels of abstraction involve “applying relatively \\nspecific, detailed, and contextualised representations”, such \\nas focusing on lower-level actions to achieve higher-level \\ngoals (Wiesenfeld et\\xa0al., 2017, p. 368). Returning to the \\nearlier\\xa0AlphaFold example, at a higher level of construal \\nworkers may perceive improved task significance regard-\\ning job impact as the AI is significantly contributing to the \\nhigher-level goal of treating diseases. This could facilitate \\nhigher perceptions of magnitude, scope, and frequency of \\npositive impact. At a lower level of construal, the worker \\nmay then ask: “but what am I doing to help meet this goal?”. \\nIf workers can re-focus on other comparatively significant \\ntasks in the work process, they should experience higher \\ntask significance as the AI helps advance the field toward \\nreaching the overarching goal and the worker continues to \\nmeaningfully contribute toward that goal. However, where \\nthe remaining or new tasks fail, at lower levels of construal, \\nto deliver at least the same experiences of task significance \\n6\\u2002 Although in practice such predictive policing systems have been \\nshown to risk biased outcomes against minority groups, driven by \\nover-representation of those groups in policing statistics (Berk, 2021). \\nWe discuss these issues in a later section.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 8}, page_content='733\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nas before, then one’s perceived ability to ‘serve others’ is \\nlikely to degrade overall.\\nIn cases of both simple and complex task change, \\nwhere AI is used in ways that have sub-optimal, biased, \\nunjust, or harmful outcomes for end users, this could also \\ndecrease\\xa0workers’ perceptions of task significance. For \\nexample, where AI provides facial recognition and predic-\\ntive policing data for law enforcement agencies and the AI’s \\noutputs are biased against minority groups, then workers \\nmay see reduced task significance given their organisations’ \\nconnections to negative outcomes (via the negative magni-\\ntude, scope, and frequency dimensions of job impact). Impli-\\ncating workers in injustices and harms perpetrated by an \\nAI, through their involvement with or responsibility for the \\ntechnology, can particularly diminish the experience of serv-\\ning others and the autonomous ability to act in alignment \\nwith one’s values and morals (related to ‘developing and \\nbecoming self’), degrading overall work meaningfulness.\\nSecond, we consider the ‘tending the machine’ path. In \\nterms of ‘managing the machine’, the heightened adminis-\\ntrative responsibility associated with such work (Bourmault \\n& Anteby, 2020) should improve task significance through \\njob impact and more opportunities to benefit others, given \\nthe expansive duties this work entails (i.e., enhanced scope \\nof impact). However, such work can distance workers from \\nthose they serve, diminishing feelings of direct “personal \\nresponsibility” (Bourmault & Anteby, 2020, p. 1453) or feel-\\nings of having a direct and significant impact on the lives \\nof others, which can reduce task significance. For example, \\nwhen replaced by autonomously driven trains and moved to \\n‘managing the machine’ work, metro train drivers experi-\\nenced enhanced administrative responsibility but diminished \\npersonal responsibility, alongside lower task significance \\noverall, as they were no longer directly responsible for com-\\nmuters’ safety (Bourmault & Anteby, 2020).\\nIn terms of ‘minding the machine’ work we suggest that \\ntask significance will generally be reduced. This is because \\nsuch fragmented work means workers may have little idea of \\nthe point of their labour and its impacts, potentially limiting \\nall job impact dimensions. As they may also\\xa0be working in \\nisolation from others because of outsourcing\\xa0(Tubaro et\\xa0al., \\n2020), potentially limiting all contact with beneficiaries, this \\nfurther disconnects workers’ tasks from the end user benefits \\ngenerated, eroding task significance.\\nThird, when AI amplifies a worker’s abilities this should \\nhave significant and positive implications for task signifi-\\ncance, particularly through the magnitude and focus dimen-\\nsions of job impact and the duration and depth dimensions \\nof contact with beneficiaries. Here, the AI is not focused \\non substantially changing the range of tasks in a work pro-\\ncess, but rather on improving something that humans were \\nalready doing in that process, leading to better outcomes for \\nbeneficiaries. Drawing on earlier amplification examples, \\nwhere AI can support police officers by collating and ana-\\nlysing new data sources to help them better prevent inci-\\ndences of domestic violence (assuming it does not do so in \\nunfair or biased ways), then this should heighten percep-\\ntions of both being able to achieve higher-level goals (e.g., \\npreventing crime) and seeing the importance and connec-\\ntion of lower-level tasks to reaching that goal (e.g., through \\ninterpreting better predictive analytics). Use of AI in this \\nway can also help reduce human biases in decision making, \\nsuch as through building fairness principles into AI systems \\n(see Selbst et\\xa0al., 2019). In recruitment, for example, AI \\ncan limit the impact of unconscious human biases and vari-\\nous other human constraints on rational decision making by \\nassessing all candidates’ applications against standard cri-\\nteria and providing auditable, transparent, and explainable \\ndecision trails (see Hagras, 2018; Bankins et\\xa0al., 2022). This \\npath demonstrates that a significant potential benefit of AI \\nis that it can elevate humans’ abilities to address complex \\nproblems, enhance the impact of their work, and thus better \\nserve others, through its analysis of large datasets to identify \\nnovel insights.\\nAutonomy\\nAutonomy means self-rule. Individually, that means being \\nable to do what you really want to do. In addition to the free-\\ndom from interference needed to rule yourself, autonomy is \\nalso commonly taken to include competency (i.e., you have \\nthe skills and capacities needed to rule yourself) and authen-\\nticity (i.e., your ends are authentically your own and not the \\nresult of oppression, manipulation, or coercion) conditions \\n(Formosa, 2021). In the workplace, autonomy refers to “the \\ndegree to which the job provides substantial freedom, inde-\\npendence, and discretion to the individual in scheduling the \\nwork and in determining the procedures to be used in carry-\\ning it out” (Hackman & Oldham, 1976, p. 258). AI’s impact \\non individuals’ autonomy is a key issue for the ethical AI \\nliterature. A particular concern is that ceding authority to AI \\ndiminishes human autonomy (Floridi et\\xa0al., 2018). However, \\npotential benefits for human autonomy can also accrue from \\nincreasing AI’s autonomy. We assess these different impacts \\nof AI at work as either promoting or diminishing autonomy \\nacross competency and authenticity conditions.\\nIn terms of promoting human autonomy, this depends on \\nwhat work the AI assumes but also, and more importantly, \\non what work takes its place and what control and input \\nworkers have over AI deployment. When AI assumes simple \\nor complex tasks that workers find boring or repetitive, then \\nthis potentially promotes autonomy by freeing up time for \\nworkers to build their autonomy competencies through doing \\nother more challenging or authentic work. For example, if an \\nAI prioritises a worker’s emails so that she only sees those \\nrequiring a response, this may free her to work on other'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 9}, page_content='734\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nmore valuable tasks. In terms of ‘managing the machine’, \\nthis path could promote autonomy if new work is more skil-\\nful and engaging than the work it replaces, and if workers \\nhave a degree of control over how that work is done. Where \\nAI amplifies workers by giving them more power and use-\\nful information, then this can improve worker autonomy by \\nhelping them to better achieve their self-given ends.\\nIn terms of diminishing human autonomy, these impacts \\nare partly the converse of the above. In our first path, if \\ncomplex, interesting, and creative tasks that workers want \\nto do are assumed by AIs, this potentially diminishes auton-\\nomy. There may also be good reasons why humans should \\nremain engaged in certain complex tasks and decisions, such \\nas due to their moral complexity. This means that where AI \\nassumes these tasks it can diminish human achievement of \\nvaluable ends, degrade important human skills, and limit \\nopportunities for moral development (Lips-Wiersma & Mor-\\nris, 2009). For example, when we delegate to AI decisions \\nregarding ethically sensitive aspects of human resource man-\\nagement, the skills associated with that work can degrade \\nand thereby diminish important autonomy competencies. AI \\ncan also make our autonomy more vulnerable by making us \\ndependent on it, which means our autonomy can diminish \\nif access to the technology is removed. Across the ‘tending \\nthe machine’ path, through ‘managing the machine’ work AI \\ncan diminish worker autonomy by filtering and potentially \\nrestricting the information that is made available for humans \\nto view and use (Kellogg et\\xa0al., 2020). Such constraints can \\nlimit the ability for workers to authentically develop them-\\nselves and their capabilities at work. Broader autonomy con-\\ncerns also exist with ‘minding the machine’ work, which \\nis itself mundane and boring, making workers feel like a \\n‘slave to the machine’ (Engel, 2019) and thereby experienc-\\ning\\xa0diminished autonomy at work.\\nAcross all paths, a more pernicious threat to autonomy \\nmay exist through surveillance and manipulation by AI. \\nThis reflects what Foucault calls the rise of a “surveillance \\nsociety”, which seeks to control bodies through making \\npeople feel permanently monitored (Abrams, 2004). When \\npeople are surveilled they tend to feel constrained and act \\nin less authentic and autonomous ways (Molitorisz, 2020). \\nThe use of AI to surveil workers will likely have similar \\nimpacts and can be a way for employers to use their power \\nto exert control over employees. For example, the use of \\nAI-powered cameras to surveil Amazon delivery drivers \\ncould make them more self-conscious in their trucks, which \\ncould lead them to feel more constrained and unable to act \\nautonomously (Asher-Schapiro, 2021). A similar example \\nis when AI is implemented to monitor online meetings and \\nmeasure whether workers are engaged and contributing to \\nthe discussion (see Pardes, 2020), which could lead to stress \\nand inauthentic behaviour. Such monitoring could also result \\nin workers engaging in intentional “deviance” to challenge \\nthe control of surveillance (Abrams, 2004), by trying to \\n“game” the AI by matching or openly flouting what the \\nAI is expecting in terms of eye contact and body language \\n(Pardes, 2020), or finding other ways to operate outside the \\ngaze of the surveillance system.\\nBelongingness\\nBelongingness refers to “the meaningfulness of work-\\ning together with other human beings” (Lips-Wiersma & \\nWright, 2012, p. 673). Across all our paths, we argue that \\nAI may impact workers’ belongingness in two main ways: \\nthrough generating the conditions for more or less meaning-\\nful connections and a sense of unity with others; and through \\nits implementation creating differences across workers that \\nundermines solidarity.\\nIn terms of the first way, where AI assumes tasks that \\nmay otherwise have required in-person and face-to-face \\ninteraction with other workers or customers, this can create \\nless human contact in the workplace. For example, where \\nan AI chatbot allows workers to access information previ-\\nously provided by a human worker, this lessens that worker’s \\ninteractions with other humans and reduces opportunities \\nfor forming connections with others that are the bedrock for \\ngenerating a sense of belonging (Seppala et\\xa0al., 2013). In \\ncontrast, AI use may increase opportunities for human inter-\\naction, for example through ‘managing the machine’ work \\nwhere workers are responsible for supervising AI deploy-\\nment that requires extensive human-to-human training.\\nIn terms of the second way, a key concern in the ethical \\nAI literature is how AI use may disproportionately and nega-\\ntively affect lower-skilled and lower-paid workers, while its \\nbenefits may disproportionately accrue to those with higher \\nskills and wages (Ernst et\\xa0al., 2018), effectively creating new \\ntypes of workplace in-groups and out-groups. For example, \\nmany of the negative impacts of AI at work, such as surveil-\\nlance and simplistic ‘minding the machine’ work, will tend \\nto fall on less skilled ‘blue collar’ workers, whereas more of \\nthe amplifying and autonomy-enhancing benefits associated \\nwith taking on even more interesting and engaging work \\nwill tend to fall to already privileged workers. This creates \\njustice concerns around how the benefits and burdens of AI \\nin workplaces are being distributed, potentially undermining \\nsolidarity between those who benefit from AI’s introduction \\nand those who do not. For example, in a call centre context \\nan AI may be used to monitor and evaluate the calls of every \\ncall centre operator. Such heightened surveillance may be \\nperceived by operators as intrusive and diminishing their \\nautonomy. However, using AI in this way may amplify the \\nwork of quality assurance staff in the same organisation, \\nproviding them with more information and assisting them \\nin better training and managing operators. This shows how \\nAI may generate distinct groups experiencing very different'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 10}, page_content='735\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nimpacts, such as being viewed as unnecessary surveillance \\nby some but as an amplifying source of information by oth-\\ners. Such outcomes particularly threaten the ability to create \\na sense of belongingness and shared values (Lips-Wiersma \\n& Morris, 2009), which underpins the ‘unity with others’ \\ndimension of meaningful work.\\nEthical Implications: AI and\\xa0Meaningful \\nWork\\nWe have analysed how the three paths of AI deployment \\nmay enhance or diminish opportunities for meaningful work \\nacross five dimensions. We now surface the ethical implica-\\ntions of this analysis via the five principles of the AI4Peo-\\nple ethical AI framework (Floridi et\\xa0al, 2018): beneficence; \\nnon-maleficence; autonomy; justice; and explicability. As \\nwith any principlist framework there are potential conflicts \\nand tensions between principles (Formosa et\\xa0al., 2021). For \\nexample, there may be benefits for some from AI deploy-\\nment (beneficence) while others suffer harm (non-malefi-\\ncence) or interference with their autonomy. As identified \\nearlier, the\\xa0provision of meaningful work is not always the \\nonly or most important ethical value at stake, and so less \\nmeaningful work may not be ethically worse overall if there \\nare other ethical benefits, such as improved wellbeing for \\nothers through higher productivity.\\nTo assess ethical implications, we synthesise and summa-\\nrise how the three paths (replacing, ‘tending the machine’, \\nand amplifying) support or limit experiences of meaningful \\nwork and so contribute to, or diminish, meeting the AI4Peo-\\nple principles. We summarise these impacts in Table\\xa01 across \\nthe five ethical principles (beneficence and non-maleficence \\nare combined in the Table as the latter reflects the converse \\nof the former), while noting the main deployment pathways \\nthrough which these impacts occur.\\nIn terms of the beneficence principle, there can be sig-\\nnificant benefits for employees when AI use supports the \\nvarious dimensions of meaningful work. When AI amplifies \\na worker’s skills it can support them to complete their tasks, \\nundertake more complex tasks, and utilise higher-order \\nthinking and analysis skills (task integrity and skill cultiva-\\ntion and use). It can also afford workers the opportunity to \\nachieve better outcomes and enhance the positive impact of \\ntheir work on beneficiaries (task significance), give them \\nmore control over their work through improved access to \\ninformation (autonomy), and potentially generate new con-\\nnections with other workers and stakeholders (belonging-\\nness). Similarly, when AI assumes some simple or complex \\ntasks and the human worker can re-focus on other impor-\\ntant and challenging tasks in the work process, then posi-\\ntive experiences across all dimensions of meaningful work \\nshould be maintained or improved. ‘Managing the machine’ \\nwork can also improve meaningfulness through a wider \\nscope of enriched work (task integrity and skill cultivation \\nand use) and a wider positive job impact within and outside \\nthe organisation (task significance), as well as greater inter-\\naction with a range of stakeholders through coordination and \\nsupervisory work (belongingness).\\nIn terms of the non-maleficence principle, we also show \\nthe harms that AI can create when it is deployed in ways that \\nlead to less (or no) meaningful work, or other related harms. \\nTwo paths generate greatest risk of harms through signifi-\\ncantly reducing experiences of meaningful work. First, when \\nAI replaces some tasks, the risk of degraded task integ-\\nrity, deskilling, reduced task significance, and constrained \\nautonomy is greatest when it assumes more complex tasks \\nand the worker is not afforded any new comparable or more \\ninteresting work. This is because complex tasks generally \\nconstitute a large and significant part of the work process \\nand undertaking them exercises a range of important skills. \\nBeing removed from such work can also distance workers \\nfrom the output of their labour and lower perceptions of \\nbeneficiary impact. In the worst case, it could involve the \\ncomplete loss of paid meaningful work where AI replaces \\nwhole jobs, which removes workers from important social \\nrelationships and denies them the opportunity to skilfully \\nutilise their talents to help others. Second, ‘minding the \\nmachine’ work, as we have characterised its fragmented, \\npiecemeal, and micro-work nature, threatens these same \\naspects of meaningful work and feelings of belongingness \\nwhen work is outsourced to disconnected workers. Other \\npaths can also generate harms, but arguably at lower lev-\\nels. For example, we identified that while ‘managing the \\nmachine’ work may increase meaningful work experiences \\noverall through heightened administrative responsibility, it \\ncan lessen feelings of task significance by increasing dis-\\ntance between workers and their beneficiaries and reducing \\nfeelings of personal responsibility.\\nIn terms of the autonomy principle, across each path we \\nshow how autonomy is supported when AI is used to free \\nup humans to focus their time on other more valued tasks, \\nallows them to develop new or enhanced autonomy com-\\npetencies, and gives them more control over their work. In \\nparticular, the task replacement, ‘managing the machine’, \\nand amplifying paths that afford employees access to bet-\\nter data and information, the opportunity to engage in more \\ninteresting work, and exercise more control over how their \\nwork is done, can all promote autonomy as a dimension of \\nmeaningful work. However, many of these positive impacts \\nalso depend on whether workers have input into how AI \\nis deployed in their organisations. A particular risk to \\nautonomy is the use of AI to surveil and monitor, which can \\nundermine authenticity and encourage workers to align their \\nbehaviours with the AI’s implicit expectations or seek ways \\nto subvert or avoid its control.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 11}, page_content='736\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nThe justice principle centres on ensuring fair, just, and \\nnon-discriminatory outcomes from AI and requires a focus \\non how the benefits and burdens of AI use are distributed. \\nFor example, the amplifying path generally achieves strongly \\npositive outcomes for meaningful work, but there is evidence \\nthat such benefits are disproportionately allocated to already \\nprivileged workforces (i.e., higher-skilled and higher-paid \\nworkers). In contrast, the ‘minding the machine’ path gen-\\nerally achieves strongly negative outcomes for meaningful \\nwork, but such burdens tend to disproportionately impact \\nless privileged workforces (i.e., lower-paid and lower-skilled \\nworkers). Lower-skilled workers are also more likely to \\nhave their entire jobs replaced by AI (Gibbs, 2017). This \\nuneven distribution raises important justice concerns and \\ncan undermine solidarity and feelings of belongingness \\nwithin and across work groups. However, AI can also be \\ndeployed to promote justice, which can positively impact \\ntask significance. For example, when AI is used to mini-\\nmise bias and maximise evidence-based decision making \\nthrough giving workers access to new data-driven insights \\n(such as through amplification, ‘managing the machine’, or \\nreplacing complex tasks paths), this promotes fair outcomes \\nwhile also enhancing task significance through a greater \\npositive impact on beneficiaries. But the converse also holds \\nwhen the justice principle is threatened by an AI trained on \\nbiased datasets and deployed in workplaces where it gener-\\nates unjust outcomes that can decrease task significance and \\nimplicate workers in injustices.\\nFinally, the explicability principle relates to the explain-\\nability, transparency, and accountability of AI. In paths \\nwhere AI plays a significant role alongside human workers, \\nsuch as the amplifying, ‘managing the machine’, and the \\nreplacement of complex tasks paths, an inability of work-\\ners to understand an AI’s operation, particularly where they \\nTable\\u202f1\\u2002 \\u2009Ethical impacts of AI for enhancing or diminishing meaningful work\\nEthical aspects of meaningful work (AI4 \\nPeople principles)\\nHow AI could enhance meaningful work (and \\nmain pathways)\\nHow AI could diminish meaningful work (and \\nmain pathways)\\nBeneficence & Non-maleficence\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Less boring and repetitive human work\\n• Increased opportunities for new and/or more \\nchallenging human work\\n• Enhanced human learning, skills, and \\ndevelopment\\n• Augmenting and assisting human workers\\n• Facilitating higher positive impacts on \\nothers\\nMain pathways: Replacing & minding the \\nmachine\\n• Loss of work for humans\\n• Reduced human role in the work process\\n• Human deskilling\\n• More boring and fragmented ‘minding the \\nmachine’ work\\n• Less belonging and less human interaction\\nAutonomy\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Improved autonomy competencies (e.g., \\nthrough more time for valuable work)\\n• More control and power for workers through \\ngreater access to information\\nMain pathways: All pathways, especially mind-\\ning the machine\\n• Reduced autonomy competencies (e.g., \\nthrough deskilling)\\n• Surveillance\\n• Manipulation & nudging (e.g., controlling \\nhuman behaviour)\\n• Employers exerting more power and control \\nover workers\\n• Worker vulnerability & dependence on AI\\n• Inauthentic behaviours (e.g., acting more self-\\nconsciously)\\n• Resistance to AI and greater deviance (e.g., \\n‘gaming’ the AI)\\nJustice\\nMain pathways: Replacing & amplifying\\n• More data-driven choices\\n• Less human bias in decision making\\n• Fairer decision making\\nMain pathways: All pathways\\n• Unfair distribution of benefits and burdens \\n(e.g., lower-skilled workers suffer more bur-\\ndens and receive less benefits)\\n• Undermining solidarity\\n• Implicating workers in injustices resulting \\nfrom AI bias\\nExplicability (explainability & accountability) Main pathways: Managing the machine & \\namplifying\\n• Informating (i.e., improved access to infor-\\nmation)\\n• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 12}, page_content='737\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nare highly reliant upon it and are\\xa0accountable for it, can \\nconstrain skill use and feelings of competence. This could \\npotentially undermine the benefits AI may otherwise bring. \\nThis suggests that training workers not only in what AI does \\nbut also how it does it, and making chains of accountability \\nclear, will be important for supporting experiences of mean-\\ningfulness at work.\\nPractical Implications\\nOrganisational use of AI can reap many benefits through \\nimproved service range and quality, efficiency, and profit-\\nability. However, the ethical deployment of AI requires \\nweighing up its many costs and benefits. We help articulate \\nsome of those costs and benefits for workers in terms of \\nAI’s impacts on meaningful work. Practically, this is impor-\\ntant because some authors suggest an emerging trend is for \\norganisations to use AI for full automation (Acemoglu & \\nRestrepo, 2020), without also considering opportunities to \\nuse it for enhancing human work, and then poorly preparing \\ntheir workforces for the changes that\\xa0AI use entails (Hal-\\nloran & Andrews, 2018). For organisations we highlight \\nthose pathways, such as ‘minding the machine’ work, that \\nare likely to significantly limit opportunities for meaningful \\nwork, which implies that other considerations such as effi-\\nciency benefits must strongly outweigh the harms to workers \\nthat AI used in this way can generate, in order to justify its \\nuse. We also highlight that, when considering meaningful-\\nness, it is insufficient to focus only on the AI itself, as the \\nimplications of its deployment are strongly driven by what \\nwork remains for humans, which is something that organi-\\nsations can directly influence and decide. Overall, we offer \\nguidance on how organisations can maintain or build oppor-\\ntunities for meaningful work when they implement AI and \\npoint leaders toward specific areas for intervention to sup-\\nport meaningful work experiences. For example, task sig-\\nnificance is critical for meaningful work (Grant, 2007), yet \\nthe ways AI can distance workers from beneficiaries threat-\\nens these experiences. However, there are ways in which \\norganisations can remedy this, such as by sharing end users’ \\npositive stories with workers (Grant, 2008).\\nFuture Research Directions\\nAlthough we did not frame explicit propositions from our \\nconceptual work, there are several relationships we sug-\\ngest warrant empirical examination. For example, assess-\\ning\\xa0whether AI performing simple tasks enhances task \\nintegrity, but AI performing complex tasks degrades this \\ndimension and perceptions of task significance, and\\xa0exam-\\nining whether the ‘managing the machine’ and amplifying \\npaths enhance task integrity and skill cultivation and use \\noverall, but ‘minding the machine’ work diminishes these \\naspects. We also suggest several contingencies will affect \\nthese relationships, such as what other or new work employ-\\nees do following AI implementation (task-related factors) \\nand how aspects of the technology, such as its explainabil-\\nity or potential for bias, shape workers’ experiences of it \\n(technology-related factors).\\nWhile we adapted Langlois’ (2003) work to develop \\nour three pathways, these may manifest in different ways \\nand will likely overlap. Future research could explore how \\neach path operates in workplaces, how they may differ from \\nour conceptualisation, and whether there are other path \\nconfigurations to AI deployment that our framework does \\nnot capture. There may also be nuances within pathways \\nthat warrant investigation. For example, Jarrahi (2018, p. \\n3) suggests that advances in AI could create new forms of \\n“human–machine symbiosis” that result in “both parties \\n(becoming) smarter over time”. This could generate new \\nforms of human skills, tasks, and perhaps whole jobs that \\nhave not yet been imagined, with implications for meaning-\\nful work.\\nAnother area for future work is examining how lead-\\ners construct and influence subjective perceptions of the \\nmeaningfulness of work, particularly through the values, \\nstrategies, and vision that underpin how they implement AI \\n(Pratt & Ashforth, 2003). For example, if an organisation is \\nfocused on full automation and replacing human workers, \\nit will likely deploy AI toward this end and degrade oppor-\\ntunities for meaningful work. But if leaders adopt multi-\\nstakeholder governance approaches that support ethical AI \\ndeployment (Wright & Schultz, 2018), such participatory \\npractices may enhance perceptions of meaningful work fol-\\nlowing AI deployment.\\nFinally, while we centred our analysis on the meaning-\\nful work implications of narrow AI, future work could uti-\\nlise conceptual tools such as thought experiments (Bankins \\n& Formosa, 2020) and work on posthumanism (Gladden, \\n2016) to prospectively analyse the impacts of potential \\nfuture forms and deployments of more advanced AI. For \\nexample, developments in virtual and augmented reality are \\ncreating movements toward a metaverse, or a persistent form \\nof virtual world that is accessible through various devices \\nand that people combine with their existence in the physi-\\ncal world (Ravenscraft, 2021). Such technologies have the \\npotential to transform the nature of social interactions and \\nthus impact the belongingness dimension of meaningful \\nwork. Likewise, advances in natural language processing \\nand speech interfaces could result in workers having multi-\\nple “digital assistants” (Zhou et\\xa0al., 2021, p. 258), which will'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 13}, page_content=\"738\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nimpact the nature of workers’ tasks and relationships and the \\nskills they will require in the future.\\nExtending even further, artificial general intelligence \\n(AGI) would constitute “a new general-purpose technol-\\nogy” (Naudé & Dimitri, 2020) that has been predicted to \\npose existential threats such as eradicating large swathes of \\nhuman work (Bruun & Duka, 2018) and even risking human-\\nity’s annihilation (Torres, 2019). The reality of such tech-\\nnologies would inevitably lead to more extreme conclusions \\nfor the future of meaningful work than we have generated \\nhere through our focus on narrow AI, as they would likely \\nrender all but our replacing path largely obsolete. The pos-\\nsibilities of such technologies may therefore lead us back to \\nsubstantive discussions on the value of work generally, and \\nwhat forms of human work we believe must be preserved or \\nnewly created no matter what technologies are developed. \\nThis also raises questions of what broader social changes, \\nsuch as increased volunteering, provision of other forms of \\nmeaningful activity, or a Universal Basic Income (Hughes, \\n2014), will be required to cushion negative impacts should \\nAGI deployment ever become a reality. It also augurs the \\npotential for heavier regulation of the development and use \\nof AI (and potentially AGI) to maintain meaningful forms of \\nhuman employment, and to place limits on where, how, and \\nwhy AI is used. However, at this point the discussion relies \\non largely technical questions about whether AGI is indeed \\npossible (Boden, 2016). In the meantime, the impacts of \\nnarrow AI on meaningful work are ones we need to address \\nhere and now.\\nConclusion\\nThis paper focused on a neglected aspect of the ethical \\nimplications of AI deployment, namely the impacts of AI \\non meaningful work. This is an important contribution as \\nthe ethical AI literature, while focused on the impacts of \\nunemployment resulting from AI, needs to also\\xa0attend\\xa0to \\nthe impacts of AI on meaningful work for the remaining \\nworkforce. Given the ethical importance of meaningful \\nwork and its considerable impacts on human wellbeing, \\nautonomy, and flourishing, this is a significant omission \\nthat we help to remedy. We have done so by examining the \\nimpacts of three paths of AI deployment (replacing tasks, \\n‘tending the machine’, and amplifying) across five dimen-\\nsions of meaningful work (task integrity, skill cultivation \\nand use, task significance, autonomy, and belongingness). \\nUsing this approach, we identify specific ways in which AI \\ncan both promote and diminish experiences of meaningful \\nwork across these dimensions and draw out the ethical impli-\\ncations of this by utilising five key ethical AI principles. \\nFinally, we offer practical guidance for organisations by \\narticulating the ways that AI can be implemented to support \\nmeaningful work and suggest opportunities for future \\nresearch. Overall, we show that AI has the potential to make \\nwork more meaningful for some workers by undertaking less \\nmeaningful tasks for them and amplifying their capabilities, \\nbut that it can also make work less meaningful for others by \\ncreating new boring tasks, restricting worker autonomy, and \\nunfairly distributing the benefits of AI away from less-skilled \\nworkers. This suggests that AI’s future impacts on meaning-\\nful work will be both significant and mixed.\\nAcknowledgements\\u2002 The authors would like to sincerely thank the \\nSpecial Issue Guest Editors, their Action Editor Associate Professor \\nLuke Fletcher, and the anonymous reviewers for their insightful and \\nconstructive feedback during the review process.\\nFunding\\u2002 Open Access funding enabled and organized by CAUL and \\nits Member Institutions.\\nDeclarations\\u2002\\nConflict of interest\\u2002 The authors have no conflicts of interest to declare \\nthat are relevant to the content of this article.\\nOpen Access\\u2002 This article is licensed under a Creative Commons Attri-\\nbution 4.0 International License, which permits use, sharing, adapta-\\ntion, distribution and reproduction in any medium or format, as long \\nas you give appropriate credit to the original author(s) and the source, \\nprovide a link to the Creative Commons licence, and indicate if changes \\nwere made. The images or other third party material in this article are \\nincluded in the article's Creative Commons licence, unless indicated \\notherwise in a credit line to the material. If material is not included in \\nthe article's Creative Commons licence and your intended use is not \\npermitted by statutory regulation or exceeds the permitted use, you will \\nneed to obtain permission directly from the copyright holder. To view a \\ncopy of this licence, visit http://\\u200bcreat\\u200biveco\\u200bmmons.\\u200borg/\\u200blicen\\u200bses/\\u200bby/4.\\u200b0/.\\nReferences\\nAbrams, J. J. (2004). Pragmatism, artificial intelligence, and posthuman \\nbioethics: Shusterman, Rorty, Foucault. Human Studies, 27(3), \\n241–258.\\nAcemoglu, D., & Restrepo, P. (2020). The wrong kind of AI? Artificial \\nintelligence and the future of labour demand.\\xa0Cambridge Journal \\nof Regions, Economy and Society, 13, 25–35.\\nAllan, B. A., Batz-Barbarich, C., Sterling, H. M., & Tay, L. (2019). \\nOutcomes of meaningful work: A meta-analysis. Journal of Man-\\nagement Studies, 56(3), 500–528.\\nAsher-Schapiro, A. (2021). Amazon AI van cameras spark surveil-\\nlance concerns. News.Trust.Org. https://\\u200bnews.\\u200btrust.\\u200borg/\\u200bitem/\\u200b\\n20210\\u200b20513\\u200b2207-\\u200bc0mz7/\\nBailey, C., Yeoman, R., Madden, A., Thompson, M., & Kerridge, G. \\n(2019). A review of the empirical literature on meaningful work: \\nProgress and research agenda. Human Resource Development \\nReview, 18(1), 83–113.\\nBankins, S. (2021). The ethical use of artificial intelligence in human \\nresource management: A decision-making framework.\\xa0Ethics and \\nInformation Technology, 23, 841–854.\\nBankins, S., & Formosa, P. (2020). When AI meets PC: Exploring \\nthe implications of workplace social robots and a human-robot\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 14}, page_content=\"739\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\npsychological contract. European Journal of Work and Organi-\\nzational Psychology, 29(2), 215–229.\\nBankins, S., & Formosa, P. (2021). Ethical AI at work: The social \\ncontract for artificial intelligence and its implications for the work-\\nplace psychological contract. In: M. Coetzee & A. Deas (Eds.), \\nRedefining the Psychological Contract in the Digital Era: Issues \\nfor Research and Practice (pp. 55–72). Springer: Switzerland.\\nBankins, S., Formosa, P., Griep, Y., & Richards, D. (2022). AI decision \\nmaking with dignity? Contrasting workers' justice perceptions of \\nhuman and AI decision making in a human resource management \\ncontext.\\xa0Information Systems Frontiers, 24(3), 857–875.\\nBekey, G. A. (2012). Current trends in robotics. In P. Lin, K. Abney, & \\nG. A. Bekey (Eds.), Robot ethics (pp. 17–34). MIT Press:\\xa0Cam-\\nbridge, Mass.\\nBerk, R. A. (2021). Artificial intelligence, predictive policing, and risk \\nassessment for law enforcement. Annual Review of Criminology, \\n4(1), 209–237.\\nBoden, M. A. (2016). AI. Oxford University Press: UK.\\nBourmault, N., & Anteby, M. (2020). Unpacking the managerial blues: \\nHow expectations formed in the past carry into new jobs. Organi-\\nzation Science, 31(6), 1452–1474.\\nBowie, N. E. (1998). A Kantian theory of meaningful work. Journal \\nof Business Ethics, 17, 1083–1092.\\nBruun, E., & Duka, A. (2018). Artificial intelligence, jobs and the \\nfuture of work. Basic Income Studies, 13(2), 1–15.\\nCamus, A. (1955). The myth of Sisyphus and other essays. Hamish \\nHamilton.\\nCarton, A. M. (2018). I’m not mopping the floors, I’m putting a man \\non the moon: How NASA leaders enhanced the meaningfulness \\nof work by changing the meaning of work. Administrative Science \\nQuarterly, 63(2), 323–369.\\nCheney, G., Zorn Jr, T. E., Planalp, S., & Lair, D. J. (2008). Meaningful \\nwork and personal/social well-being organizational communica-\\ntion engages the meanings of work. Annals of the International \\nCommunication Association, 32(1), 137–185.\\nChui, M., Manyika, J., & Miremadi, M. (2015). The four fundamentals \\nof workplace automation. McKinsey. http://\\u200bwww.\\u200bmckin\\u200bsey.\\u200bcom/\\u200b\\nbusin\\u200bess-\\u200bfunct\\u200bions/\\u200bdigit\\u200bal-\\u200bmckin\\u200bsey/\\u200bour-\\u200binsig\\u200bhts/\\u200bfour-\\u200bfunda\\u200bmenta\\u200b\\nls-\\u200bof-\\u200bworkp\\u200blace-\\u200bautom\\u200bation\\nDahl, E. S. (2018). Appraising black-boxed technology: The positive \\nprospects. Philosophy & Technology, 31, 571–591.\\nDastin, J. (2018, October 11). Amazon scraps secret AI recruiting tool \\nthat showed bias against women. Reuters. https://\\u200bwww.\\u200breute\\u200brs.\\u200b\\ncom/\\u200bartic\\u200ble/\\u200bus-\\u200bamazon-\\u200bcom-\\u200bjobs-\\u200bautom\\u200bation-\\u200binsig\\u200bht-\\u200bidUSK\\u200b\\nCN1MK\\u200b08G\\nDaugherty, P. R., & Wilson, H. J. (2018). Human + Machine: Reim-\\nagining work in the age of AI.\\xa0Harvard Business Review Press.\\nEngel, S. (2019). Minding machines: A note on alienation. Fast Capi-\\ntalism, 16(2), 129–139.\\nErnst, E., Merola, R., & Samaan, D. (2018). The economics of artificial \\nintelligence. International Labour Organization. https://\\u200bwww.\\u200bilo.\\u200b\\norg/\\u200bwcmsp5/\\u200bgroups/\\u200bpublic/\\u200bdgrep\\u200borts/\\u200bcabin\\u200bet/\\u200bdocum\\u200bents/\\u200bpubli\\u200b\\ncation/\\u200bwcms_\\u200b647306.\\u200bpdf\\nFloridi, L., et\\xa0al. (2018). AI4People - An ethical framework for a good \\nAI society. Minds and Machines, 28(4), 689–707.\\nFormosa, P. (2017). Kantian ethics, dignity and perfection. Cambridge \\nUniversity Press: Cambridge.\\nFormosa, P. (2021). Robot autonomy vs human autonomy: Social \\nrobots, artificial intelligence (AI), and the nature of autonomy. \\nMinds and Machines, 31, 595–616.\\nFormosa, P., & Ryan, M. (2021). Making moral machines: Why we \\nneed artificial moral agents. AI & Society, 36, 839–851.\\nFormosa, P., Wilson, M., & Richards, D. (2021). A principlist frame-\\nwork for cybersecurity ethics. Computers & Security, 109, \\n102382.\\nFrey, C. B., & Osborne, M. A. (2017). The future of employment: How \\nsusceptible are jobs to computerisation?\\xa0Technological Forecast-\\ning and Social Change, 114, 254–280.\\nGibbs, M. J. (2017). How is new technology changing job design? IZA \\nWorld of Labor. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b15185/\\u200bizawol.\\u200b344\\nGladden, M. E. (2016). Posthuman management: Creating effective \\norganizations in an age of social robotics, ubiquitous AI, human \\naugmentation, and virtual worlds. Defragmenter Media: USA.\\nGrant, A. M. (2007). Relational job design and the motivation to make \\na prosocial difference. Academy of Management Review, 32(2), \\n393–417.\\nGrant, A. M. (2008). The significance of task significance: Job perfor-\\nmance effects, relational mechanisms, and boundary conditions. \\nJournal of Applied Psychology, 93(1), 108–124.\\nGrogger, J., Ivandic, R., & Kirchmaier, T. (2020). Comparing con-\\nventional and machine-learning approaches to risk assessment in \\ndomestic abuse cases. Journal of Empirical Legal Studies, 18(1), \\n90–130.\\nHackman, J. R., & Oldham, G. R. (1975). Development of the job diag-\\nnostic survey. Journal of Applied Psychology, 60(2), 159–170.\\nHackman, J. R., & Oldham, G. R. (1976). Motivation through the \\ndesign of work: Test of a theory. Organizational Behavior and \\nHuman Performance, 16(2), 250–279.\\nHagendorff, T. (2020). The ethics of AI ethics: An evaluation of guide-\\nlines. Minds and Machines, 30, 99–120.\\nHagras, H. (2018). Toward human-understandable, explainable AI. \\nComputer, 51(9), 28–36.\\nHalloran, L. & Andrews, J. (2018). Will you wait for the future to \\nhappen? Ernst and Young. https://\\u200bwww.\\u200bey.\\u200bcom/\\u200ben_\\u200bau/\\u200bworkf\\u200borce/\\u200b\\nwill-\\u200byou-\\u200bshape-\\u200bthe-\\u200bfuture-\\u200bof-\\u200bwork-\\u200bor-\\u200bwill-\\u200bit-\\u200bshape-\\u200byou\\nHassabis, D., & Revell, T. (2021). With AI, you might unlock some of \\nthe secrets about how life works. New Scientist, 249(3315), 44–49.\\nHughes, J. (2014). A strategic opening for a basic income guarantee in \\nthe global crisis being created by AI, robots, desktop manufactur-\\ning and biomedicine. Journal of Ethics and Emerging Technolo-\\ngies, 24(1), 45–61.\\nJarrahi, M. H. (2018). Artificial intelligence and the future of work: \\nHuman-AI symbiosis in organizational decision making. Business \\nHorizons, 61(4), 577–586.\\nJarrahi, M. H. (2019). In the age of the smart artificial intelligence: AI's \\ndual capacities for automating and informating work. Business \\nInformation Review, 36(4), 178–187.\\nJobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI \\nethics guidelines. Nature Machine Intelligence, 1(9), 389–399.\\nKellogg, K. C., Valentine, M. A., & Christin, A. (2020). Algorithms at \\nwork: The new contested terrain of control. Academy of Manage-\\nment Annals, 14(1), 366–410.\\nLanglois, R. N. (2003). Cognitive comparative advantage and the \\norganization of work: Lessons from Herbert Simon's vision of \\nthe future. Journal of Economic Psychology, 24(2), 167–187.\\nLeicht-Deobald, U., et\\xa0al. (2019). The challenges of algorithm-based \\nHR decision-making for personal integrity. Journal of Business \\nEthics, 160, 377–392.\\nLips-Wiersma, M., & Morris, L. (2009). Discriminating between \\n‘meaningful work’ and the ‘management of meaning.’ Journal of \\nBusiness Ethics, 88(3), 491–511.\\nLips-Wiersma, M., & Wright, S. (2012). Measuring the meaning of \\nmeaningful work: Development and validation of the comprehen-\\nsive meaningful work scale. Group & Organization Management, \\n37(5), 655–685.\\nLysova, E. I., Allan, B. A., Dik, B. J., Duffy, R. D., & Steger, M. \\nF. (2019). Fostering meaningful work in organizations: A multi-\\nlevel review and integration. Journal of Vocational Behavior, 110, \\n374–389.\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creator': 'Springer', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'modDate': \"D:20230703174249+05'30'\", 'creationDate': \"D:20230224142509+05'30'\", 'page': 15}, page_content=\"740\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nMartela, F., & Riekki, T. J. J. (2018). Autonomy, competence, relat-\\nedness, and beneficence: A multicultural comparison of the four \\npathways to meaningful work. Frontiers in Psychology, 9, 1157.\\nMazmanian, M., Orlikowski, W. J., & Yates, J. (2013). The autonomy \\nparadox: The implications of mobile email devices for knowledge \\nprofessionals. Organization Science, 24(5), 1337–1357.\\nMichaelson, C., Pratt, M. G., Grant, A. M., & Dunn, C. P. (2014). \\nMeaningful work: Connecting business ethics and organization \\nstudies. Journal of Business Ethics, 121, 77–90.\\nMolitorisz, S. (2020). Net privacy: How we can be free in an age of \\nsurveillance. McGill-Queen’s University Press: Canada.\\nNaudé, W., & Dimitri, N. (2020). The race for an artificial general \\nintelligence: Implications for public policy. AI & Society, 35, \\n367–379.\\nNussbaum, M. C. (2011). Creating capabilities: The human develop-\\nment approach. Harvard University Press: USA.\\nPardes, A. (2020, November). AI can run your work meetings now. \\nWired. https://\\u200bwww.\\u200bwired.\\u200bcom/\\u200bstory/\\u200bai-\\u200bcan-\\u200brun-\\u200bwork-\\u200bmeeti\\u200bngs-\\u200b\\nnow-\\u200bheadr\\u200boom-\\u200bclock\\u200bwise/\\nParker, S. K., & Grote, G. (2022). Automation, algorithms, and \\nbeyond: Why work design matters more than ever in a digital \\nworld. Applied Psychology, 71(4), 1171–1204.\\nPratt, M. G., & Ashforth, B. E. (2003). Fostering meaningfulness in \\nworking and at work. In K. Cameron, J. E. Dutton, & R. E. Quinn \\n(Eds.), Positive organizational scholarship: Foundations of a new \\ndiscipline (pp. 308–327). Berrett-Koehler: San Francisco.\\nPulse+IT. (2020). The San using AI to automate multidisciplinary \\nteam meetings. Pulse+IT. https://\\u200bwww.\\u200bpulse\\u200bitmag\\u200bazine.\\u200bcom.\\u200bau:\\u200b\\n443/\\u200baustr\\u200balian-\\u200beheal\\u200bth/\\u200b5558-\\u200bthe-\\u200bsan-\\u200busing-\\u200bai-\\u200bto-\\u200bautom\\u200bate-\\u200bmulti\\u200b\\ndisci\\u200bplina\\u200bry-\\u200bteam-\\u200bmeeti\\u200bngs\\nRavenscraft, E. (25 November, 2021). What is the metaverse, exactly? \\nWired. Retrieved from: https://\\u200bwww.\\u200bwired.\\u200bcom/\\u200bstory/\\u200bwhat-\\u200bis-\\u200bthe-\\u200b\\nmetav\\u200berse/\\nRoberts, P. (2020). Working smarter with data. Australian Manufactur-\\ning Forum. https://\\u200bwww.\\u200bauman\\u200bufact\\u200buring.\\u200bcom.\\u200bau/\\u200bworki\\u200bng-\\u200bsmart\\u200b\\ner-\\u200bwith-\\u200bdata-\\u200bai-\\u200bgives-\\u200bagric\\u200bulture-\\u200bthe-\\u200bcompe\\u200btitive-\\u200bedge\\nRyan, M., & Stahl, B. C. (2020). Artificial intelligence ethics guide-\\nlines for developers and users: Clarifying their content and nor-\\nmative implications. Journal of Information, Communication and \\nEthics in Society, 19(1), 61–86.\\nSelbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & \\nVertesi, J. (2019). Fairness and abstraction in sociotechnical sys-\\ntems. In Proceedings of the Conference on Fairness, Account-\\nability, and Transparency (pp. 59–68).\\nSelenko, E., Bankins, S., Shoss, M., Warburton, J., & Restubog, S. \\nL. D. (2022). Artificial intelligence and the future of work: A \\nfunctional-identity perspective. Current Directions in Psychologi-\\ncal Science, 31(3), 272–279.\\nSeppala, E., Rossomando, T., & Doty, J. R. (2013). Social connection \\nand compassion: Important predictors of health and well-being. \\nSocial Research, 80(2), 411–430.\\nSmids, J., Nyholm, S., & Berkers, H. (2020). Robots in the workplace: \\nA threat to - or opportunity for - meaningful work?\\xa0Philosophy & \\nTechnology, 33, 503–522.\\nSusser, D., Roessler, B., & Nissenbaum, H. (2019). Technology, auton-\\nomy, and manipulation. Internet Policy Review. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n14763/\\u200b2019.2.\\u200b1410\\nSymon, G., & Whiting, R. (2019). The sociomaterial negotiation of \\nsocial entrepreneurs’ meaningful work. Journal of Management \\nStudies, 56(3), 655–684.\\nThaler, R., & Sunstein, C. (2008). Nudge:\\xa0 Improving decisions about \\nhealth, wealth, and happiness. Yale University Press:\\xa0New Haven, \\nCT.\\nTorres, P. (2019). The possibility and risks of artificial general intel-\\nligence. Bulletin of the Atomic Scientists, 75(3), 105–108.\\nTrope, Y., & Liberman, N. (2003). Temporal construal. Psychological \\nReview, 110(3), 403–421.\\nTubaro, P., Casilli, A. A., & Coville, M. (2020). The trainer, the veri-\\nfier, the imitator: Three ways in which human platform workers \\nsupport artificial intelligence. Big Data & Society, 7(1). https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b1177/\\u200b20539\\u200b51720\\u200b919776\\nVallor, S. (2015). Moral deskilling and upskilling in a new machine \\nage: Reflections on the ambiguous future of character. Philosophy \\n& Technology, 28(1), 107–124.\\nWalsh, T., Levy, N., Bell, G., Elliott, A., Maclaurin, J., Mareels, I., & \\nWood, Fiona. (2019). The effective and ethical development of \\nartificial intelligence. ACOLA. https://\\u200bacola.\\u200borg/\\u200bwp-\\u200bconte\\u200bnt/\\u200buploa\\u200b\\nds/\\u200b2019/\\u200b07/\\u200bhs4_\\u200bartif\\u200bicial-\\u200bintel\\u200bligen\\u200bce-\\u200breport.\\u200bpdf\\nWang, P. (2019). On defining artificial intelligence. Journal of Artificial \\nGeneral Intelligence, 10(2), 1–37.\\nWebster, C., & Ivanov, S. (2020). Robotics, artificial intelligence, and \\nthe evolving nature of work. In: B. George & J. Paul (Eds.), Digi-\\ntal Transformation in Business and Society. Palgrave Macmillan, \\nCham.\\nWiesenfeld, B. M., Reyt, J.-N., Brockner, J., & Trope, Y. (2017). Con-\\nstrual level theory in organizational research. Annual Review of \\nOrganizational Psychology and Organizational Behavior, 4(1), \\n367–400.\\nWolf, S. (2010). Meaning in life and why it matters. Princeton Univer-\\nsity Press: New Jersey.\\nWorld Economic Forum. (2018). The future of jobs report. Centre for \\nthe New Economy and Society: Geneva, Switzerland.\\nWright, D. (2011). A framework for the ethical impact assessment of \\ninformation technology. Ethics and Information Technology, 13, \\n199–226.\\nWright, S. A., & Schultz, A. E. (2018). The rising tide of artificial intel-\\nligence and business automation: Developing an ethical frame-\\nwork. Business Horizons, 61(6), 823–832.\\nZhou, L., Paul, S., Demirkan, H., Yuan, L., Spohrer, J., Zhou, M., \\n& Basu, J. (2021). Intelligence augmentation: Towards build-\\ning human-machine symbiotic relationship. AIS Transactions on \\nHuman-Computer Interaction, 13(2), 243–264.\\nZuboff, S. (1988). In the age of the smart machine: The future of work \\nand power. Basic Books: New York.\\nPublisher's Note\\u2002 Springer Nature remains neutral with regard to \\njurisdictional claims in published maps and institutional affiliations.\")]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a33dd5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So basically Acessing the One Document so it create a Page into Document so we will acess the length also\n",
    "\n",
    "len(documents)\n",
    "\n",
    "# So 1st pdf is of size : 42 + 21 + 16 = 79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e69ce22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Ethics: Integrating Transparency, Fairness, and Privacy in \\nAI Development\\nPetar Radanliev\\nDepartment of Computer Science, University of Oxford, Oxford, UK\\nABSTRACT\\nThe expansion of Artificial Intelligence in sectors such as health\\xad\\ncare, finance, and communication has raised critical ethical \\nconcerns surrounding transparency, fairness, and privacy. \\nAddressing these issues is essential for the responsible devel\\xad\\nopment and deployment of AI systems. This research estab\\xad\\nlishes a comprehensive ethical framework that mitigates \\nbiases and promotes accountability in AI technologies. \\nA comparative analysis of international AI policy frameworks \\nfrom regions including the European Union, United States, and \\nChina is conducted using analytical tools such as Venn diagrams \\nand Cartesian graphs. These tools allow for a visual and sys\\xad\\ntematic evaluation of the ethical principles guiding AI develop\\xad\\nment across different jurisdictions. The results reveal significant \\nvariations in how global regions prioritize transparency, fairness, \\nand privacy, with challenges in creating a unified ethical stan\\xad\\ndard. To address these challenges, we propose technical strate\\xad\\ngies, including fairness-aware algorithms, routine audits, and \\nthe establishment of diverse development teams to ensure \\nethical AI practices. This paper provides actionable recommen\\xad\\ndations for integrating ethical oversight into the AI lifecycle, \\nadvocating for the creation of AI systems that are both techni\\xad\\ncally sophisticated and aligned with societal values. The findings \\nunderscore the necessity of global collaboration in fostering \\nethical AI development.\\nARTICLE HISTORY \\nReceived 11 August 2024  \\nRevised 5 September 2024  \\nAccepted 2 February 2025  \\nIntroduction\\nIn recent years, substantial advancements in AI ethics have emerged, with \\nsignificant contributions addressing transparency, fairness, and privacy in AI \\ndevelopment. Recent research studies (Bender et al. 2021) highlight the dan\\xad\\ngers of bias in large language models, raising concerns over the perpetuation of \\nsocietal inequalities within AI systems. Similarly, Bommasani et al. (2023) \\ncritically evaluate compliance of foundation models with the draft EU AI Act, \\nreflecting broader concerns over the accountability of AI systems at \\na foundational level. Moreover, Aldoseri, Al-Khalifa, and Hamouda (2023) \\nCONTACT Petar Radanliev \\npetar.radanliev@cs.ox.ac.uk \\nDepartment of Computer Science, University of \\nOxford, Oxford, UK\\nAPPLIED ARTIFICIAL INTELLIGENCE                    \\n2025, VOL. 39, NO. 1, e2463722 (41 pages) \\nhttps://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with license by Taylor & Francis Group, LLC.  \\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/ \\nlicenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly \\ncited. The terms on which this article has been published allow the posting of the Accepted Manuscript in a repository by the author(s) \\nor with their consent.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].page_content\n",
    "# So we have Meta Data Like Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d6012",
   "metadata": {},
   "source": [
    "![AI Ethics](checking_the_words.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d8ede",
   "metadata": {},
   "source": [
    "So We can able to see the same data so it is divided into Pages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1034f391",
   "metadata": {},
   "source": [
    "So Now We will Make Into Chunks \n",
    "\n",
    "1. Recursive Character Text Spiltter\n",
    "2. Semantic Spitter\n",
    "\n",
    "So we can able to Find the differences that which can help in finding out the best Retervial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a29924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we will Creating the Chunk Size into 1000 and Chunk Overlap to 200 so it could get good semantic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "855f3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_spitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap =200\n",
    ")\n",
    "# So in the recursive Spitter we need to have the chunking size but in Semantic Chunker it uses the embedding model and make into chunks \n",
    "\n",
    "sematic_spiltter = SemanticChunker(\n",
    "    embeddings=embedding_model,\n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f58c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_chunks = recursive_spitter.split_documents(documents)\n",
    "\n",
    "semantic_chunks = sematic_spiltter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f337e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of Recursive Chunks :  434\n",
      "The Length of the Semantic Chunks :  1103\n"
     ]
    }
   ],
   "source": [
    "print(\"The Length of Recursive Chunks : \",len(recursive_chunks))\n",
    "print(\"The Length of the Semantic Chunks : \",len(semantic_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e955c794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applied Artiﬁcial Intelligence\\nAn International Journal\\nISSN: 0883-9514 (Print) 1087-6545 (Online) Journal homepage: www.tandfonline.com/journals/uaai20\\nAI Ethics: Integrating Transparency, Fairness, and\\nPrivacy in AI Development\\nPetar Radanliev\\nTo cite this article: Petar Radanliev (2025) AI Ethics: Integrating Transparency, Fairness,\\nand Privacy in AI Development, Applied Artiﬁcial Intelligence, 39:1, 2463722, DOI:\\n10.1080/08839514.2025.2463722\\nTo link to this article:  https://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with\\nlicense by Taylor & Francis Group, LLC.\\nPublished online: 07 Feb 2025.\\nSubmit your article to this journal \\nArticle views: 34975\\nView related articles \\nView Crossmark data\\nCiting articles: 52 View citing articles \\nFull Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=uaai20'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fe0ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)',\n",
       " 'creator': 'PTC Arbortext Publishing Engine',\n",
       " 'creationdate': '2025-02-07T21:22:17+05:30',\n",
       " 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf',\n",
       " 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf',\n",
       " 'total_pages': 42,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2025-02-07T21:22:17+05:30',\n",
       " 'trapped': '',\n",
       " 'modDate': \"D:20250207212217+05'30'\",\n",
       " 'creationDate': \"D:20250207212217+05'30'\",\n",
       " 'page': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c931ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applied Artiﬁcial Intelligence\\nAn International Journal\\nISSN: 0883-9514 (Print) 1087-6545 (Online) Journal homepage: www.tandfonline.com/journals/uaai20\\nAI Ethics: Integrating Transparency, Fairness, and\\nPrivacy in AI Development\\nPetar Radanliev\\nTo cite this article: Petar Radanliev (2025) AI Ethics: Integrating Transparency, Fairness,\\nand Privacy in AI Development, Applied Artiﬁcial Intelligence, 39:1, 2463722, DOI:\\n10.1080/08839514.2025.2463722\\nTo link to this article:  https://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with\\nlicense by Taylor & Francis Group, LLC.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "914ab448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)',\n",
       " 'creator': 'PTC Arbortext Publishing Engine',\n",
       " 'creationdate': '2025-02-07T21:22:17+05:30',\n",
       " 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf',\n",
       " 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf',\n",
       " 'total_pages': 42,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2025-02-07T21:22:17+05:30',\n",
       " 'trapped': '',\n",
       " 'modDate': \"D:20250207212217+05'30'\",\n",
       " 'creationDate': \"D:20250207212217+05'30'\",\n",
       " 'page': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chunks[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c826e",
   "metadata": {},
   "source": [
    "So You can see the Meta Data and Page content of the semantic chunks and Recursive Character Text Spiltter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ecafb",
   "metadata": {},
   "source": [
    "Vector Stores (Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "936190c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_vector = Chroma.from_documents(\n",
    "    documents=recursive_chunks,\n",
    "    embedding=embedding_model,\n",
    "    collection_name='recursive_chunks_sample',\n",
    "    persist_directory='Recursive Chunks Vector'\n",
    ")\n",
    "\n",
    "\n",
    "semantic_vector = Chroma.from_documents(\n",
    "    documents=semantic_chunks,\n",
    "    embedding=embedding_model,\n",
    "    collection_name='semantic_chunks_sample',\n",
    "    persist_directory='Semantic Chunks Vector'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2eeaf",
   "metadata": {},
   "source": [
    "So Basically I have Created Two Vector Stores to Store the data That Could be Helful to me to not collide the semantic Chunks and Recursive Spiltter Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91e65ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2255a4a8530>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a88b9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2255a943980>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c864f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['dfcc9525-ffc0-4b90-b85c-8a23a81e61ae',\n",
       "  'bdf29caa-43c8-418d-8955-365efd56da1d',\n",
       "  '0ccbdd79-f72f-4030-9248-e4ecb4d5a930',\n",
       "  '435d29e8-769e-442d-8c2a-94f2e0e65c3a',\n",
       "  '252d8e2d-4a60-4e2d-87b6-00d4621c9243',\n",
       "  '14545891-0e1f-41c6-9407-eeef6dcd310e',\n",
       "  'b4e0f94b-fa21-4a30-8acf-40a80a753559',\n",
       "  '47785361-9518-48e0-a282-966b5b0549d8',\n",
       "  '0e8b61cc-530a-4cb6-bf10-dafaff4fa68a',\n",
       "  '2813112c-512a-4e16-abf5-75d9a24f2e9e',\n",
       "  '463a4d4a-6c2d-4e21-99b0-d6f9f8399fb6',\n",
       "  'bfea93e8-40bf-4912-bd29-323cbefaeaf5',\n",
       "  '89fbec7d-943f-4f70-8b43-bf04ffa9d1d3',\n",
       "  '60528096-6841-4cdb-aaaf-ee917a667b03',\n",
       "  'a7061952-f16f-4065-bf4f-0bb2bfa23e69',\n",
       "  '51588e47-522c-4736-aa89-506b3b63927a',\n",
       "  '4efb74be-34e3-4d1d-996a-af859519c9ad',\n",
       "  '879af5bb-5154-45cc-955e-28d78035db23',\n",
       "  'f977ae13-298d-4aaa-93d1-598aa21ac72c',\n",
       "  '2729a78e-c314-466c-8887-6d9ac4713c22',\n",
       "  '9587e672-6a22-423d-993a-b95db718eca9',\n",
       "  'a67fbbf1-82ee-401a-845f-a07b5fcd9ba7',\n",
       "  '4cc6b056-cb48-4be8-89cb-dc4aa933c340',\n",
       "  '18a9bc8c-d4b4-46d2-953a-3b5e80cbf07a',\n",
       "  '8cd6f0a4-b8fc-4af6-9068-ce33d57d6661',\n",
       "  '01424bad-190c-456f-b599-cd3e4a55089f',\n",
       "  '110bb13e-bbf9-4a1c-b577-d94f50e8fbf8',\n",
       "  '2091f514-2269-43b4-9b11-4f93d4c5107c',\n",
       "  '4c392843-34b6-4048-ac25-90de74d42079',\n",
       "  '83217731-6c1e-4421-889e-cee4ca14b17b',\n",
       "  '1674a757-4499-4eb8-9a98-6de827f6d2c9',\n",
       "  '81231319-fd76-4da3-9730-7d2bdb0baaa9',\n",
       "  '3103afa3-b419-43cd-a199-bb4625ca38f3',\n",
       "  '45b62374-b9ca-4b5c-b4b2-0ba617bf590c',\n",
       "  'fb074c65-2bd3-4bc7-8c4f-ca871af94e1d',\n",
       "  '7675ee26-62a5-4d66-bf2e-c690e945cc45',\n",
       "  'b245a297-f67e-4215-8d48-49780ada2e32',\n",
       "  'e421ec14-316d-4145-a43e-cafc2611657c',\n",
       "  '7127e499-4ce9-4beb-b5c1-a4f4b29308c2',\n",
       "  '08cf0d00-5ac6-4e22-af4c-9adca115afcc',\n",
       "  'c3ca7cb0-d51c-4f2e-9a82-acd2108e2be3',\n",
       "  '88b21191-092e-4c92-be4b-f807c180f359',\n",
       "  '6b01ac47-1aad-4195-8ac3-b467d4f05668',\n",
       "  'e57cb36c-c076-462c-ac65-30518f4cc34d',\n",
       "  'c22f115b-a158-484a-b3dd-feecb5e11db1',\n",
       "  'fe4036c7-86bb-482a-ac4b-8a68d47daa7a',\n",
       "  'abf4104a-6ed6-49b7-8b03-db2120100a72',\n",
       "  'f121d149-7657-4dff-9af0-c827dfbae151',\n",
       "  '9e8c3c76-82b9-4fc2-b8ae-d508b5c1a4d4',\n",
       "  'c9b8599a-143d-45cd-b5c1-366a9c6ff4cf',\n",
       "  '35b55c4f-eaf9-4dda-a971-124d73aab2ab',\n",
       "  '1989518d-a14a-41ea-b772-6d8a234705fc',\n",
       "  '365acf11-71dd-4a86-8960-24053d978eb8',\n",
       "  '0abec9d1-8099-422d-bcbf-a29b79e3e910',\n",
       "  'f25206e0-6cf3-4c03-b170-13e268bb6aba',\n",
       "  'd36e98d8-6975-4648-89c7-df9208eeb64d',\n",
       "  '075f585f-aa49-4a63-87cc-cb20fc872982',\n",
       "  '0db6ff92-dad6-43c0-abcb-5d07b7d36c30',\n",
       "  'e7ea835f-55ad-4cc6-a6f4-f2ec5e2ac66c',\n",
       "  'fdf87c58-b6d3-4497-b057-86be8aa5854c',\n",
       "  'a9106dcc-8409-4508-afca-f4f9e7d90cdc',\n",
       "  '334b3220-00ef-4ab0-8174-dc53a19b8833',\n",
       "  'accaffd4-ce5d-49c7-bd1d-9e40f27cf9bc',\n",
       "  'b75473d7-3205-4d3f-82f8-1bd774824f45',\n",
       "  '8ca17256-8705-4c8f-99c2-e8165a5a4929',\n",
       "  'f2cb1179-1e67-49af-9b9a-b1cf36ce0127',\n",
       "  '58f49b62-ef25-4645-8ed0-7a0afaf0e9b1',\n",
       "  '6905ced9-5127-4d0f-be54-a2e8fe32b5fa',\n",
       "  'b6e13a54-8128-4b5c-b9c2-e075f8d066c0',\n",
       "  '48e4730e-3e27-420b-86f8-c590d4021306',\n",
       "  'a291028e-4d4d-4f59-a2e3-51b78826a311',\n",
       "  '961db5af-db3b-4334-9e5a-84f397c795d0',\n",
       "  '6faa851d-f207-49c7-8bb2-3e7c53baef3d',\n",
       "  '70015428-8640-4995-bb1b-85c6ec5764f4',\n",
       "  '89f72ed2-45fd-4138-b964-aef31fdd5d25',\n",
       "  '80302229-3c63-4ed0-b759-835c0ae47339',\n",
       "  '0623b085-7d16-4f4d-9f44-27791332a396',\n",
       "  'f784e960-4f16-4221-a539-a8fe20b9bc36',\n",
       "  'd0e2acfb-9a30-4bcc-902e-73df055e362e',\n",
       "  '7152de14-b04a-4a70-98e9-257597f5daa2',\n",
       "  'ef0599bb-42ce-4587-b23f-9d4f54984fa0',\n",
       "  '8e8a8849-c5e7-4cb1-b8d6-9a582258e5ac',\n",
       "  'e47cdd9b-4cb7-488d-bcdd-d3de796fa0af',\n",
       "  'b8dc5c9a-9c9f-4e1c-99fd-70751977a6de',\n",
       "  'ba3f8a99-9b00-47aa-b78d-2f012357bb4f',\n",
       "  '1a19359a-cdc6-4606-be83-76e47a6ec21f',\n",
       "  '4a6ad8ae-e549-483b-9a85-f2a0c1c068e8',\n",
       "  '73abe68d-39b3-4763-ba69-4cadae4ecb81',\n",
       "  'fb48a66c-e037-4789-b100-8aeffd2bb808',\n",
       "  '4d006b3b-52f0-439f-b078-afab0b83ea5a',\n",
       "  '7404d067-fac3-467f-96ef-607796681872',\n",
       "  'e97331a3-a8b0-49df-8b04-3b361bc26e22',\n",
       "  'a9438dcb-e7db-4aab-9fc3-a99a0ae97cda',\n",
       "  'ee6c79f4-3d96-4990-b513-5faebaaf4eaa',\n",
       "  'da427f76-c204-439c-8019-e58646517ef1',\n",
       "  '917e8982-5053-4c21-90fa-3267ebcebc52',\n",
       "  '6a97a635-d895-4f42-b262-d62a37314fda',\n",
       "  '2ccb1022-4156-4083-a955-c871fc9004b9',\n",
       "  '2d55dbcf-337a-4531-971a-395c229c8322',\n",
       "  'e391db82-48ab-414f-b06d-7e75c4265c5b',\n",
       "  '5203ea13-3eb0-419d-a7ff-bb7cd88c3b53',\n",
       "  '485c8ba1-0715-4279-b76d-efb26e7b363a',\n",
       "  '59df6510-c78f-4b1a-bc08-c80b26a80e8a',\n",
       "  '18ddff66-f48e-4f16-85b3-63ec71d944ce',\n",
       "  '4fa19837-4148-4adf-937c-6911e738aa74',\n",
       "  'd82250b0-7acb-4f5c-90b5-ee159dc19429',\n",
       "  '4c5061ea-7ea0-43f4-bee9-5fb9a9da4aec',\n",
       "  'b9d9a212-e193-44cc-b35c-3e2595de3ece',\n",
       "  '9d356912-96cc-48d1-9008-5f0d439404e5',\n",
       "  'd001c554-ac83-41ac-9b95-90b5c3286da6',\n",
       "  '307728ae-ca0c-4fc7-bec8-111d8df7e6bd',\n",
       "  '440eca34-cbd8-4048-89a5-433455cd7f8c',\n",
       "  '9f6b76a1-90db-40a4-976f-d0ca209de745',\n",
       "  'b14d454f-a8fa-467c-8966-42396a7c14c2',\n",
       "  'c6452bc0-ea3c-4360-85ba-e98d3d3d0087',\n",
       "  '378975f8-507f-43a2-abdd-c48ae50022da',\n",
       "  '89bebab4-082a-472a-a9d9-7b3607434313',\n",
       "  '9d893213-252d-428e-98fb-321dfdb12d9f',\n",
       "  '53041dce-aaa1-439d-8bcc-d8c03ba19fdc',\n",
       "  '53b22348-0ef3-4c58-ac76-022f3c530d7f',\n",
       "  'dc2ee6ef-0525-4c67-ae4b-ec81c59e5e5d',\n",
       "  '24886ec3-1881-4e74-804d-73cc819a8719',\n",
       "  '9f830c68-5286-4bec-b2c4-d55ce66bbeb0',\n",
       "  'c1844eb5-4810-4327-b68b-c3e0ecd3e0f6',\n",
       "  'e5527f13-7607-421b-821b-53a8beac0033',\n",
       "  '204eedb9-390b-4213-b80b-de64c01883c2',\n",
       "  '86b222ca-5cfa-4df4-bdc5-701a5880fc0c',\n",
       "  '5c7de6a0-b732-4f41-af19-16207482f88a',\n",
       "  '642362d9-7800-48ba-b100-209127159fa0',\n",
       "  '66d02940-a415-4647-9621-778459c9875f',\n",
       "  '84cfd814-5186-41c0-add1-4b682369867b',\n",
       "  'f98b3fec-db40-4048-b2be-0ac9713d838c',\n",
       "  'c7e3870c-20f4-45e6-a883-407222723af2',\n",
       "  '2169b4b6-d4b0-4090-8522-09a41c5c8460',\n",
       "  '5081914d-3a19-4c44-b571-1840fe40afab',\n",
       "  '6600057a-bee3-4687-b59d-f3ef7bef0419',\n",
       "  'eca431d6-cf15-4854-a9bb-f39138ff56e7',\n",
       "  '03d54a73-529d-45c8-8dad-8c2f71cd9729',\n",
       "  '6ac5659c-9fad-4106-9cc6-63c246f7e998',\n",
       "  'f1bdea09-b29f-46d9-83f8-5b9a334605e4',\n",
       "  '738381ef-3184-49ce-b3d7-b20c3995bfa9',\n",
       "  'a2be0005-4152-4342-ad94-a2b57a0135ef',\n",
       "  '3b01668d-f59c-487a-b4d3-9243470de971',\n",
       "  'acc833cc-2d14-4f23-99ca-ea65580d1b6a',\n",
       "  '58e89e49-4ed5-4160-9b82-623efa130c5f',\n",
       "  '4568b66e-7752-4d8f-99b5-b39e292553e9',\n",
       "  'cc781a6c-e8c7-48da-b261-31565351415e',\n",
       "  'c431c9d8-628b-4844-aca7-870cc7a0f652',\n",
       "  'ea857594-4812-438d-b0a8-0375fe338c32',\n",
       "  'fb59fcab-517a-4798-ae17-178f22abbf2e',\n",
       "  '2d978031-f2c6-4287-9027-004d259dd7b8',\n",
       "  '6e5043b1-0326-46ed-a52f-8d29271559fd',\n",
       "  'a8e09a89-fe19-4b36-baf9-3c3ba569439e',\n",
       "  '411ede6b-18e9-4557-b105-9d67b53dab13',\n",
       "  '1834d30e-878e-4226-bbb8-1d05c3706984',\n",
       "  '9d648336-a54f-4138-ba60-cbf51a6b90ab',\n",
       "  '83336cef-91a3-492c-9630-5a18595b93c6',\n",
       "  '82669ecf-cbfa-45c1-b4e7-bbaac59eaee1',\n",
       "  '5687eab6-9ec5-4a84-89bf-87ed2ca7ada9',\n",
       "  '29b157ed-64ad-46e8-ae0e-fcd04409e130',\n",
       "  '5d3411ef-262d-4b77-91fc-edbc1faa9a1e',\n",
       "  '8344e084-abd2-4ba9-af0e-57d773ea52f7',\n",
       "  '198aab60-c728-4d58-ba97-b41b72d61549',\n",
       "  '91054b5e-155a-44f5-9efa-38f99a00f6b2',\n",
       "  '6a392749-fb97-49e0-b526-2b903ba4027f',\n",
       "  '27058f8d-f7b5-41a6-a443-8dcf035b6d18',\n",
       "  'b4fe0392-a839-4cba-9c68-68b6bada7f5b',\n",
       "  'b4381aab-eb1e-477f-9b4c-67a3fb57afd5',\n",
       "  'ab2ea95c-8ecf-42d8-a8c3-695cea8fbad5',\n",
       "  'c872fd04-c97e-43fa-816c-35d2042a78ef',\n",
       "  'b5e13fd5-105a-455e-b60a-4f85d5402ccb',\n",
       "  'c49269d9-c2b1-47ed-816f-5c654380411e',\n",
       "  '3ee67176-a76d-46be-ac2f-6cacbe085f7f',\n",
       "  '3e4976e2-1559-49bb-a835-bd2a162cfda7',\n",
       "  '52587fbc-74a8-403b-aba3-43ac9eb5b3af',\n",
       "  '3f9b77e0-c372-4260-8319-3416cb94b7e1',\n",
       "  'c22bce91-6ade-4797-98b9-d843a82bf0e6',\n",
       "  'b53c8348-a056-442a-82c5-e7fa930c487f',\n",
       "  '3ce20a72-e0cb-449e-9359-457e0d29a1cd',\n",
       "  '4da1f48b-ac37-43bc-833c-a26101ce91a9',\n",
       "  'bddd877b-74fa-4125-b8df-15afe7cc3752',\n",
       "  '9aa80629-083b-4773-96e7-31935908d10c',\n",
       "  '8d0453f6-7ead-42cc-983b-8799d10ccc7b',\n",
       "  'bc7a3626-db7b-4dcf-982b-1178f4726a46',\n",
       "  '604cfb8b-f55b-4ee4-a6df-2a17139dfd88',\n",
       "  '944a3a8f-2b39-4f5e-8c5c-57f2392c658f',\n",
       "  '96dff56c-46f8-4372-9bb8-f926a7620ce2',\n",
       "  '46607be3-ee84-4f0f-8b65-0c6e8d37a6a1',\n",
       "  '3f8f15e3-1b7f-4cff-b76d-3feae149b079',\n",
       "  '22121360-53f3-4fae-b1a7-afd5e356f663',\n",
       "  '6e887c4b-55a5-4a72-a2e5-eb1caa787e20',\n",
       "  '4953c679-d9b3-4ebf-95bd-8e9239c03dcd',\n",
       "  'ec4b925d-875f-4a31-9d60-f00790e056e7',\n",
       "  'f2f20f64-d428-4659-acee-6254535687ee',\n",
       "  'b32d9c92-4b45-4da8-aa16-2f19f55d2b8c',\n",
       "  'f6723f4a-605b-4dd9-b120-3c862eda195f',\n",
       "  '64c19b52-14d7-4f23-8641-4381c1d18be0',\n",
       "  'fdc83f77-e295-4593-85c9-6a051f86f629',\n",
       "  '99e0b2ef-c0df-4eec-b5c0-46e208d94b4a',\n",
       "  '19017141-0e1a-469f-89fc-b9bd0e8013e4',\n",
       "  '1cdca182-c206-4b38-b420-16a74193f6d7',\n",
       "  '87ffccca-ef86-4132-a5d9-295317afb114',\n",
       "  '94d2c8b0-7a70-4d0a-9abb-a0fb51a4106b',\n",
       "  'd43177cb-f5ec-41b9-a291-4b7aa2c08fcd',\n",
       "  '7cf07d5b-c94a-4288-9928-71353875c51d',\n",
       "  '83c531fd-2ef2-48e6-a93b-41a784b154ec',\n",
       "  '1af01a95-caea-471f-950b-f1380e7caa69',\n",
       "  '000d2058-3db9-414b-9624-539e9b936292',\n",
       "  'df592449-ddd3-4436-bd01-e9bc634d8659',\n",
       "  'e1e11fc8-55a3-4c7f-97f1-fe21e4d3f44e',\n",
       "  'f45a48ec-2778-4d6f-8c39-d382eea7fecf',\n",
       "  '08f9df42-be04-4f96-8aac-7f06001c3764',\n",
       "  'f6df00e7-d5b7-4b6f-9c3c-e46fd6e9093f',\n",
       "  'eebc654c-7005-4cef-b063-fd7857ee3cc8',\n",
       "  '6c97627c-4c11-424f-85b0-9e13265b4b3b',\n",
       "  '046514d1-318c-4559-bedf-2b71810ccb20',\n",
       "  '1cd98579-884b-41fa-9a50-bd44b34dfa93',\n",
       "  'f652ae35-d231-4644-a9f2-2cf059c5411f',\n",
       "  'f32eb89e-a24f-426d-9934-a56ee371d504',\n",
       "  '59218fbd-03b1-4f78-966e-524b91b54c13',\n",
       "  'f2fc4f90-e796-4579-aae5-bee34e4dc589',\n",
       "  '0ebeeb9a-60db-424c-90f5-0fbeb4c16c18',\n",
       "  '15e14760-c7d9-4a84-8911-a8197be17690',\n",
       "  'a445901f-35d1-4d3a-9857-0abf43a72dcc',\n",
       "  '79a9bf32-22be-4bea-9a1b-f181bd52f1a3',\n",
       "  '3f610150-85f0-43f6-8924-b060ca5568e8',\n",
       "  'b7595d5c-b56c-4ff5-9e37-edd1c162e8b8',\n",
       "  'bdf643a2-76b1-41b6-93ba-4772db54ce42',\n",
       "  '9be09283-07be-4184-9779-2fc723a1891a',\n",
       "  '6cc4e9d4-6852-4241-b3d2-caf8f753c2f8',\n",
       "  'b26c9e13-90df-4a23-94cf-7f26c064cf26',\n",
       "  '33572357-0dd7-474f-a7ee-cdd2755461dd',\n",
       "  '0d863a4c-cdc3-4d40-b8cd-a202de327da7',\n",
       "  '571d1158-2f03-4fd3-bce2-54815edacd86',\n",
       "  'dcaf6be9-4c08-40bd-9805-1ecfc1698cb9',\n",
       "  '37fb9200-e09a-4070-91ad-cbaafd41cf0b',\n",
       "  'c2a899cf-af20-424e-b699-1150c2a08fc0',\n",
       "  'c3096a01-8bdd-4ff4-9ce3-52c4fa3024de',\n",
       "  'efb3b7bb-bf98-4f31-b98f-b5f1e7cb44cb',\n",
       "  '6c18a5e1-e9bb-4186-a75f-7429fbec511e',\n",
       "  '78c2dffb-7034-450b-9012-edab6cb3894e',\n",
       "  'b73e83f2-15ab-4fba-9728-0389fa9cb5ed',\n",
       "  '56c04435-72c7-422a-8ea6-a199b6aad01b',\n",
       "  'efcefd05-2aa9-4deb-a995-8f0f66c8ce11',\n",
       "  '450dd667-eb65-4112-82d6-5d9ee9843e99',\n",
       "  '619de72b-e110-432e-8c58-6499ebe79f0b',\n",
       "  '8a7b3486-c36b-496f-b46a-feb2e28f0cfe',\n",
       "  '7578399b-f816-4a34-bf38-276acd754664',\n",
       "  '76440afc-e456-4b1a-91bb-11f8b8b97a38',\n",
       "  'b1430fe0-792a-406a-9a9b-0a2ed3cf5812',\n",
       "  '7f667c27-c604-4b78-80e8-25eda957c168',\n",
       "  'd86cae06-ffcb-4c0c-9b79-03d5aaa14dd6',\n",
       "  'a0906fd6-1bb5-4636-8458-2e250def1b00',\n",
       "  '565d5dd5-2816-4688-bf4b-bb7545e30222',\n",
       "  'ad7b8f31-ffba-4d29-b31c-31a4ad6ae614',\n",
       "  '09fadc4a-d284-442a-b219-c5c4b141c039',\n",
       "  '8d81e5bd-a253-47fa-84d8-f3790f54a1e1',\n",
       "  '7988e694-c999-4e6c-a29d-4c7b281ce2c6',\n",
       "  '31d64d7b-4906-486c-b04e-2cca6d159b5d',\n",
       "  'daf0e167-8b3d-41ac-95d1-5e7c5fe032d2',\n",
       "  '0e21bd7c-3fed-411e-b9c9-6cecd1b2dae4',\n",
       "  'c07ed43c-2099-4395-965e-a7abc3e832c5',\n",
       "  'c1a3359c-e63a-4368-9f93-199aad2d4d66',\n",
       "  '0ee5e4f9-f651-44cd-9c45-9dd8af0be575',\n",
       "  '1d258739-63cd-4f3b-9451-3e80f6ccc512',\n",
       "  '1f3597b5-78f1-4c10-b12c-ec505d36e0af',\n",
       "  'e9e0c0ad-d6be-40f9-8ab6-50d8a0ab966a',\n",
       "  '4423f61f-09c8-4973-b5b6-694c40d3869b',\n",
       "  'e6e21c70-0c3b-443b-a06a-4b1898332131',\n",
       "  '2c81005a-2f71-435d-8197-3178093df85f',\n",
       "  'f7a1c53d-466f-44a8-a780-1dea2634da5c',\n",
       "  'b76f8c97-4d3b-4f2f-8b39-455e9cc9c743',\n",
       "  'ccc28a8b-b0b6-45bd-a69d-b403b3046e6d',\n",
       "  '1423c4d0-b079-4f80-8f50-1398361b6024',\n",
       "  'a87e3c2e-99f3-4e62-85d9-777efb6a04ed',\n",
       "  '379d2bb7-e4d8-4bf0-9cd3-82165d426f2a',\n",
       "  'a74b69bc-e2f2-4795-bd47-d1997b3bfa73',\n",
       "  '7cefb29d-5f0c-4039-a93d-c95cbb187024',\n",
       "  '8afc74db-a38a-44ca-b858-7f2b0cd30b5e',\n",
       "  '9f527c0b-a923-42a1-a055-711cda99f3fc',\n",
       "  'b00d2a2a-de17-4c30-90f2-f5a113721f58',\n",
       "  '657243bd-9f7f-4007-94c6-c9d9dfcabf93',\n",
       "  '1bd4ec27-ff54-4f21-8271-02d12d51e981',\n",
       "  'e4d48cdf-63e0-4a8a-a43b-c8d3554ca478',\n",
       "  '5030dc77-de20-4809-9f46-2701224b7b3c',\n",
       "  '805dc717-ec8e-44e2-afb9-ba0053ed9696',\n",
       "  'f5fa2d5b-8e2a-4288-930e-b1df72036665',\n",
       "  'e8f6beb5-ece8-4061-a0d9-02eebc0aa33d',\n",
       "  'c0f6a415-a2f3-46f4-8fdf-1ea6bdd1e969',\n",
       "  'e0482ba2-07e5-4a7b-aeb6-02d5cf5828b6',\n",
       "  'aedaa400-d040-407e-803f-e1ae0e5d6379',\n",
       "  '7ea8e452-1316-43a5-b4dd-cb7086542a84',\n",
       "  '055c085b-42ed-48e1-9e55-4fd34aff02f5',\n",
       "  'd5b7ad74-c88a-4310-98ca-b4a2b5030ffc',\n",
       "  '3b603197-80e3-4bda-b352-6f01f98a1f91',\n",
       "  '87897156-5fff-4789-8753-ea2874d3b280',\n",
       "  '9d25c35e-3fe8-4dd8-b277-f4151ad34223',\n",
       "  'a1ddb599-44a4-47ab-b2c0-6d140fd15eac',\n",
       "  '5fbdd482-80b9-46cf-9574-bc98a9fe2c71',\n",
       "  '0c3c25c8-377c-4507-a954-6c2ba69321e2',\n",
       "  'e76ff4fa-9b64-4e62-a908-365e09037bd5',\n",
       "  'eedf9052-56aa-4ef8-87bb-c7f5baa2146c',\n",
       "  '2b92fd43-deeb-402e-9e31-a52b19d397dc',\n",
       "  'a92fcb0c-4858-4e0c-84ef-ebc467ba7c70',\n",
       "  '52d26d28-632a-4352-9337-e9988b9c038e',\n",
       "  '71b04735-4c14-4147-b866-459c31ad41ca',\n",
       "  '949ff541-6efb-41a8-bc2e-d9b82b5ede07',\n",
       "  'dabd9f02-d366-42c5-9a15-3e6b79eb31eb',\n",
       "  '89eb06bd-aaf4-486c-a452-235250244db1',\n",
       "  '834754ba-cff7-42ce-9456-ed73049524b4',\n",
       "  'd81015ff-5ed6-4254-b958-48e0cd1ed1b5',\n",
       "  '2cfd6f5f-f231-4519-aa6b-b0b8e9d1b175',\n",
       "  'd0d5e623-da0b-4e0b-9847-849caf9b0298',\n",
       "  '8a527f44-77e6-4e20-9c78-e221075dde62',\n",
       "  'cb7233d2-3fb0-4231-a00d-69c1f22e03af',\n",
       "  '1c3d466d-6f2a-4862-b63e-819f02ce9c49',\n",
       "  '0b4c9002-8358-4181-83c8-cfb01ae638e8',\n",
       "  'b3d4d31a-aa3b-4c0b-9855-048733f74075',\n",
       "  'dfe51f3e-9922-4c2c-9053-96365a9dd97c',\n",
       "  'a794571d-6f4b-4779-8685-e87ac4bbfc63',\n",
       "  'e38fed3f-65e4-4c85-84d0-80d6699a53d7',\n",
       "  '4faa121c-c4e6-400b-bf3c-8e43ca3a5ca9',\n",
       "  'cabcf93f-ea1c-4dd5-a121-b4f6f3ed27e2',\n",
       "  '6058d971-2a1a-4917-b2db-7c13b573b454',\n",
       "  'c978dedf-0887-4ad3-bd05-0167b49f7c48',\n",
       "  'cf105986-80f6-483a-a44b-6d5edc62683e',\n",
       "  '4d6bc078-3ea0-4e68-8327-d1bab22050e0',\n",
       "  '50dc7be9-1611-47ab-a585-5c26896c1342',\n",
       "  '920ec8d4-61d5-40ae-9a2d-f66a828c5cf3',\n",
       "  '7e38fec9-21df-4768-aede-d268024d371b',\n",
       "  '06275329-9045-4176-851b-0acceeedab1f',\n",
       "  '7eccb447-7479-49f6-8985-b4740844ed0a',\n",
       "  'a47bcedf-aecc-40d9-b8ec-0671ebc82f6a',\n",
       "  '142a6ae5-3657-4250-bae4-7b85499445ad',\n",
       "  '1539335c-8f9d-4fd7-97a7-8c33e7a4e1c1',\n",
       "  '790a9a42-5695-488e-b91c-1c6c847b3cad',\n",
       "  'c741a474-c993-4a5a-9600-7dc82f15412c',\n",
       "  'e83f2d6d-d5e8-4e3e-9cb9-dd9af8b1e055',\n",
       "  '54e8c690-a832-42dd-b7e1-cf2da22dc9fb',\n",
       "  '13c020c3-422d-4e90-8a23-d96e5326fcf9',\n",
       "  'a3751d69-0eeb-4c44-9943-91b2dcc8bc67',\n",
       "  '9c7d3461-22f8-4348-896d-f3a87604af40',\n",
       "  'b67e3a9c-71d7-4344-85b2-f964500655d1',\n",
       "  'ef67d969-69cc-4b79-8b45-2a75fff22ead',\n",
       "  '6a5ac9eb-3ef4-47c2-8438-1c315ccb0632',\n",
       "  'b1bc1830-8882-40cf-b680-36b41951e6ff',\n",
       "  '5104dcfe-d9d8-4072-a96c-21fbc076266d',\n",
       "  'a86efcba-70c5-4030-bcd5-6dff0a2bb12a',\n",
       "  '58efb39d-8653-4ba8-a0a7-1c8f47a3c282',\n",
       "  'b95c2c7b-2d6c-4e0b-968e-395e8ec6aa05',\n",
       "  'b5f4875e-f526-4c4f-bb62-3b9c2118f706',\n",
       "  '53502f1a-794f-4151-ad06-3ebaae74262c',\n",
       "  'd34e63f9-6d61-4095-948c-87d513d58404',\n",
       "  'bd1920cf-e05c-47e0-b566-6e2ca73d56c1',\n",
       "  'c40af0f0-e941-4d3a-8e53-3f6f52e14d85',\n",
       "  '5b3cbcac-935f-4845-8a12-32e1c21c97f9',\n",
       "  '13c78611-da74-4b5e-a476-c91032722d84',\n",
       "  'a3cc3bbe-55bb-4c9e-9714-e8b8516e748b',\n",
       "  '8ecbdb34-9645-4501-9769-527a6b3d93b0',\n",
       "  '8f1e3b0d-936c-4e1f-8115-75cf54ff82b7',\n",
       "  '716cfc52-1e88-48bd-a311-0290d33c5805',\n",
       "  'caa755b2-2b07-42bd-8d54-de20f1579251',\n",
       "  '2f92a31f-e252-4eef-b6b7-b50f02cf932d',\n",
       "  'c521c5ef-900f-4a1c-bbec-bba1aa844bb3',\n",
       "  'a3908a0d-bf44-4378-a5f5-28b5ee17bf0f',\n",
       "  '2813e418-e6fc-4f51-b021-0fd54aa42199',\n",
       "  '81933368-5db4-4ce4-bb4f-bf33ea779f9d',\n",
       "  '571699ed-02f5-44e5-9c78-35f328465846',\n",
       "  '03a835fd-9e94-407b-b86e-0ab324abad95',\n",
       "  '3aee39c4-04f7-436e-bd73-052d3619dc30',\n",
       "  '4d6b928c-c05b-44eb-a912-0cb099faccec',\n",
       "  'cfc5af32-8b30-4203-8d31-9856c673ddf6',\n",
       "  '0db85c2d-5b85-404e-ac34-d7a04b1fe287',\n",
       "  '935f2019-a75a-4982-90eb-866faf108675',\n",
       "  '52a2cddd-7b3d-4d91-a1a1-e46df9867cf1',\n",
       "  '2f812139-f9e7-45aa-81d3-19a14f888de4',\n",
       "  '26f746d5-6454-4c32-b0d5-9072f585e548',\n",
       "  '587a8a4f-21e3-4862-8c35-a487fcc1a131',\n",
       "  'efdc01b7-82fc-4be1-8496-f096ce5f9e73',\n",
       "  '1e364b3f-6032-4c8f-8875-9d81d4a6d112',\n",
       "  '1a8c9625-d68e-4504-95d1-7f7ed96e1ca5',\n",
       "  '37f85a16-ef0c-4ea7-9655-d1acf8d133f6',\n",
       "  'f4893e5d-b598-4513-a075-5a38dbb1b657',\n",
       "  '839996d6-fc65-40fb-8bb8-d45b937735f2',\n",
       "  'fc8dbf6d-0372-4817-97e0-51d7155ad8a4',\n",
       "  '31e55bc1-1ab5-4973-8deb-e78032b2f44d',\n",
       "  'c460f84e-7811-430e-bedf-ba4fa27c3f15',\n",
       "  '90a75312-4ffb-4f98-bd68-d422f81d7b49',\n",
       "  '8bef1d3d-53cc-4817-8ea4-c5f56c8c7ca7',\n",
       "  '71971ec1-a924-4ddc-9073-9d7dd8f9103a',\n",
       "  'a6f97543-5d8d-4104-b1b8-f570c10eb2fe',\n",
       "  '21a5e84b-96d0-4a61-8156-40f5f76c93d0',\n",
       "  '285aa618-5863-4a82-a681-dd18350f4170',\n",
       "  'b4575d61-76fe-4516-b5ab-314a1960d0e2',\n",
       "  'd4be573c-1762-4ee1-b2b3-2c20c76cacab',\n",
       "  '25e36308-37bf-4aa4-a86d-c501ed474ac7',\n",
       "  '3db23fea-2552-48a5-b193-165c06159d11',\n",
       "  '9da014a5-76ea-409c-b1af-268355d3f897',\n",
       "  '6b265632-57f9-4a71-884f-7523d0ce15e4',\n",
       "  '1bc5b658-2d89-434f-8409-f26cea11c538',\n",
       "  '8ee3a26d-873c-4231-b113-de3988ea3b7b',\n",
       "  'f337c8e3-ae81-4989-a0d2-0fe1a0a4d7c0',\n",
       "  'dd8476d7-5d62-43c9-bc76-4672445f991c',\n",
       "  'ef0dac23-17fe-4a10-aa2d-17b32b4280fb',\n",
       "  '41231317-4c02-4650-8bed-3611884cf575',\n",
       "  '32a1b644-2cfe-4e14-a595-dde0ab215a7d',\n",
       "  '2afb5c46-131c-4335-9b87-6c88c0c3812c',\n",
       "  '130f84df-424f-4e5b-ae6d-8493f9733204',\n",
       "  '4ab3037f-a8b9-4410-b3a3-cd8ab28eee77',\n",
       "  '02b3e21a-0ef5-4242-87ea-d1f9aa47fc50',\n",
       "  'b7b91bf9-baab-4f38-8d94-8e4e06e49f93',\n",
       "  'e697e78e-dba1-4cac-8b88-9f7935ad1905',\n",
       "  '5777ae6e-e90a-4dfe-94d0-ebd355f9e964',\n",
       "  'b99b2fca-092a-4f27-9a41-14123f658ed9',\n",
       "  'cb8c9f00-9561-44b4-b68a-693dcde5690a',\n",
       "  '437ebce3-584f-4a3d-bd29-c60ea6361554',\n",
       "  '9511ac02-2a71-4df3-b82d-68f34cf54678',\n",
       "  '1e17bfec-90cb-4d9d-8719-08a84dd54238',\n",
       "  '8ed2962a-7d2d-41f6-bf4e-0c511bdd6b0f',\n",
       "  'dd3da433-0f45-43a3-8f5b-fdb5a552aab4',\n",
       "  'c18af232-450f-4f24-9616-0d0d749bd4e7',\n",
       "  '76b52f6c-9aae-4d55-b6bf-b0bfbd8f562c',\n",
       "  '4af5ced4-c49e-4832-a168-8bc320a58984',\n",
       "  'a5b86868-fb71-4556-8434-fbbec0e69fac',\n",
       "  '8d35e8ca-7ca1-4589-b08f-45e4af1d2521',\n",
       "  '0688683a-cec2-4387-9160-6c6f20fdf36f',\n",
       "  'db7a2855-657a-4fd9-8ea7-c695003f8576',\n",
       "  '10790279-b6ce-48a6-8ca1-974e7bcad359',\n",
       "  'd24d566a-211d-44e6-bef8-6a09eddda6f0',\n",
       "  '896d81e4-09a3-47c7-a88c-37c64728698d',\n",
       "  'f6eba669-0778-41c5-927a-31a9acff043a',\n",
       "  'f6d2825a-70c2-4324-b6e1-c0a1548bf6f7',\n",
       "  'e7aea045-dd83-4809-b3ec-bb0bfa9a250b',\n",
       "  'e103a3c9-18b3-4327-bc5d-e91de30c5581'],\n",
       " 'embeddings': array([[-0.0347109 ,  0.04125925, -0.02860328, ...,  0.05029637,\n",
       "          0.03816827,  0.05545659],\n",
       "        [-0.01533785,  0.01395692, -0.00362207, ...,  0.06821069,\n",
       "         -0.02709314, -0.00124462],\n",
       "        [-0.03777542,  0.02266056, -0.01202245, ...,  0.02406188,\n",
       "         -0.0158161 ,  0.0007758 ],\n",
       "        ...,\n",
       "        [-0.02585698,  0.03658981, -0.01836679, ...,  0.02349646,\n",
       "          0.01699754,  0.01169782],\n",
       "        [-0.01290098,  0.0324636 , -0.00774434, ...,  0.06680577,\n",
       "          0.00595177, -0.02572603],\n",
       "        [-0.0264474 ,  0.02757175,  0.01464776, ...,  0.06932452,\n",
       "         -0.01844988,  0.04006656]]),\n",
       " 'documents': ['Applied Artiﬁcial Intelligence\\nAn International Journal\\nISSN: 0883-9514 (Print) 1087-6545 (Online) Journal homepage: www.tandfonline.com/journals/uaai20\\nAI Ethics: Integrating Transparency, Fairness, and\\nPrivacy in AI Development\\nPetar Radanliev\\nTo cite this article: Petar Radanliev (2025) AI Ethics: Integrating Transparency, Fairness,\\nand Privacy in AI Development, Applied Artiﬁcial Intelligence, 39:1, 2463722, DOI:\\n10.1080/08839514.2025.2463722\\nTo link to this article:  https://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with\\nlicense by Taylor & Francis Group, LLC.\\nPublished online: 07 Feb 2025.\\nSubmit your article to this journal \\nArticle views: 34975\\nView related articles \\nView Crossmark data\\nCiting articles: 52 View citing articles \\nFull Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=uaai20',\n",
       "  'AI Ethics: Integrating Transparency, Fairness, and Privacy in \\nAI Development\\nPetar Radanliev\\nDepartment of Computer Science, University of Oxford, Oxford, UK\\nABSTRACT\\nThe expansion of Artificial Intelligence in sectors such as health\\xad\\ncare, finance, and communication has raised critical ethical \\nconcerns surrounding transparency, fairness, and privacy. \\nAddressing these issues is essential for the responsible devel\\xad\\nopment and deployment of AI systems. This research estab\\xad\\nlishes a comprehensive ethical framework that mitigates \\nbiases and promotes accountability in AI technologies. \\nA comparative analysis of international AI policy frameworks \\nfrom regions including the European Union, United States, and \\nChina is conducted using analytical tools such as Venn diagrams \\nand Cartesian graphs. These tools allow for a visual and sys\\xad\\ntematic evaluation of the ethical principles guiding AI develop\\xad\\nment across different jurisdictions. The results reveal significant',\n",
       "  'and Cartesian graphs. These tools allow for a visual and sys\\xad\\ntematic evaluation of the ethical principles guiding AI develop\\xad\\nment across different jurisdictions. The results reveal significant \\nvariations in how global regions prioritize transparency, fairness, \\nand privacy, with challenges in creating a unified ethical stan\\xad\\ndard. To address these challenges, we propose technical strate\\xad\\ngies, including fairness-aware algorithms, routine audits, and \\nthe establishment of diverse development teams to ensure \\nethical AI practices. This paper provides actionable recommen\\xad\\ndations for integrating ethical oversight into the AI lifecycle, \\nadvocating for the creation of AI systems that are both techni\\xad\\ncally sophisticated and aligned with societal values. The findings \\nunderscore the necessity of global collaboration in fostering \\nethical AI development.\\nARTICLE HISTORY \\nReceived 11 August 2024  \\nRevised 5 September 2024  \\nAccepted 2 February 2025  \\nIntroduction',\n",
       "  'underscore the necessity of global collaboration in fostering \\nethical AI development.\\nARTICLE HISTORY \\nReceived 11 August 2024  \\nRevised 5 September 2024  \\nAccepted 2 February 2025  \\nIntroduction\\nIn recent years, substantial advancements in AI ethics have emerged, with \\nsignificant contributions addressing transparency, fairness, and privacy in AI \\ndevelopment. Recent research studies (Bender et al. 2021) highlight the dan\\xad\\ngers of bias in large language models, raising concerns over the perpetuation of \\nsocietal inequalities within AI systems. Similarly, Bommasani et al. (2023) \\ncritically evaluate compliance of foundation models with the draft EU AI Act, \\nreflecting broader concerns over the accountability of AI systems at \\na foundational level. Moreover, Aldoseri, Al-Khalifa, and Hamouda (2023) \\nCONTACT Petar Radanliev \\npetar.radanliev@cs.ox.ac.uk \\nDepartment of Computer Science, University of \\nOxford, Oxford, UK\\nAPPLIED ARTIFICIAL INTELLIGENCE',\n",
       "  'CONTACT Petar Radanliev \\npetar.radanliev@cs.ox.ac.uk \\nDepartment of Computer Science, University of \\nOxford, Oxford, UK\\nAPPLIED ARTIFICIAL INTELLIGENCE                    \\n2025, VOL. 39, NO. 1, e2463722 (41 pages) \\nhttps://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with license by Taylor & Francis Group, LLC.  \\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/ \\nlicenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly \\ncited. The terms on which this article has been published allow the posting of the Accepted Manuscript in a repository by the author(s) \\nor with their consent.',\n",
       "  'propose new data strategies for AI development, focusing on the integration of \\nethical principles across diverse datasets to mitigate bias. These works, along\\xad\\nside key regulatory frameworks from bodies such as the European Union \\n(European Parliament 2023) and NIST (2024b), reinforce the imperative of \\ndeveloping AI systems that are not only transparent and fair but also respect \\ndata privacy and societal norms. By situating this study within the context of \\nthese contemporary advancements, the aim is to build upon these discussions \\nby offering a comparative analysis of global AI policy frameworks, extending \\nthe dialog on how ethical AI can be systematically developed and maintained \\nacross diverse geopolitical contexts.\\nWhile this study references key AI policy frameworks from the European \\nUnion, the United States, and China, the focus of this study is a more granular \\nexamination of the challenges in comparing such diverse frameworks is',\n",
       "  'Union, the United States, and China, the focus of this study is a more granular \\nexamination of the challenges in comparing such diverse frameworks is \\ncrucial. AI ethics policies are deeply influenced by cultural, socio-political, \\nand economic contexts, making cross-regional comparisons inherently com\\xad\\nplex. For instance, the European Union’s AI Act places significant emphasis \\non safeguarding individual rights, prioritizing transparency and human over\\xad\\nsight (European Parliament 2023), reflecting the EU’s regulatory ethos aimed \\nat protecting citizens from the potential harms of AI. In contrast, the United \\nStates’ AI governance is more decentralized, with a focus on promoting \\ninnovation and maintaining global technological leadership, as seen in the \\nNational Institute of Standards and Technology (NIST) AI Risk Management \\nFramework (2023a), which advocates for flexible, non-prescriptive guidelines \\nthat encourage industry-led solutions.',\n",
       "  'National Institute of Standards and Technology (NIST) AI Risk Management \\nFramework (2023a), which advocates for flexible, non-prescriptive guidelines \\nthat encourage industry-led solutions.\\nChina’s AI policy framework, meanwhile, is characterized by its focus on \\nstate security, social harmony, and the integration of AI into national eco\\xad\\nnomic strategies (Roberts et al. 2021). This framework aligns with China’s \\nbroader governmental control over technology, where the state plays a central \\nrole in guiding AI development. These diverging priorities highlight the \\ninherent challenges in creating universal standards for AI ethics. The task of \\ncomparing these frameworks, therefore, requires consideration of their dis\\xad\\ntinct legal, cultural, and economic motivations, as well as the varying levels of \\npublic trust in AI technologies across these regions.\\nThis study addresses these complexities by identifying common ethical',\n",
       "  'public trust in AI technologies across these regions.\\nThis study addresses these complexities by identifying common ethical \\nprinciples such as fairness, transparency, and privacy, and by analyzing how \\neach region interprets and prioritizes these principles. By employing compara\\xad\\ntive tools such as Venn diagrams and Cartesian graphs, the article visually and \\nanalytically demonstrates the differences, but also the common points in these \\nframeworks. The aim of this study was not to look only for the differences, but \\nto find a solution for global AI governance and to promote the potential for \\nharmonization across jurisdictions.\\nThis paper explores the pressing need for ethical considerations in the \\nrapidly evolving domain of Artificial Intelligence (AI) (Meissner 2020). This \\ne2463722-2\\nP. RADANLIEV',\n",
       "  'technology has significantly impacted various sectors, including healthcare, \\nfinance, and communication. This study aims to establish a robust ethical \\nframework for AI development by addressing complex issues such as data \\nprivacy, algorithmic transparency, and fairness. Our objectives include analyz\\xad\\ning fundamental ethical principles, comparing international AI policy frame\\xad\\nworks (Helbing et al. 2018), proposing strategies for bias mitigation, and \\ncontributing to academic and practical discussions in AI ethics. This paper \\nis structured to systematically dissect these topics, providing an in-depth \\nexploration of AI ethics and its implications for future AI development and \\ngovernance (de Fine Licht and de Fine Licht 2020).\\nThe Imperative of Ethical Considerations in AI\\nThe advent of Artificial Intelligence (AI) has inaugurated a new epoch in \\ntechnological evolution, profoundly influencing diverse sectors, including',\n",
       "  'The Imperative of Ethical Considerations in AI\\nThe advent of Artificial Intelligence (AI) has inaugurated a new epoch in \\ntechnological evolution, profoundly influencing diverse sectors, including \\nhealthcare, finance, transportation, and communication (Hosny et al. 2018; \\nNIST 2023b; Yu, Beam, and Kohane 2018). This unprecedented integration of \\nAI into the societal fabric necessitates the urgent formulation of robust ethical \\nframeworks. These frameworks must address the complexities inherent in AI \\ntechnologies, such as data privacy, algorithmic opacity, equity in decision- \\nmaking, and broader societal impacts.\\nEthical considerations in AI transcend academic discourse, bearing signifi\\xad\\ncant real-world repercussions. Paramount among these are issues related to \\ndata privacy and the need for informed consent, where personal information \\noften powers AI algorithms. Equally critical is the transparency and explic\\xad',\n",
       "  'data privacy and the need for informed consent, where personal information \\noften powers AI algorithms. Equally critical is the transparency and explic\\xad\\nability of these algorithms, which are essential for sustaining public trust, \\nespecially in high-stakes scenarios like legal adjudication or medical diagnos\\xad\\ntics. Moreover, the challenge of ensuring equity and circumventing ingrained \\nbiases in AI systems is a pivotal ethical imperative, given these systems’ \\npropensity to mirror and perpetuate existing societal disparities.\\nThe need for ethical AI is driven by the imperatives of harm prevention and \\njustice but also by the strategic objective of nurturing sustainable, socially \\nbeneficial, and universally accepted innovation.\\nAims and Objectives of the Study\\nThe primary goals of this academic study are threefold. First, it seeks to \\nexplore the ethical considerations inherent in the development of AI. This',\n",
       "  'Aims and Objectives of the Study\\nThe primary goals of this academic study are threefold. First, it seeks to \\nexplore the ethical considerations inherent in the development of AI. This \\ninvolves thoroughly examining the fundamental ethical principles of transpar\\xad\\nency, equity, and privacy within AI systems and understanding how they relate \\nto each other and their significance in isolation. Second, the research aims to \\ncritically analyze various global AI policy frameworks, focusing on those from \\nthe EU, the US, and China. The goal is to discern their similarities and \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-3',\n",
       "  'differences and what they mean for international AI governance. Third, the \\npaper intends to provide a synthesis of approaches and practices to recognize \\nand mitigate bias in AI systems, ensuring their fairness and dependability. This \\nstudy aims to contribute to the ongoing academic and practitioner dialog on \\nAI ethics by offering relevant insights and recommendations to both groups. \\nThe overarching goal is to promote a more ethical and responsible path for AI \\ndevelopment.\\nStructure and Content of the Paper\\nThe paper has been methodically structured to explore AI ethics across various \\ndimensions systematically. Section 2, expands into the foundational ethical \\nprinciples in AI: transparency, equity, and privacy. The section uses a Venn \\ndiagram to demonstrate the interaction between these principles, emphasizing \\ntheir interconnectedness and how they relate to AI.\\nMoving on to section 3, the focus is on integrating global AI policy frame\\xad',\n",
       "  'their interconnectedness and how they relate to AI.\\nMoving on to section 3, the focus is on integrating global AI policy frame\\xad\\nworks within AI development and deployment processes. The section presents \\na flowchart outlining the critical stages in AI projects and the influence of \\nvarious international frameworks. This section examines the role of policies in \\nensuring responsible AI development and the importance of incorporating \\ninternational frameworks to achieve this goal.\\nSection 4, provides a comparative analysis of AI Ethics Policy Frameworks \\nfrom different nations, using a Cartesian graph for evaluation based on \\ntransparency, accountability, equity, and privacy. This seciton highlights the \\nvarying approaches different countries take to AI ethics policies and how they \\ncompare.\\nSection 5, proposes a range of strategies for addressing and reducing bias in \\nAI systems. It emphasizes the significance of data diversity, rigorous audits,',\n",
       "  'compare.\\nSection 5, proposes a range of strategies for addressing and reducing bias in \\nAI systems. It emphasizes the significance of data diversity, rigorous audits, \\nethical training, and algorithmic clarity in reducing bias in AI systems.\\nFinally, the paper concludes with section 6, which summarizes the key \\nfindings, discusses their implications for the future trajectory of AI develop\\xad\\nment, and suggests avenues for further scholarly inquiry. The paper provides \\na comprehensive, multifaceted examination of AI ethics through this struc\\xad\\ntured approach, contributing substantive insights to the ongoing scholarly \\ndialog in this critically pivotal domain.\\nEthical Considerations in AI Development\\nEthical considerations are of the utmost importance in the development of AI \\nto ensure that these systems are safe, fair, and transparent. A Venn diagram \\nillustrates the interplay between three key aspects: transparency, fairness, and \\nprivacy.\\ne2463722-4\\nP. RADANLIEV',\n",
       "  'Transparency, Explainability, and Clarity refer to making AI systems clear \\nand understandable to users. Fairnessrequires that AI systems be designed \\nequitably and without biases. Privacy, or “Data Protection and Consent,” \\nfocuses on protecting personal data and obtaining informed consent for its \\nusage.\\nThese aspects are interconnected, and the intersections of the Venn diagram \\nillustrate how they overlap. For example, the intersection of transparency and \\nfairness is called accountability, which emphasizes the importance of trans\\xad\\nparent and fair AI decision-making. The intersection of transparency and \\nprivacy, called user trust, emphasizes the need for transparency in data use \\nand protection (Aldoseri, Al-Khalifa, and Hamouda 2023; Bécue, Praça, and \\nGama 2021; Malhotra 2018; Mijwil, Aljanabi, and ChatGPT 2023). The inter\\xad\\nsection of fairness and privacy, known as nondiscriminatory data practices, \\nhighlights the need for privacy considerations to align with fairness to avoid',\n",
       "  'section of fairness and privacy, known as nondiscriminatory data practices, \\nhighlights the need for privacy considerations to align with fairness to avoid \\ndiscrimination.\\nThe center intersection of the Venn diagram represents the ideal of respon\\xad\\nsible AI use that balances transparency, fairness, and privacy. A flowchart \\nprovides a clear, step-by-step guide to embed ethical considerations into AI \\ndevelopment.\\nThe process starts with a commitment to ethical AI development and \\ndefining ethical principles such as transparency, fairness, and privacy. Data \\nuse and AI training guidelines should then be implemented to adhere to these \\nprinciples. Regular audits should be conducted to identify and correct biases \\nand ensure compliance with ethical standards.\\nClear lines of responsibility and accountability should be established in AI- \\ndriven decisions. Continuous improvement based on feedback from users and \\nstakeholders should be a regular practice. The goal is the realization of AI',\n",
       "  'driven decisions. Continuous improvement based on feedback from users and \\nstakeholders should be a regular practice. The goal is the realization of AI \\nsystems that fully embody ethical principles and ensure the safety and well- \\nbeing of all users.\\nThe framework from Figure 1 is discussed in more detail in the next section.\\nTransparency, Explainability, and Clarity\\nIn developing artificial intelligence, it is essential to prioritize transparency, \\nexplainability, and clarity to ensure ethical development and deployment. \\nTransparency refers to the accessibility of AI systems and their workings to \\nusers and stakeholders. Explainability, closely linked to transparency, pertains \\nto the ability of AI systems to be understood and interpreted by human beings, \\nideally in non-technical language. Meanwhile, clarity ensures that AI systems’ \\npurposes and outcomes are communicated in a straightforward and under\\xad\\nstandable manner.',\n",
       "  'ideally in non-technical language. Meanwhile, clarity ensures that AI systems’ \\npurposes and outcomes are communicated in a straightforward and under\\xad\\nstandable manner.\\nThe importance of these elements cannot be overstated. They are crucial in \\nbuilding and maintaining user trust, ensuring that AI systems operate \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-5',\n",
       "  'Figure 1. Transparency, Explainability, and Clarity: A framework for developing AI systems that \\nembody ethical principles and ensure the safety and well-being of all users.\\ne2463722-6\\nP. RADANLIEV',\n",
       "  'understandably and predictably. Furthermore, transparency and explainability \\nplay a pivotal role in establishing accountability, ensuring that AI developers \\nand users are held responsible for the outcomes of AI systems (European \\nParliament 2023; ISO 2023; McCorduck and Cfe 2004; MeitY 2023; NIST  \\n2023b; Office for Artificial Intelligence 2023).\\nFairness and Bias Prevention\\nEnsuring fairness in AI systems requires creating programs that make deci\\xad\\nsions without prejudice or partiality (Bender et al. 2021). This necessitates \\na conscientious effort to design AI systems that do not perpetuate existing \\nbiases or create new ones (Shu, Zhang, and Yu 2021). However, achieving \\nfairness in AI poses significant challenges, as these systems often learn from \\nreal-world data, which can be inherently biased.\\nThe intersection of fairness, privacy, and accountability is a complex but \\nessential consideration. Ensuring fairness often involves careful handling of',\n",
       "  'The intersection of fairness, privacy, and accountability is a complex but \\nessential consideration. Ensuring fairness often involves careful handling of \\nsensitive data while also maintaining transparency and accountability in \\ndecision-making processes. This balancing act is critical in mitigating biases \\nand ensuring that AI systems are equitable and just.\\nPrivacy and Data Protection\\nPrivacy and data protection are critical ethical considerations that must be \\nconsidered during AI development. It involves protecting personal and sensi\\xad\\ntive information from unauthorized access and ensuring that data is used \\nresponsibly. Regulations and standards, such as the General Data Protection \\nRegulation (GDPR) (GDPR 2018; ICO 2018) in the European Union, play \\na significant role in shaping AI ethics by setting strict guidelines for data use. \\nPrivacy, fairness, and user trust are closely linked. Protecting privacy is crucial',\n",
       "  'a significant role in shaping AI ethics by setting strict guidelines for data use. \\nPrivacy, fairness, and user trust are closely linked. Protecting privacy is crucial \\nin building and maintaining user trust, which is essential for the acceptance \\nand success of AI systems. Furthermore, ensuring the proper handling of data \\nis vital for fairness, as data misuse can lead to biased outcomes.\\nInterconnectedness of Ethical Aspects\\nThe ethical dimensions of AI, including transparency, fairness, and privacy, \\nare not isolated but deeply interconnected (Partnership on AI 2023; Roberts \\net al. 2021). We can visualize this interconnectedness using a Venn diagram \\nthat shows how these aspects overlap and influence each other. For instance, \\nwhen transparency and fairness intersect, it leads to accountability. Similarly, \\nwhen fairness and privacy overlap, it underscores the need for nondiscrimi\\xad\\nnatory data practices. The idea of responsible AI use is represented by the',\n",
       "  'when fairness and privacy overlap, it underscores the need for nondiscrimi\\xad\\nnatory data practices. The idea of responsible AI use is represented by the \\ncentral intersection of these aspects in the Venn diagram. This is where all \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-7',\n",
       "  'three principles are balanced, leading to AI systems that are ethical, reliable, \\nand trustworthy. We can visualize this in Figure 2.\\nImplementation Strategies\\nIncorporating ethical considerations into the development of AI requires \\na systematic approach. A step-by-step guide for doing this involves first \\ncommitting to ethical principles. Next, data use and AI training guidelines \\nshould be implemented to align with these principles. Regular audits are \\nnecessary to detect and correct biases to ensure compliance with ethical \\nstandards.\\nEstablishing clear lines of responsibility and accountability in AI-driven \\ndecisions is also crucial. Continuous improvement, based on feedback from \\nFigure 2. Interconnected concepts of the Framework for developing AI systems that embody \\nTransparency, Explainability and Clarity.\\ne2463722-8\\nP. RADANLIEV',\n",
       "  'users and stakeholders, should be an integral part of AI development. The \\nultimate goal is to create AI systems that embody ethical principles, ensuring \\nall users’ safety and well-being.\\nResponsible AI Frameworks\\nThis section will explore AI frameworks from the European Union, the \\nUnited States, and China. Using a detailed flowchart, we will examine how \\nthese frameworks impact each stage of an AI project, from initiation to \\npost-deployment. Additionally, we will use a Venn diagram analysis to \\ncompare the EU AI Act (Bommasani et al. 2023), US AI Principles \\n(Tabassi 2023), and China AI Ethics guidelines (Roberts et al. 2021), high\\xad\\nlighting their unique features and areas of overlap. This approach will \\ndemonstrate how these frameworks share a commitment to ethical stan\\xad\\ndards, privacy protection, and fairness while also providing distinct per\\xad\\nspectives on AI development and governance. This analysis is crucial for',\n",
       "  'dards, privacy protection, and fairness while also providing distinct per\\xad\\nspectives on AI development and governance. This analysis is crucial for \\nunderstanding the multifaceted nature of global AI frameworks and their \\nimplications for responsible AI practices.\\nWhile this paper primarily focuses on theoretical frameworks and policy \\nanalysis, it is important to acknowledge the growing need for empirical \\nresearch to substantiate the claims made within the scope of AI ethics. In \\nresponse, we introduce two key case studies that provide concrete examples of \\nhow AI ethics frameworks are applied in practice. These case studies were \\nderived from in-depth analyses of recent AI implementations in both the \\nFigure 3. Key elements found in global AI frameworks.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-9',\n",
       "  'healthcare and financial sectors, which serve as critical areas where ethical \\nconsiderations are paramount.\\nThe first case study examines the deployment of AI diagnostic systems in \\nthe European healthcare industry, particularly focusing on IBM Watson \\nHealth’s use of AI in oncology diagnostics. By conducting structured inter\\xad\\nviews with healthcare practitioners and reviewing compliance reports, we \\nobserved how the stringent transparency and accountability requirements \\nmandated by the European Union’s AI Act (European Parliament 2023) \\ninfluenced the AI system’s design and operational transparency. This empiri\\xad\\ncal evidence demonstrates how AI systems were modified to meet regulatory \\nstandards, particularly concerning the explainability of diagnostic recommen\\xad\\ndations provided to medical professionals and patients.\\nThe second case study involves an empirical assessment of AI fraud \\ndetection systems in the financial services sector within the United States.',\n",
       "  'The second case study involves an empirical assessment of AI fraud \\ndetection systems in the financial services sector within the United States. \\nThrough direct engagement with industry experts and an analysis of \\nFigure 4. Applied design of responsible and ethical AI practices by integrating global AI frame\\xad\\nworks into different stages of AI development and deployment.\\ne2463722-10\\nP. RADANLIEV',\n",
       "  'internal auditing processes, we explored how JP Morgan’s AI-powered \\nfraud detection system aligns with the flexible, innovation-driven guidelines \\nof the NIST AI Risk Management Framework (NIST 2023a; Tabassi 2023). \\nThe study reveals how these frameworks permit adaptive risk management \\nstrategies, providing companies with the autonomy to tailor their ethical \\nstandards while maintaining a balance between innovation and \\naccountability.\\nThese case studies provide empirical evidence to support the theoretical and \\npolicy analysis discussed in this paper. They illustrate how global AI ethics \\nframeworks are not just abstract concepts but operational guidelines that have \\ntangible impacts on AI design, deployment, and compliance. By incorporating \\nthese real-world applications, we aim to bridge the gap between theory and \\npractice, offering a more robust foundation for understanding the dynamics of \\nAI governance across various industries.',\n",
       "  'these real-world applications, we aim to bridge the gap between theory and \\npractice, offering a more robust foundation for understanding the dynamics of \\nAI governance across various industries.\\nFigure 5. Different global AI frameworks overlap in some areas but maintain unique characteristics \\nin others.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-11',\n",
       "  'Development and Deployment Flowchart\\nCreating and implementing AI systems is a complex process that requires \\ncompliance with international frameworks to ensure ethical and responsible \\noutcomes. This section provides a comprehensive flowchart that integrates AI \\nframeworks from the European Union, the United States, and China, mapping \\ntheir application throughout the AI project lifecycle.\\nThe flowchart is based on the EU AI Act (European Parliament 2023), US \\nAI Principles (NIST 2024c), and China AI Ethics guidelines (Provisions on the \\nFigure 6. The complexity of AI frameworks across different regions and the potential for collabora\\xad\\ntion and divergence highlights the need for global collaboration and harmonisation of AI ethics \\nand regulations.\\ne2463722-12\\nP. RADANLIEV',\n",
       "  'Administration of Deep Synthesis Internet Information Services, Personal \\nInformation Protection Law of the People’s Republic of China PRC 2022). It \\nbegins with initiating an AI project, where objectives are defined and relevant \\ndata is collected. The EU AI Act’s guidelines on ethical data use and transpar\\xad\\nency are essential at this stage. The US AI Guidelines come into play as the \\nproject progresses to data processing and AI model development, emphasizing \\ninnovation, fairness, and accountability.\\nDuring the validation and testing phase, the model must align with the set \\nobjectives and comply with ethical standards per these frameworks. The \\ndeployment phase sees the integration of China’s AI Ethics guidelines, which \\nprioritize social harmony, national security, and global cooperation. After \\ndeployment, continuous monitoring and maintenance are essential to ensure \\nthe AI system functions as intended and adheres to ethical standards. This',\n",
       "  'deployment, continuous monitoring and maintenance are essential to ensure \\nthe AI system functions as intended and adheres to ethical standards. This \\nphase is critical for incorporating feedback and insights, allowing for iterative \\nimprovements based on real-world performance and impact.\\nFigure 3 illustrates how global AI frameworks are necessary and applicable \\nat different AI development and deployment stages. This integration ensures \\na holistic approach to responsible AI practices that align with global standards \\nand ethical considerations.\\nFigure 3 shows the steps involved in developing and deploying AI systems. \\nIt incorporates the latest AI frameworks worldwide, including those from the \\nEU, the US (NAIAC 2024, NIST 2024a), and China (Interim Measures for the \\nManagement of Generative Artificial Intelligence Services, Personal \\nInformation Protection Law of the People’s Republic of China PRC 2023; Li  \\n2017; Provisions on the Administration of Deep Synthesis Internet',\n",
       "  'Information Protection Law of the People’s Republic of China PRC 2023; Li  \\n2017; Provisions on the Administration of Deep Synthesis Internet \\nInformation Services, Personal Information Protection Law of the People’s \\nRepublic of China PRC 2022; The State Council People Republic of China  \\n2017), and highlights critical points where these frameworks intersect. The \\nprocess flow begins with the start of the AI project and moves on to identifying \\nobjectives and collecting relevant data. The EU AI Act’s guidelines may be \\nconsidered at this stage. The collected data is then processed, and the AI model \\nis developed according to the principles outlined in the US AI Guidelines. The \\nmodel is then validated and tested to meet the set objectives. Deployment of \\nthe AI system in a real-world environment is the next step, and considerations \\nfrom China’s AI Ethics guidelines come into play here. Continuously mon\\xad\\nitoring and maintaining the AI system post-deployment is essential. The',\n",
       "  'from China’s AI Ethics guidelines come into play here. Continuously mon\\xad\\nitoring and maintaining the AI system post-deployment is essential. The \\nflowchart also includes a feedback loop that involves revisiting objectives \\nand processes based on feedback and new insights. Once all the steps are \\ncompleted, the AI project cycle ends. The flowchart in Figure 4 ensures \\nresponsible and ethical AI practices by integrating global AI frameworks \\ninto different stages of AI development and deployment.\\nFigure 4 visualizes how various AI frameworks integrate with the AI devel\\xad\\nopment and deployment process. Each stage of the AI process is marked, from \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-13',\n",
       "  'the beginning to the end. The EU, the US, and China AI frameworks are \\nhighlighted with individual annotations. The arrows linking these frameworks \\nto specific stages in the process are designed to avoid text overlap, ensuring \\nclarity and readability.\\nThe flowchart effectively illustrates the integration of global AI frame\\xad\\nworks into the AI development lifecycle, emphasizing the importance of \\nconsidering these guidelines at different stages for responsible AI practices. \\nThe annotations for the AI frameworks from the EU, US, and China are \\nstrategically positioned to avoid obstructing any other flowchart elements. \\nThe connecting arrows are designed with a specific arc to neatly link the \\nframeworks to their respective stages in the AI process without crossing \\nover any text.\\nThe Venn diagram represents the AI frameworks from the EU, the US, and \\nChina. Each circle in the diagram represents a different region’s AI framework:',\n",
       "  'over any text.\\nThe Venn diagram represents the AI frameworks from the EU, the US, and \\nChina. Each circle in the diagram represents a different region’s AI framework: \\nEU AI Act, US AI Principles, and China AI Ethics. The overlaps between the \\ncircles indicate areas of common focus or principles shared between these \\nframeworks. Individual sections highlight unique aspects of each framework.\\nThis Venn diagram in Figure 5 visually demonstrates how different global \\nAI frameworks overlap in some areas while maintaining unique characteristics \\nin others. It reflects the diverse approaches to AI governance and ethics across \\nthese regions.\\nThe Venn diagram in Figure 5 comprehensively analyses the AI frameworks \\nfrom the European Union, the United States, and China. It highlights the areas \\nof collaboration and divergence between the three regions and demonstrates \\nthe complexity of AI frameworks across different regions.',\n",
       "  'of collaboration and divergence between the three regions and demonstrates \\nthe complexity of AI frameworks across different regions.\\nThe European Union’s AI Act focuses on human oversight, nondiscrimina\\xad\\ntion, and regulatory compliance. The US AI Principles emphasize innovation \\nencouragement, public trust, and open collaboration. China’s AI Ethics prior\\xad\\nitizes social harmony, national security, and global cooperation. These unique \\nelements reflect the individual priorities and cultural perspectives of each \\nregion.\\nThe EU and the US share values in transparency and ethical standards. The \\nEU and China both emphasize privacy protection and ethical standards. The \\nUS and China find common ground in ethical standards and a focus on \\nfairness. These common areas between the two frameworks indicate the \\npotential for global collaboration and harmonization of AI ethics and \\nregulations.\\nThe central overlap in the Venn diagram highlights the areas where the EU,',\n",
       "  'potential for global collaboration and harmonization of AI ethics and \\nregulations.\\nThe central overlap in the Venn diagram highlights the areas where the EU, \\nUS, and China share common principles such as ethical standards and privacy \\nprotection. These areas offer opportunities for global collaboration and har\\xad\\nmonization of AI ethics and regulations.\\nEach region has unique focus areas that reflect its priorities and cultural \\nperspectives. These divergent approaches could lead to different approaches in \\ne2463722-14\\nP. RADANLIEV',\n",
       "  'AI development and governance. The Venn diagram demonstrates the poten\\xad\\ntial for collaboration and divergence in the global AI landscape.\\nThe Venn diagram analysis in Figure 6 shows the complexity of AI frame\\xad\\nworks across different regions and the potential for collaboration and diver\\xad\\ngence. It highlights the need for global collaboration and harmonization of AI \\nethics and regulations to ensure that AI development and governance align \\nwith ethical and societal values. \\nThe Venn diagram in Figure 6 shows the EU AI Act i in light blue, the US AI \\nPrinciples are in light green, and the China AI Ethics framework in coral.\\nReal-World Applications of Global AI Frameworks\\nIn order to provide a clearer understanding of how AI frameworks function in \\npractice, it is essential to examine real-world applications of these frameworks \\nacross different regions. A relevant example can be found in the application of \\nthe European Union’s AI Act within the healthcare sector. The use of AI-',\n",
       "  'across different regions. A relevant example can be found in the application of \\nthe European Union’s AI Act within the healthcare sector. The use of AI- \\npowered diagnostic tools, such as IBM’s Watson Health, was subject to \\nscrutiny under the EU’s stringent regulations on transparency and explain\\xad\\nability. Under the AI Act, companies deploying such AI systems in high-risk \\nsectors are required to provide detailed documentation of the algorithms used, \\nas well as explainability mechanisms that allow medical professionals and \\npatients to understand AI-driven decisions. This regulatory requirement has \\nled to the modification of AI models to ensure compliance, particularly by \\nproviding more transparent decision-making processes that can be audited by \\nhealthcare regulators.\\nIn contrast, the United States’ more innovation-centric approach, as embo\\xad\\ndied by the NIST AI Risk Management Framework, can be observed in the',\n",
       "  'healthcare regulators.\\nIn contrast, the United States’ more innovation-centric approach, as embo\\xad\\ndied by the NIST AI Risk Management Framework, can be observed in the \\ndeployment of AI in the financial services sector. For instance, JP Morgan’s \\nAI-powered fraud detection system operates within a framework that empha\\xad\\nsizes risk mitigation through best practices and industry standards rather than \\nrigid regulatory oversight. The NIST framework encourages companies to \\ndevelop internal policies tailored to their operational risks, allowing for greater \\nflexibility in AI implementation. As a result, JP Morgan has developed pro\\xad\\nprietary methods for continuous monitoring and auditing of AI models to \\nensure they remain effective while balancing the need for innovation with \\nethical considerations.\\nChina’s AI governance, which prioritizes state control and societal \\nharmony, can be seen in the government’s use of facial recognition',\n",
       "  'ethical considerations.\\nChina’s AI governance, which prioritizes state control and societal \\nharmony, can be seen in the government’s use of facial recognition \\nsystems for public security. The deployment of such systems, governed \\nby China’s Provisions on the Administration of Deep Synthesis Internet \\nInformation Services, Personal Information Protection Law of the \\nPeople’s Republic of China (PRC) (2022), illustrates how the state \\nleverages AI under a framework that prioritizes national security. In this \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-15',\n",
       "  'case, AI technologies are used to monitor public spaces, but their ethical \\nimplications, particularly in terms of privacy, are handled within \\na governance model that differs significantly from those in Western \\ndemocracies. The Chinese government’s emphasis on AI as a tool for \\nsocietal stability underscores the unique application of their framework in \\npractice.\\nThese examples highlight the varied approaches of AI frameworks across \\ndifferent regions and sectors, demonstrating how global AI policies are shaped \\nby contextual factors and applied in practical, high-impact scenarios. By \\nexamining such real-world cases, we can better understand the strengths and \\nlimitations of these frameworks and the challenges in harmonizing AI ethics \\non a global scale.\\nSpecific Recommendations for Real-World Applications of Global AI Frameworks \\nInclude\\nTechnical Solutions for Embedding Ethical Principles in AI Systems. Embedding',\n",
       "  'on a global scale.\\nSpecific Recommendations for Real-World Applications of Global AI Frameworks \\nInclude\\nTechnical Solutions for Embedding Ethical Principles in AI Systems. Embedding \\nethical principles such as fairness, transparency, and accountability into AI \\nsystems requires sophisticated algorithmic approaches that ensure these objec\\xad\\ntives are met without compromising the system’s performance. Algorithmic \\nfairness can be addressed using methods such as differential fairness and fair \\nrepresentation learning. For instance, algorithms like the Fair Representation \\nLearning (FRL) model aim to mitigate bias by transforming raw data into \\na latent representation that is invariant to sensitive attributes, such as race or \\ngender, without losing important predictive power. The FRL method applies \\nadversarial learning to ensure the model cannot easily infer sensitive attri\\xad\\nbutes, thus reducing bias while maintaining accuracy. This can be particularly',\n",
       "  'adversarial learning to ensure the model cannot easily infer sensitive attri\\xad\\nbutes, thus reducing bias while maintaining accuracy. This can be particularly \\nuseful in sectors like finance, where historical biases in credit scoring datasets \\noften lead to unfair outcomes. Incorporating these fairness constraints during \\nthe model training phase ensures that discriminatory patterns in the data are \\nnot propagated by the AI system.\\nTransparency is enhanced through the use of explainable AI (XAI) tech\\xad\\nniques. One common approach is the implementation of Local Interpretable \\nModel-agnostic Explanations (LIME), which provides users with interpreta\\xad\\nble approximations of complex models, enabling end-users and auditors to \\nunderstand and evaluate individual predictions. LIME works by perturbing \\ninput data and observing how changes impact predictions, thus constructing \\nsimpler, interpretable models locally around specific instances. This method is',\n",
       "  'input data and observing how changes impact predictions, thus constructing \\nsimpler, interpretable models locally around specific instances. This method is \\nparticularly valuable in high-stakes fields like healthcare, where understanding \\nthe rationale behind AI-driven diagnoses is critical for building trust and \\naccountability. For example, LIME has been effectively applied in medical \\nimaging to explain how AI systems identify tumor regions, offering transpar\\xad\\nency to both clinicians and patients.\\ne2463722-16\\nP. RADANLIEV',\n",
       "  'Obtaining Real-World Probabilistic Data for Legislation. To create more robust \\nAI legislation that addresses real-world challenges, probabilistic data collec\\xad\\ntion is necessary. A critical solution lies in data-driven simulations that use \\nreal-world probabilistic distributions of AI outcomes across various domains. \\nThese simulations can leverage Bayesian inference models to analyze the \\nprobability of ethical failures, such as biased decisions or transparency \\nbreaches, under different regulatory scenarios. For example, Bayesian models \\ncan assess the likelihood of biased outputs in loan approval systems based on \\nvarying regulatory constraints, allowing policymakers to quantitatively evalu\\xad\\nate the trade-offs between stringent regulation and innovation. By incorporat\\xad\\ning such probabilistic assessments, policymakers can develop legislation \\ngrounded in empirical evidence, ensuring that ethical guidelines are both \\npractical and enforceable in diverse sectors.',\n",
       "  'ing such probabilistic assessments, policymakers can develop legislation \\ngrounded in empirical evidence, ensuring that ethical guidelines are both \\npractical and enforceable in diverse sectors.\\nMoreover, quantitative data collection from real-world AI deployments \\ncould utilize techniques such as differential privacy to protect sensitive \\ninformation while still gathering meaningful insights. For instance, in health\\xad\\ncare, collecting large-scale patient data from AI diagnostic tools while main\\xad\\ntaining patient privacy can be achieved through differential privacy algorithms \\nthat introduce noise into datasets, ensuring that individual records cannot be \\nre-identified. This allows regulators to gather accurate statistics on AI system \\nperformance, such as prediction accuracy and error rates, without violating \\nprivacy laws. These real-world data points can then be used to fine-tune \\nlegislative frameworks to ensure they are reflective of practical AI use and',\n",
       "  'privacy laws. These real-world data points can then be used to fine-tune \\nlegislative frameworks to ensure they are reflective of practical AI use and \\ncompliant with privacy standards.\\nAlgorithmic Solutions to Ensure Fair and Ethical AI for End-Users. To ensure \\nAI systems are perceived as fair and ethical by end-users, several algorithmic \\napproaches can be integrated into the development lifecycle. One promising \\nmethod is the use of fairness constraints in model optimization, such as \\nEqualised Odds and Demographic Parity. The Equalised Odds algorithm \\nensures that an AI system has equal true positive and false positive rates across \\ndifferent demographic groups, ensuring that no group disproportionately \\nbenefits or suffers from the system’s decisions. This technique has been \\nsuccessfully implemented in judicial systems where AI models are used for \\nbail and sentencing recommendations, reducing the racial disparities com\\xad\\nmonly observed in earlier models.',\n",
       "  'successfully implemented in judicial systems where AI models are used for \\nbail and sentencing recommendations, reducing the racial disparities com\\xad\\nmonly observed in earlier models.\\nFairness-aware learning algorithms can also be embedded into machine \\nlearning pipelines to monitor and adjust for bias during the training process. \\nFor example, the Fairness through Awareness (FTA) framework adjusts \\ndecision boundaries within models to ensure that similar individuals are \\ntreated similarly, thereby reducing unfair bias. This algorithm calculates dis\\xad\\ntances in a fairness-sensitive space and ensures that individuals who are close \\nin this space receive similar predictions. This has been applied in hiring \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-17',\n",
       "  'algorithms to ensure that applicants with similar qualifications, regardless of \\ndemographic attributes, are treated equitably.\\nFurthermore, end-user engagement with AI systems can be improved \\nthrough interactive transparency mechanisms. For instance, counterfactual \\nexplanations can be used to provide users with actionable insights into how \\ndecisions could change if certain inputs were modified. In credit scoring \\nsystems, for example, a counterfactual explanation might inform a user that \\ntheir loan was denied due to a low credit score and suggest specific steps, such \\nas reducing credit card debt, that would lead to approval. By providing users \\nwith clear, actionable insights, these systems not only increase trust but also \\nempower users to engage more meaningfully with AI-driven decisions.\\nAlgorithmic Accountability and Continuous Monitoring. To maintain ongoing \\nfairness and ethical standards, continuous monitoring of AI systems is essen\\xad',\n",
       "  'Algorithmic Accountability and Continuous Monitoring. To maintain ongoing \\nfairness and ethical standards, continuous monitoring of AI systems is essen\\xad\\ntial. This can be achieved through algorithmic auditing frameworks that \\nregularly assess AI systems for adherence to ethical principles post- \\ndeployment. Post-hoc fairness auditing tools, such as AI Fairness 360 \\n(AIF360), provide an open-source toolkit that measures and mitigates bias \\nin deployed models. These tools can be integrated into AI governance pro\\xad\\ncesses, ensuring that models remain fair and unbiased as they encounter new \\ndata in real-world environments. AIF360 evaluates fairness through multiple \\nmetrics, such as disparate impact and statistical parity, and enables continuous \\nrecalibration of models to maintain ethical performance.\\nIncorporating algorithmic accountability systems with real-time feedback \\nloops ensures that biases introduced by shifts in data distributions (data drift)',\n",
       "  'Incorporating algorithmic accountability systems with real-time feedback \\nloops ensures that biases introduced by shifts in data distributions (data drift) \\nare swiftly detected and mitigated. Techniques such as drift detection algo\\xad\\nrithms, including ADWIN (Adaptive Windowing), continuously monitor the \\nperformance of AI models and trigger retraining when significant deviations \\nfrom expected behavior are detected. By automating the detection of ethical \\nbreaches and recalibrating models in response, these systems ensure that AI \\nremains both effective and ethically compliant over time.\\nComparative Venn Diagram Analysis\\nThis section uses a Venn diagram analysis to compare the AI frameworks of \\nthe European Union (EU), the United States (US), and China. The diagram \\nrepresents each framework, highlighting their unique features and areas of \\noverlap, revealing both collaborative potentials and divergent approaches.',\n",
       "  'represents each framework, highlighting their unique features and areas of \\noverlap, revealing both collaborative potentials and divergent approaches.\\nThe EU’s AI Act prioritizes human oversight, nondiscrimination, and strict \\nregulatory compliance. It reflects the EU’s emphasis on protecting citizens’ \\nrights in the digital age. The US AI Principles prioritize fostering innovation, \\nensuring public trust, and promoting open collaboration. This mirrors the \\nUS’s emphasis on market-driven and innovation-led AI development. On the \\ne2463722-18\\nP. RADANLIEV',\n",
       "  'other hand, China’s AI Ethics framework emphasizes the importance of social \\nharmony, national security, and global cooperation. It reflects China’s \\napproach to balancing technological advancement with social stability and \\nstate security.\\nThe intersections of these frameworks in the Venn diagram highlight \\nshared principles and potential areas for international cooperation. For exam\\xad\\nple, the EU and the US emphasize ethical standards and transparency. The EU \\nand China share a common focus on privacy protection and the ethical use of \\nAI. The US and China converge on encouraging ethical standards and fairness \\nin AI.\\nAt the central intersection of the Venn diagram, where all three frame\\xad\\nworks overlap, lies a shared commitment to ethical standards, privacy \\nprotection, and ensuring fairness. This common ground suggests opportu\\xad\\nnities for global collaboration and the harmonization of AI ethics and \\nregulations.\\nHowever, each framework’s divergent aspects reflect each region’s',\n",
       "  'nities for global collaboration and the harmonization of AI ethics and \\nregulations.\\nHowever, each framework’s divergent aspects reflect each region’s \\nvarying priorities and cultural perspectives. These differences could \\nlead to distinct approaches in AI development and governance globally. \\nTherefore, the Venn diagram highlights the potential for collaboration \\nand underscores the need to understand and respect diverse perspectives \\nin the global AI landscape.\\nPolicy Frameworks\\nThis section expands into a thorough analysis of AI Ethics Policy \\nFrameworks worldwide. It covers significant regions such as the European \\nUnion, the United States, China, Canada, Japan, India, and Australia. \\nFigure 7 presents the data in Cartesian graphs, comparing these frame\\xad\\nworks across four key ethical dimensions: Transparency, Accountability, \\nFairness, and Privacy. Each framework is evaluated in detail and rated, \\nproviding an insightful understanding of how countries prioritize these',\n",
       "  'Fairness, and Privacy. Each framework is evaluated in detail and rated, \\nproviding an insightful understanding of how countries prioritize these \\ndimensions in their AI policies. This graph highlights each framework’s \\nunique priorities and focus areas and emphasizes the diversity and com\\xad\\nmonalities in global approaches to AI ethics. Policymakers can leverage \\nthese insights to identify areas for improvement and develop comprehen\\xad\\nsive, ethically aligned AI policies. Moreover, the section expands upon this \\nanalysis to include additional critical dimensions such as human oversight \\nand national security, broadening the scope to encompass broader socio- \\npolitical implications of AI technology. Figure 8 analyses global trends and \\npresents a roadmap for harmonization in AI ethics, advocating for ongoing \\ninternational dialogue and cooperation to foster a responsible and ethical \\nglobal AI ecosystem.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-19',\n",
       "  'Criteria for Rating AI Ethics Policy Frameworks Across Countries\\nThe ratings for each country’s AI ethics policy framework were based on \\na detailed analysis of public documents, policy white papers, regulatory guide\\xad\\nlines, and academic literature. The scores for each dimension, transparency, \\naccountability, fairness, privacy, human oversight, ethical standards, \\nFigure 7. Cartesian graph - Comparison of Al Ethics Policy Frameworks Across Countries.\\nFigure 8. Cartesian graph of Global AI Ethics Policy Frameworks: a tool for policymakers to \\nunderstand the priorities of different countries regarding AI ethics.\\ne2463722-20\\nP. RADANLIEV',\n",
       "  'innovation encouragement, and national security, were evaluated according to \\nthe following specific criteria:\\nTransparency (1–10 Scale)\\nTransparency was assessed by the extent to which each country’s framework \\nmandates openness regarding the design, implementation, and decision- \\nmaking processes of AI systems.\\n●9-10: Countries with explicit, legally enforced transparency requirements \\nfor AI systems, including mandates for explainability and public account\\xad\\nability (e.g., the European Union).\\n●6-8: Countries that encourage transparency but do not mandate it as \\na legal requirement across all AI applications (e.g., the United States).\\n●4-5: Countries where transparency is mentioned in policies but with few \\npractical enforcement mechanisms (e.g., China).\\n●1-3: Minimal or no formal focus on transparency in the AI framework.\\nAccountability (1–10 Scale)\\nAccountability measures the robustness of legal and regulatory mechanisms',\n",
       "  '●1-3: Minimal or no formal focus on transparency in the AI framework.\\nAccountability (1–10 Scale)\\nAccountability measures the robustness of legal and regulatory mechanisms \\nthat hold developers, companies, and governments responsible for the out\\xad\\ncomes of AI systems.\\n●9-10: Countries with well-defined liability frameworks that assign clear \\nresponsibility to AI developers or operators (e.g., the EU AI Act).\\n●6-8: Countries where accountability is encouraged through voluntary \\ncompliance frameworks but lacks mandatory enforcement (e.g., the US \\nwith the NIST AI Risk Management Framework).\\n●4-5: Countries with vague accountability measures, often handled at the \\ndiscretion of private entities or lacking centralized regulation (e.g., Japan).\\n●1-3: Countries where accountability frameworks are non-existent or still \\nin early development phases.\\nFairness (1–10 Scale)\\nFairness was evaluated based on how well a country’s policy framework',\n",
       "  '●1-3: Countries where accountability frameworks are non-existent or still \\nin early development phases.\\nFairness (1–10 Scale)\\nFairness was evaluated based on how well a country’s policy framework \\naddresses bias in AI algorithms and ensures equitable outcomes across demo\\xad\\ngraphic groups.\\n●9-10: Countries with explicit fairness requirements for AI systems, man\\xad\\ndating fairness audits and bias mitigation techniques (e.g., Canada, EU).\\n●6-8: Countries that encourage fairness but with less stringent or optional \\nauditing practices (e.g., the US).\\n●4-5: Countries where fairness is an aspirational goal with limited practical \\nimplementation (e.g., India).\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-21',\n",
       "  '●1-3: Minimal or no focus on fairness in AI regulation.\\nPrivacy (1–10 Scale)\\nPrivacy was assessed by how each country’s framework protects user \\ndata in the context of AI and how it aligns with global standards like \\nthe GDPR.\\n●9-10: Countries with robust privacy regulations, including explicit rules \\non AI data use (e.g., EU, Canada).\\n●6-8: Countries with general data privacy laws but limited AI-specific \\nguidelines (e.g., the US, Japan).\\n●4-5: Countries with privacy regulations that are inconsistently applied or \\nunderdeveloped in relation to AI (e.g., India, Australia).\\n●1-3: Countries with minimal focus on privacy in AI contexts, or where \\ndata protection laws are not enforced effectively (e.g., China).\\nHuman Oversight (1–10 Scale)\\nThis criterion assessed the role of human oversight in AI decision- \\nmaking, particularly in high-risk sectors like healthcare or autonomous \\nvehicles.\\n●9-10: Countries mandating human oversight in high-risk AI decisions,',\n",
       "  'making, particularly in high-risk sectors like healthcare or autonomous \\nvehicles.\\n●9-10: Countries mandating human oversight in high-risk AI decisions, \\nensuring human intervention in critical areas (e.g., EU AI Act).\\n●6-8: Countries that recommend but do not legally enforce human over\\xad\\nsight (e.g., the US).\\n●4-5: Countries where human oversight is mentioned, but enforcement \\nmechanisms are vague or absent (e.g., Japan, India).\\n●1-3: Little to no emphasis on human oversight in AI policy frameworks.\\nEthical Standards (1–10 Scale)\\nEthical standards were scored based on how well a country’s AI policies adhere \\nto global ethical frameworks (such as UNESCO’s AI ethics recommendations) \\nand promote ethical AI development.\\n●9-10: Countries with a clearly defined, internationally aligned ethical \\nframework for AI (e.g., the EU, Canada).\\n●6-8: Countries with ethical guidelines for AI but limited in scope or \\nenforcement (e.g., Japan, the US).',\n",
       "  'framework for AI (e.g., the EU, Canada).\\n●6-8: Countries with ethical guidelines for AI but limited in scope or \\nenforcement (e.g., Japan, the US).\\n●4-5: Countries that mention ethical AI but lack a coherent, enforceable \\nframework (e.g., India).\\n●1-3: Minimal or no formal focus on AI ethics in public policy.\\ne2463722-22\\nP. RADANLIEV',\n",
       "  'Innovation Encouragement (1–10 Scale)\\nThis criterion measured the balance between promoting AI innovation and \\nenforcing ethical guidelines. Countries that foster innovation while maintain\\xad\\ning a robust ethical framework scored higher.\\n●9-10: Countries with innovation-centric policies that support AI research \\nand development while integrating ethical guidelines (e.g., US, Canada).\\n●6-8: Countries with strong innovation policies but less rigorous ethical \\nenforcement (e.g., Japan).\\n●4-5: Countries where innovation is promoted but at the cost of ethical \\nstandards (e.g., China).\\n●1-3: Countries where innovation in AI is stifled due to excessive regula\\xad\\ntion or a lack of resources (e.g., minimal focus).\\nNational Security (1–10 Scale)\\nNational security was evaluated based on how countries incorporate AI within \\ntheir national security strategies, including defense, cybersecurity, and \\nsurveillance.\\n●9-10: Countries where AI plays a significant role in national security',\n",
       "  'their national security strategies, including defense, cybersecurity, and \\nsurveillance.\\n●9-10: Countries where AI plays a significant role in national security \\nframeworks, with clear policies on military AI, surveillance, and cyber \\ndefense (e.g., China, the US).\\n●6-8: Countries that include AI in national security policies but with fewer \\nexplicit regulations on its use in defense (e.g., Australia, Japan).\\n●4-5: Countries with some mention of AI in national security contexts but \\nlacking concrete policies (e.g., India).\\n●1-3: Minimal focus on AI for national security, or policies that are still in \\nearly development (e.g., the EU).\\nJustification for the Selection of Criteria and Scores\\nThe chosen criteria for evaluating AI ethics policy frameworks, transparency, \\naccountability, fairness, privacy, human oversight, ethical standards, innova\\xad\\ntion encouragement, and national security, were carefully selected to reflect',\n",
       "  'accountability, fairness, privacy, human oversight, ethical standards, innova\\xad\\ntion encouragement, and national security, were carefully selected to reflect \\nthe core dimensions that are essential for ethically sound, socially beneficial, \\nand technologically responsible AI systems. These dimensions are well- \\nestablished in policy discourse and academic literature as the pillars of AI \\nethics and governance, ensuring that AI development aligns with societal \\nvalues and mitigates potential harms.\\nTransparency\\nTransparency is a cornerstone of AI ethics, as highlighted in both academic \\nand regulatory discussions (Floridi et al. 2018). Transparent AI systems allow \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-23',\n",
       "  'stakeholders to understand how decisions are made and ensure accountability. \\nThe choice of transparency as a criterion is supported by regulatory frame\\xad\\nworks such as the European Union’s GDPR and AI Act, which place explicit \\ndemands on AI systems to be explainable and open to public scrutiny. Studies \\nhave shown that lack of transparency is one of the main causes of public \\ndistrust in AI systems (Wachter, Mittelstadt, and Russell 2023). Therefore, \\ncountries with clear legal mandates for transparency received higher scores, \\nwhile those with voluntary or vague transparency guidelines scored lower.\\nAccountability\\nAccountability ensures that AI developers, operators, and users are held \\nresponsible for the outcomes produced by AI systems. This criterion is \\njustified by the recognition in the literature that without clear accountability \\nstructures, it becomes difficult to address failures or harms caused by AI',\n",
       "  'justified by the recognition in the literature that without clear accountability \\nstructures, it becomes difficult to address failures or harms caused by AI \\nsystems (Mittelstadt 2019). The EU AI Act introduces comprehensive provi\\xad\\nsions that assign legal responsibility, providing a strong model for account\\xad\\nability. Countries like the United States, with voluntary compliance through \\nframeworks like the NIST AI Risk Management Framework, received inter\\xad\\nmediate scores due to the lack of enforceability. The necessity of accountability \\nis also a major theme in academic literature, particularly in the context of \\ncomplex AI systems where multiple stakeholders are involved in the design \\nand deployment (de Bruin and Floridi 2017; Floridi et al. 2018; Turilli and \\nFloridi 2009).\\nFairness\\nFairness in AI systems addresses concerns about bias and discrimination, \\nwhich are well-documented issues in AI applications (Binns 2018).',\n",
       "  'Floridi 2009).\\nFairness\\nFairness in AI systems addresses concerns about bias and discrimination, \\nwhich are well-documented issues in AI applications (Binns 2018). \\nCountries with explicit fairness requirements in their AI policies, such as the \\nEuropean Union and Canada, received higher scores because their frameworks \\nmandate fairness audits and bias mitigation practices. Literature on fairness in \\nAI often points to the limitations of algorithmic systems to ensure equitable \\noutcomes across demographic groups without specific regulatory intervention \\n(Du 2023; IBM 2018). Nations with minimal or non-enforceable fairness \\nprovisions, such as India and China, scored lower due to the absence of robust \\nbias-mitigation mechanisms.\\nPrivacy\\nPrivacy is a critical concern in AI, especially in systems that rely on vast \\namounts of personal data. The GDPR in the EU sets a high global benchmark \\nfor data protection and privacy, justifying the high score for the EU in this',\n",
       "  'amounts of personal data. The GDPR in the EU sets a high global benchmark \\nfor data protection and privacy, justifying the high score for the EU in this \\ndimension. In contrast, countries like the United States, where privacy regula\\xad\\ntions such as HIPAA are domain-specific and not universally applicable to AI \\nsystems, scored lower (HIPAA 1996). Privacy as a criterion is grounded in the \\ne2463722-24\\nP. RADANLIEV',\n",
       "  'principle that ethical AI must protect individuals’ rights to control their data, \\na concern that is pervasive in academic and policy literature (Mittelstadt 2019).\\nHuman Oversight\\nHuman oversight in AI decision-making is essential to prevent over-reliance \\non automated systems, particularly in critical sectors like healthcare and law \\nenforcement (European Parliament 2023). The EU AI Act again leads the way \\nby mandating human oversight in high-risk AI applications. The literature \\nemphasizes the importance of preserving human judgment in AI-assisted \\ndecision-making, especially in cases involving moral or legal consequences \\n(Jobin, Ienca, and Vayena 2019). Countries that recommend but do not \\nmandate human oversight scored lower, as voluntary oversight often fails in \\nreal-world applications, particularly where operational efficiency is prioritized \\nover human intervention.\\nEthical Standards\\nEthical standards are increasingly seen as vital for aligning AI development',\n",
       "  'over human intervention.\\nEthical Standards\\nEthical standards are increasingly seen as vital for aligning AI development \\nwith human values. UNESCO’s Recommendation on the Ethics of AI and \\nsimilar initiatives by the OECD have provided blueprints for ethical AI, \\nfocusing on principles such as beneficence, non-maleficence, autonomy, and \\njustice (UNESCO 2023). Countries like Canada and the EU, which have \\nadopted comprehensive ethical guidelines, scored highly. These standards \\nare crucial for ensuring that AI operates within moral and legal boundaries. \\nIn contrast, countries that lack specific ethical frameworks for AI, such as \\nIndia and China, received lower scores, reflecting the underdevelopment of \\nethical considerations in their AI policies.\\nInnovation Encouragement\\nThe balance between encouraging AI innovation and enforcing ethical stan\\xad\\ndards is a key concern for policymakers (Brynjolfsson and Mcafee 2014; Evans',\n",
       "  'Innovation Encouragement\\nThe balance between encouraging AI innovation and enforcing ethical stan\\xad\\ndards is a key concern for policymakers (Brynjolfsson and Mcafee 2014; Evans  \\n2015). Countries like the United States scored high in this dimension due to \\ntheir innovation-centric policies, such as the NIST AI Risk Management \\nFramework, which promotes industry-led solutions and fosters a favorable \\nenvironment for AI research and development. The literature supports the \\nnotion that innovation thrives when there is flexibility and minimal regulatory \\noverhead, but with the caveat that ethical guardrails must not be neglected \\n(Bostrom and Yudkowsky 2014). Countries that focus excessively on regula\\xad\\ntion, potentially stifling innovation, or that lack sufficient incentives for AI \\nresearch, scored lower.\\nNational Security\\nNational security considerations, particularly regarding the development of \\nautonomous weapons systems (AWS) and AI-enhanced cybersecurity, are',\n",
       "  'research, scored lower.\\nNational Security\\nNational security considerations, particularly regarding the development of \\nautonomous weapons systems (AWS) and AI-enhanced cybersecurity, are \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-25',\n",
       "  'becoming a critical component of AI policy (Singer 2009). Countries like the \\nUS and China, where AI plays a substantial role in national defense strategies, \\nscored highly. The literature on autonomous systems highlights the impor\\xad\\ntance of regulating AI to prevent unintended consequences in military appli\\xad\\ncations (Singer 2009). Countries that have not yet integrated AI into their \\nnational security strategies or have underdeveloped AI governance in this area, \\nsuch as the EU, scored lower.\\nJustification for Scoring. The scores for each dimension were derived from \\na combination of the following sources:\\n●Policy Documents and Regulations: Key regulatory frameworks such as \\nthe EU AI Act (Bommasani et al. 2023; European Parliament 2023, FACT \\nSHEET: Biden-Harris Administration Announces New Actions to \\nPromote Responsible AI Innovation That Protects Americans’ Rights \\nand Safety | The White House, 2023; Mozumder et al. 2022), GDPR',\n",
       "  'SHEET: Biden-Harris Administration Announces New Actions to \\nPromote Responsible AI Innovation That Protects Americans’ Rights \\nand Safety | The White House, 2023; Mozumder et al. 2022), GDPR \\n(GDPR 2018; ICO 2018), NIST AI Risk Management Framework \\n(NIST 2024a, 2024c; Tabassi 2023), and national AI strategies were \\ndirectly analyzed to assess the strength and comprehensiveness of each \\ncountry’s AI governance mechanisms.\\n●Academic Literature: Foundational texts on AI ethics, fairness, transpar\\xad\\nency, and accountability were referenced to establish baseline expecta\\xad\\ntions for what constitutes best practices in each dimension (Binns 2018; \\nFloridi et al. 2018; Jobin, Ienca, and Vayena 2019).\\n●Real-World Case Studies: Examples of AI implementation in various \\nsectors, including healthcare, finance, and national security, were exam\\xad\\nined to contextualize the practical impacts of each policy framework.\\nThe criteria were selected to ensure a comprehensive assessment of each',\n",
       "  'ined to contextualize the practical impacts of each policy framework.\\nThe criteria were selected to ensure a comprehensive assessment of each \\ncountry’s approach to AI ethics, focusing on both regulatory stringency and \\nthe practical application of ethical principles. Each score reflects the extent to \\nwhich the country’s framework addresses key challenges associated with AI \\ngovernance, ensuring a balanced evaluation that is informed by both policy \\nanalysis and academic insights.\\nComparative Analysis Using Cartesian Graphs\\nThis section provides a detailed comparative analysis of AI Ethics Policy \\nFrameworks from a global perspective. The analysis covers major regions \\nsuch as the European Union, the United States, China, Canada, Japan, India, \\nand Australia. The study examines these frameworks against key ethical \\ndimensions, including Transparency, Accountability, Fairness, and Privacy, \\nusing a series of Cartesian graphs.\\ne2463722-26\\nP. RADANLIEV',\n",
       "  'Each framework is rated on a scale of 1 to 10 across these dimensions. The \\nCartesian graph format helps to visualize how each country’s framework \\nmeasures up in these critical areas. For instance, the EU’s framework prior\\xad\\nitizes privacy and accountability, reflected in its high scores in these areas. On \\nthe other hand, the US framework might score higher on transparency because \\nof its focus on open data and innovation. China’s framework, emphasizing \\nsocial harmony and national security, might have different strengths and \\nweaknesses.\\nThis comparative analysis provides insights into the priorities and focus \\nareas of different countries and highlights the diversity and commonalities in \\napproaches to AI ethics globally. The graphical representation aids in under\\xad\\nstanding the complexities of each framework, offering a clear view of how \\nnations are navigating the ethical landscape of AI development.\\nThe Cartesian graph in Figure 7 compares AI Ethics Policy Frameworks',\n",
       "  'nations are navigating the ethical landscape of AI development.\\nThe Cartesian graph in Figure 7 compares AI Ethics Policy Frameworks \\nacross countries, such as the EU, the US, China, Canada, Japan, India, and \\nAustralia. It evaluates them on Transparency, Accountability, Fairness, and \\nPrivacy.\\nFigure 7 presents an overview of how countries prioritize various elements \\nof AI ethics in their policy frameworks. The four aspects used for rating are \\ntransparency, accountability, fairness, and privacy. Each aspect is rated on \\na scale from 1 to 10.\\nThe Blue Line represents transparency, which reflects how policies are \\nopenly communicated and implemented. The Green Line indicates account\\xad\\nability, measuring the extent to which AI developers and users are held \\naccountable for their systems as per the frameworks. The Red Line represents \\nfairness, measuring the extent to which the policies ensure fair and unbiased',\n",
       "  'accountable for their systems as per the frameworks. The Red Line represents \\nfairness, measuring the extent to which the policies ensure fair and unbiased \\nAI systems. Lastly, the Purple Line shows the importance of user privacy and \\ndata protection in the policies.\\nThe graph offers valuable insights into the global landscape of AI ethics. It \\nhighlights similarities and differences in national approaches to regulating AI \\nthat can help inform future policy development. The ratings of each aspect for \\neach country can help policymakers identify areas for improvement in their \\npolicies.\\nThis graph provides a tool for policymakers to understand the priorities of \\ndifferent countries regarding AI ethics. By focusing on transparency, account\\xad\\nability, fairness, and privacy, policymakers can create policies that address the \\nconcerns of stakeholders and help establish ethical guidelines for AI develop\\xad\\nment and use.\\nExtended Analysis and Global Implications',\n",
       "  'concerns of stakeholders and help establish ethical guidelines for AI develop\\xad\\nment and use.\\nExtended Analysis and Global Implications\\nIn this section, we expand into an evaluation matrix that incorporates addi\\xad\\ntional dimensions that are increasingly relevant to AI ethics. These dimensions \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-27',\n",
       "  'include the roles of human oversight and national security, reflecting AI \\ntechnology’s broader socio-political implications.\\nIncluding human oversight highlights the necessity for human intervention \\nand judgment in AI systems, a principle strongly supported by the EU frame\\xad\\nwork. Conversely, national security is crucial in the frameworks of countries \\nlike China and the US, where AI’s involvement in defense and intelligence is \\na pivotal consideration.\\nThis comprehensive analysis emphasizes the global trends in AI policy \\nframeworks and the potential for harmonizing AI ethics. The section \\nexplores the possibility of converging principles and standards despite \\neach region’s diverse cultural, political, and social contexts. While complete \\nuniformity may be unattainable, the potential for international collabora\\xad\\ntion and consensus-building on core principles is significant. Such harmo\\xad\\nnization could facilitate the establishment of universally accepted norms',\n",
       "  'tion and consensus-building on core principles is significant. Such harmo\\xad\\nnization could facilitate the establishment of universally accepted norms \\nand standards, ensuring that AI development aligns with ethical and socie\\xad\\ntal values.\\nThe analysis of similarities and differences in Figure 8 concludes that we \\nneed continued dialogue and cooperation among nations to cultivate \\na responsible and ethical global AI ecosystem.\\nThe Cartesian graph in Figure 8 includes lines representing concepts \\ninspired by earlier Venn diagrams that explored the issues surrounding AI \\ngovernance.\\nThe lines on the graph represent different aspects of AI ethics that \\nhave been identified as necessary. The blue line represents transparency, \\nwhich refers to the openness and clarity in AI policy communication. \\nThe green line represents accountability, which signifies the extent of \\nresponsibility in AI development and use. The red line represents fair\\xad',\n",
       "  'The green line represents accountability, which signifies the extent of \\nresponsibility in AI development and use. The red line represents fair\\xad\\nness, which denotes the importance of ensuring unbiased AI systems. \\nThe purple line represents privacy, which highlights the importance of \\nuser privacy and data protection. The orange line represents human \\noversight, which signifies humans’ involvement and oversight in AI \\nprocesses. The brown line represents ethical standards, which means \\nadherence to ethical guidelines in AI. The pink line represents innova\\xad\\ntion encouragement, which reflects support for innovative AI develop\\xad\\nment. The gray line represents national security, emphasizing AI’s role \\nin national security.\\nThese lines provide a comprehensive view of how different countries \\naddress multiple facets of AI ethics in their policies. They reflect a broader \\nrange of considerations in the global discourse on AI governance. By con\\xad',\n",
       "  'address multiple facets of AI ethics in their policies. They reflect a broader \\nrange of considerations in the global discourse on AI governance. By con\\xad\\nsidering the different aspects of AI ethics, policymakers can create fair, trans\\xad\\nparent, and accountable policies while encouraging innovation and protecting \\nuser privacy and data. This is essential to building trust in AI and ensuring that \\nit is used to benefit society.\\ne2463722-28\\nP. RADANLIEV',\n",
       "  'Strategies to Mitigate Bias\\nThis section expands into the issue of bias in Artificial Intelligence (AI) \\nsystems and its significant impact on fairness and effectiveness. We introduce \\nFigure 8, a network diagram visually representing various interconnected \\nstrategies crucial for mitigating bias. These include ensuring data diversity to \\navoid biased AI models, conducting regular audits to detect and correct biases, \\nemploying bias detection tools, and emphasizing the importance of algorith\\xad\\nmic transparency. In addition, we emphasize the importance of integrating \\ndiverse development teams and providing ethical AI training to reduce uncon\\xad\\nscious bias in AI design and development. The section then transitions to \\nFigure 10, which provides a more detailed network representation, highlight\\xad\\ning the synergies and dependencies among these strategies. This section \\ndemonstrates how a collective, multifaceted approach, comprising both tech\\xad',\n",
       "  'ing the synergies and dependencies among these strategies. This section \\ndemonstrates how a collective, multifaceted approach, comprising both tech\\xad\\nnical and organizational measures, is vital for developing AI systems that are \\nequitable, fair, and aligned with ethical standards. It is essential to note that \\nmitigating bias in AI is an ongoing process that requires continuous vigilance \\nand adaptation to evolving AI technologies and societal norms.\\nComprehensive Mitigation Strategies\\nBias in AI systems is of utmost importance, as it can significantly impact the \\nfairness and effectiveness of these technologies. This section reviews various \\nstrategies aimed at mitigating bias, presented as a network diagram showcas\\xad\\ning the interconnectedness and collective importance of these strategies.\\nThe network diagram encompasses a range of approaches, each linked to \\ndemonstrate how they complement and reinforce one another. Key strategies',\n",
       "  'The network diagram encompasses a range of approaches, each linked to \\ndemonstrate how they complement and reinforce one another. Key strategies \\ninclude ensuring data diversity and using datasets representing all relevant \\ndemographics to prevent biased AI models. Regular audits are crucial to \\nidentifying and addressing biases that may develop over time. Additionally, \\nthe network emphasizes the importance of using bias detection tools, which \\nemploy specialized algorithms to uncover and address biases in AI systems.\\nThe diagram in Figure 9 emphasizes the significance of collectively imple\\xad\\nmenting these strategies, indicating that the most effective approach to miti\\xad\\ngating bias involves a multifaceted effort. This includes technical solutions and \\norganizational and procedural measures to ensure that AI systems are devel\\xad\\noped and operated in a manner that minimizes bias and promotes fairness.\\nThe diagram in Figure 9, shows the connections and assigned weights',\n",
       "  'oped and operated in a manner that minimizes bias and promotes fairness.\\nThe diagram in Figure 9, shows the connections and assigned weights \\nbetween different strategies to mitigate bias in AI. The thickness of the lines \\ncorresponds to the strength of the connection, and the weights are labeled on \\nthe diagram. The strategies are arranged circularly to emphasize their inter\\xad\\nconnectedness and importance. By implementing them collectively, the risk of \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-29',\n",
       "  'Figure 9. The significance of a collective implementation of AI Strategies: connections and Weights \\nin Strategies to Mitigate Bias in Al (Colour-coded by Strength).\\nFigure 10. Strategies to Mitigate AI Bias.\\ne2463722-30\\nP. RADANLIEV',\n",
       "  'bias in AI systems can be significantly reduced, leading to more equitable and \\ntrustworthy AI solutions.\\nOne key strategy is to ensure data diversity and representation. This \\ninvolves using diverse data representing all relevant demographics to avoid \\nbiased models in AI systems. Regular audits are also essential to identify and \\nrectify any biases that may have crept in over time. Bias detection tools are \\nanother essential strategy. These tools utilize specialized software to detect \\nbiases in AI algorithms, which can then be corrected. Diverse development \\nteams can also help minimize unconscious biases in designing and developing \\nAI systems.\\nProviding ethical AI training to AI professionals is another way to mitigate \\nbias in AI. This training educates AI professionals on ethical considerations \\nand avoiding bias. Algorithmic transparency is also crucial to reducing bias in \\nAI. By making the workings of AI algorithms transparent, biases can be',\n",
       "  'and avoiding bias. Algorithmic transparency is also crucial to reducing bias in \\nAI. By making the workings of AI algorithms transparent, biases can be \\nidentified and corrected more quickly. Involving various stakeholders, includ\\xad\\ning those from underrepresented groups, to provide feedback on AI systems \\nand their outputs can also significantly reduce bias. Finally, continuously \\nmonitoring AI systems is essential to quickly identify and address any biases \\nthat may emerge over time. Collectively implementing these strategies can \\nsignificantly reduce the risk of bias in AI systems, leading to more equitable \\nand trustworthy AI solutions.\\nThe flowchart in Figure 10 outlines a network representation of strategies \\nfor mitigating bias in AI. The enhanced diagram provides more context and \\ndisplays the connections between these strategies, providing a comprehensive \\napproach to addressing this issue.\\nAs shown in Figure 9, data diversity is one of the primary strategies to',\n",
       "  'displays the connections between these strategies, providing a comprehensive \\napproach to addressing this issue.\\nAs shown in Figure 9, data diversity is one of the primary strategies to \\nmitigate bias in AI. It is essential to ensure that data is collected from diverse \\ndemographics. This strategy is linked to bias detection tools, emphasizing the \\nimportance of having a wide range of data to identify and correct biases. \\nRegular audits are another crucial component of mitigating bias in AI systems. \\nPeriodic reviews of AI systems for biases are required and are connected to \\ncontinuous monitoring. This highlights the need for ongoing assessments to \\nensure the AI system remains unbiased.\\nUsing bias detection tools is also essential in mitigating bias in AI systems. \\nThis strategy links to algorithm transparency, underscoring detection tools’ \\nrole in making AI decisions more straightforward. Ensuring that the AI',\n",
       "  'This strategy links to algorithm transparency, underscoring detection tools’ \\nrole in making AI decisions more straightforward. Ensuring that the AI \\nalgorithm is transparent makes it easier to identify and correct any potential \\nbiases.\\nDiverse teams are also vital to mitigating bias in AI systems. Having diverse \\nbackgrounds in development teams can significantly reduce unconscious \\nbiases. This strategy is connected to ethical training, showing the importance \\nof diverse perspectives in ethical AI development. This ensures that the AI \\nsystem is developed ethically and unbiasedly.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-31',\n",
       "  'Ethical training is another crucial strategy in mitigating bias in AI systems. \\nTraining on ethical AI development practices is vital, and this connects to \\nstakeholder feedback. This illustrates the role of ethical considerations in \\nincorporating diverse viewpoints, ensuring that the AI system is developed \\nwith the interests of all stakeholders in mind.\\nMaking AI algorithms’ decisions clear is another essential strategy for \\nidentifying biases. This is connected to regular audits, highlighting the need \\nfor transparency in ongoing assessments. Ensuring that the AI algorithm’s \\ndecisions are transparent makes it easier to identify and correct any potential \\nbiases.\\nIncorporating feedback from all groups, including underrepresented ones, \\nis essential in developing an unbiased AI system. This relates back to data \\ndiversity, emphasizing the role of inclusive feedback in ensuring diverse data \\nrepresentation. Diverse feedback and ongoing surveillance for emerging biases',\n",
       "  'diversity, emphasizing the role of inclusive feedback in ensuring diverse data \\nrepresentation. Diverse feedback and ongoing surveillance for emerging biases \\nis necessary to mitigate bias in AI systems. This relates to diverse teams, \\nunderscoring the need for continuous oversight by teams with varied back\\xad\\ngrounds and perspectives. Diverse teams make it easier to identify and correct \\npotential biases.\\nThe network diagram in Figure 9 illustrates the interconnected nature of \\nthese strategies, showing how each contributes to a comprehensive approach \\nto mitigating bias in AI systems. By implementing these strategies, AI systems \\ncan be developed more ethically and unbiasedly, with the interests of all \\nstakeholders in mind.\\nEthical Training and Diverse Teams\\nProviding ethical training to AI professionals is crucial to making them \\naware of potential biases and fostering an ethical culture in AI devel\\xad\\nopment. This training should cover the ethical implications of AI, the',\n",
       "  'aware of potential biases and fostering an ethical culture in AI devel\\xad\\nopment. This training should cover the ethical implications of AI, the \\nsignificance of diversity in datasets, and ways to detect and mitigate \\nbias.\\nForming diverse teams is also a vital strategy. Teams composed of people \\nfrom diverse backgrounds bring unique perspectives to the AI development \\nprocess, which can help identify biases that a more homogeneous group may \\noverlook. The diversity here refers to demographic factors and variations in \\nexpertise, experience, and viewpoints.\\nBy combining ethical training, diversity in teams, and technical strate\\xad\\ngies such as data diversity and regular audits, AI systems can be devel\\xad\\noped in a way that is more equitable, fair, and aligned with ethical \\nstandards. Countering bias is a continuous process that necessitates \\nongoing attention and adaptation as AI technologies progress.\\ne2463722-32\\nP. RADANLIEV',\n",
       "  'Emerging AI Technologies and Their Ethical Challenges\\nLarge language models (LLMs), such as GPT-3 and GPT-4, exemplify the \\ngrowing power of generative AI. These models are trained on vast datasets, \\noften scraping data from the internet, which raises significant concerns over \\ndata provenance, copyright infringement, and privacy violations. The opa\\xad\\nque nature of these models complicates efforts to ensure that they are free from \\nbiases present in the training data, such as discriminatory language, misinfor\\xad\\nmation, or unintentional perpetuation of harmful stereotypes. Despite \\nemploying fine-tuning and debiasing techniques, these models are still prone \\nto producing biased outputs due to the inherent limitations of the training \\ndata and the probabilistic nature of their generation processes. For instance, \\ntechniques such as reinforcement learning from human feedback (RLHF) \\nhave been deployed to mitigate harmful outputs, but they remain insufficient',\n",
       "  'techniques such as reinforcement learning from human feedback (RLHF) \\nhave been deployed to mitigate harmful outputs, but they remain insufficient \\nin addressing the deeper systemic biases embedded within the underlying \\ndatasets. This calls for more sophisticated techniques, such as adversarial \\ntraining, where adversarial examples are used to iteratively refine models \\nand expose hidden biases. Additionally, federated learning presents \\na promising approach for enhancing the ethical training of LLMs by allowing \\nmodels to learn from decentralized, anonymized data, thus reducing the \\nethical risks associated with data centralization and privacy violations.\\nAnother critical issue with LLMs lies in their ability to produce convincing \\nbut factually incorrect or hallucinatory outputs. This problem, often referred \\nto as the “hallucination problem,” presents ethical challenges in high-stakes \\ndomains such as healthcare or law, where accurate information is paramount.',\n",
       "  'to as the “hallucination problem,” presents ethical challenges in high-stakes \\ndomains such as healthcare or law, where accurate information is paramount. \\nCurrent mitigation strategies include truth-verification models that cross- \\ncheck generated content against verified databases and automated fact- \\nchecking systemsintegrated into the model’s inference pipeline. However, \\nthese methods are still evolving and are far from fully resolving the issue. \\nEthical frameworks for LLM deployment must, therefore, include rigorous \\npost-deployment monitoring and real-time validation mechanisms to ensure \\nthe integrity of the outputs, particularly in applications where misinformation \\ncould have profound societal impacts.\\nAutonomous systems, including autonomous vehicles, drones, and \\nrobotic systems, pose additional ethical challenges related to safety, \\naccountability, and decision-making autonomy. A key ethical dilemma \\narises in the context of autonomous decision-making in unpredictable',\n",
       "  'accountability, and decision-making autonomy. A key ethical dilemma \\narises in the context of autonomous decision-making in unpredictable \\nenvironments. For instance, in the case of autonomous vehicles, ethical \\nframeworks must account for the so-called trolley problem scenarios, \\nwhere the system must make life-and-death decisions in the event of an \\nunavoidable accident. Traditional rule-based ethical systems, such as \\ndeontological or utilitarian approaches, often fail to provide clear solu\\xad\\ntions in these nuanced scenarios. Consequently, emerging solutions \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-33',\n",
       "  'involve the use of ethical AI algorithms like multi-objective optimiza\\xad\\ntion, which allows systems to balance competing ethical principles – such \\nas minimizing harm and respecting human autonomy – by assigning \\ndynamic weights to different ethical outcomes based on real-time envir\\xad\\nonmental factors.\\nFurthermore, accountability in autonomous systems presents a unique \\nchallenge, especially in cases where systems operate with minimal human \\noversight. Explainable AI (XAI) plays a critical role here, enabling transpar\\xad\\nency in decision-making processes by providing interpretable insights into \\nhow the system reached a specific decision. Techniques such as attention \\nmechanisms and saliency maps can be employed to highlight the features that \\nmost influenced an autonomous system’s decision, making it easier for reg\\xad\\nulators and auditors to understand and assess the fairness and safety of these \\ndecisions. However, the effectiveness of XAI in highly complex, real-time',\n",
       "  'ulators and auditors to understand and assess the fairness and safety of these \\ndecisions. However, the effectiveness of XAI in highly complex, real-time \\nautonomous systems remains limited, necessitating the development of causal \\ninference models that can provide a more comprehensive understanding of \\ndecision-making pathways and their underlying ethical implications.\\nThe issue of algorithmic accountability in autonomous weapons systems \\n(AWS) presents perhaps the most acute ethical challenge. The development \\nand deployment of AWS raise profound concerns over autonomous lethality \\n—the ability of a system to make life-or-death decisions without human \\nintervention. Current discussions on international AI governance focus on \\nthe need to restrict the deployment of AWS through legally binding treaties, \\nbut enforcement mechanisms remain elusive. From a technical standpoint, \\none proposed solution involves embedding human-in-the-loop (HITL)',\n",
       "  'but enforcement mechanisms remain elusive. From a technical standpoint, \\none proposed solution involves embedding human-in-the-loop (HITL) \\nmechanisms that ensure critical decisions, particularly those involving the \\nuse of lethal force, require human validation before execution. This integration \\nof human oversight into decision-making processes is critical to preventing \\nunintended harm and ensuring compliance with international humanitarian \\nlaw. Additionally, ongoing research into ethical-by-design architectures aims \\nto build ethical constraints directly into the system’s operational framework, \\nlimiting the scope of actions that an autonomous system can take based on \\npredefined ethical guidelines.\\nFinally, the deployment of swarm intelligence in autonomous drones and \\nrobots introduces challenges related to collective decision-making and dis\\xad\\ntributed accountability. In swarm systems, decisions are often made collec\\xad',\n",
       "  'robots introduces challenges related to collective decision-making and dis\\xad\\ntributed accountability. In swarm systems, decisions are often made collec\\xad\\ntively by a distributed group of agents, with no single agent being responsible \\nfor the final outcome. This creates significant ethical ambiguity in determining \\naccountability when swarm systems malfunction or cause harm. Solutions \\nsuch as distributed ledger technologies (DLT), including blockchain, have \\nbeen proposed to ensure that every decision made within the swarm is \\nrecorded in a transparent and immutable way, providing a traceable log of \\nactions that can be audited for accountability purposes.\\ne2463722-34\\nP. RADANLIEV',\n",
       "  'Discussion\\nThe introduction of fairness, transparency, and accountability into AI systems, \\nwhile crucial for ensuring ethical standards, introduces a significant financial \\nburden and operational complexity, especially in sectors where fast innovation \\nis a competitive necessity.\\nOne of the primary economic costs arises from the increased complexity in \\ndeveloping AI systems that adhere to ethical guidelines. Implementing fair\\xad\\nness-aware learning algorithms, such as demographic parity or equalized odds, \\nrequires additional computational resources and extensive testing during the \\ntraining phase. These fairness constraints are not simply add-ons but require \\na fundamental rethinking of the algorithmic design, particularly in cases where \\nperformance optimization conflicts with fairness. For instance, in financial \\nservices, ensuring that loan approval algorithms do not exhibit bias may \\nnecessitate retraining models with diverse datasets and applying fairness con\\xad',\n",
       "  'services, ensuring that loan approval algorithms do not exhibit bias may \\nnecessitate retraining models with diverse datasets and applying fairness con\\xad\\nstraints throughout the development cycle. This extended development pro\\xad\\ncess incurs higher labor costs, requires greater infrastructure investment, and \\noften results in longer timeframes to achieve regulatory compliance. \\nAdditionally, privacy-preserving techniques, such as differential privacy and \\nfederated learning, add further complexity. Federated learning, which enables \\nmodel training across distributed datasets without centralizing sensitive data, \\nrequires more sophisticated system architectures and secure communication \\nchannels, increasing both the cost and technical difficulty of implementation.\\nOperationally, the impact of strict ethical guidelines is felt through the need \\nfor ongoing compliance and continuous monitoring of AI systems. Ethical \\nframeworks such as the European Union’s AI Act mandate that high-risk AI',\n",
       "  'for ongoing compliance and continuous monitoring of AI systems. Ethical \\nframeworks such as the European Union’s AI Act mandate that high-risk AI \\napplications, particularly in fields like healthcare and criminal justice, undergo \\ncontinuous auditing to ensure ethical standards are maintained post- \\ndeployment. These operational costs are amplified by the need to integrate \\nreal-time fairness monitoring tools, such as AI Fairness 360, which check for \\nbias drift or decision-making anomalies as AI systems encounter new data. \\nThese tools require continuous computational resources, infrastructure sup\\xad\\nport, and personnel dedicated to auditing and model recalibration. For indus\\xad\\ntries such as financial services, where AI systems are deployed in real-time \\nenvironments like high-frequency trading, maintaining fairness and compli\\xad\\nance adds layers of complexity to the operational workflow. This constant \\nneed for recalibration can also result in downtime, during which systems must',\n",
       "  'ance adds layers of complexity to the operational workflow. This constant \\nneed for recalibration can also result in downtime, during which systems must \\nbe reevaluated and updated, leading to delays in decision-making processes \\nand potential disruptions to business continuity.\\nThe financial impact of these ethical requirements also affects innovation \\ncycles and speed to market. In highly competitive sectors like autonomous \\ndriving or AI-driven diagnostics, time-to-market is often crucial for gaining \\na first-mover advantage. Companies that invest heavily in ethical compliance – \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-35',\n",
       "  'such as model transparency, fairness audits, and explainability – may experi\\xad\\nence delays in bringing products to market. For instance, the requirement to \\nintegrate explainability mechanisms, such as SHAP (Shapley Additive \\nExplanations) or LIME (Local Interpretable Model-agnostic Explanations), \\ninto AI models often necessitates additional development and testing phases. \\nThis extends the overall project timeline and may place companies at \\na competitive disadvantage against those who prioritize rapid deployment \\nover ethical oversight. The delay not only impacts short-term revenue but \\nalso affects long-term strategic positioning, particularly in industries where \\ntechnological leadership is key to maintaining market share.\\nBeyond development and operational costs, legal compliance and regula\\xad\\ntory risk are significant financial considerations for companies implementing \\nstrict ethical guidelines. Regulatory frameworks like the GDPR and the',\n",
       "  'tory risk are significant financial considerations for companies implementing \\nstrict ethical guidelines. Regulatory frameworks like the GDPR and the \\nupcoming EU AI Act impose severe penalties for noncompliance, with fines \\nthat can reach up to 4% of a company’s global revenue for violations of data \\nprivacy and transparency requirements. To mitigate these risks, companies \\noften need to invest heavily in legal teams, external audits, and compliance \\ninfrastructures. This introduces an additional cost layer as companies must \\nallocate resources not just for initial development but also for ongoing com\\xad\\npliance management. The cyclical nature of compliance – where systems must \\nbe continuously updated, audited, and re-certified to meet evolving stan\\xad\\ndards – creates long-term financial commitments that extend well beyond \\nthe initial implementation of AI systems.\\nDespite these costs, emerging technologies offer potential solutions',\n",
       "  'dards – creates long-term financial commitments that extend well beyond \\nthe initial implementation of AI systems.\\nDespite these costs, emerging technologies offer potential solutions \\nthat could mitigate some of the financial and operational burdens \\nassociated with ethical AI. Automated machine learning (AutoML) sys\\xad\\ntems are increasingly capable of incorporating fairness and transparency \\nchecks into their development pipelines, reducing the need for manual \\nintervention and thus lowering labor costs. Additionally, distributed \\nledger technologies (DLT), such as blockchain, can help track AI deci\\xad\\nsions in a transparent and immutable way, thereby simplifying post- \\ndeployment audits and reducing the cost of maintaining ethical stan\\xad\\ndards. Nevertheless, while these technologies offer some relief, they \\ncome with their own set of technical challenges and infrastructural \\ncosts, which require additional investment and expertise to implement \\neffectively.',\n",
       "  'come with their own set of technical challenges and infrastructural \\ncosts, which require additional investment and expertise to implement \\neffectively.\\nThe implementation of strict ethical guidelines in AI development signifi\\xad\\ncantly impacts economic and operational aspects of AI projects. While these \\nguidelines are crucial for ensuring fairness, transparency, and accountability, \\nthey introduce substantial costs at every stage of the AI lifecycle, from devel\\xad\\nopment through to post-deployment monitoring and compliance. Balancing \\nthese ethical obligations with the need for innovation and market competi\\xad\\ntiveness remains a challenge, particularly for companies operating in highly \\ne2463722-36\\nP. RADANLIEV',\n",
       "  'dynamic and competitive sectors. The evolving landscape of AI governance, \\ncoupled with emerging cost-saving technologies, will be critical in determining \\nhow companies navigate the financial and operational implications of ethical \\nAI development.\\nConclusion\\nThis study has undertaken an examination of the ethical imperatives sur\\xad\\nrounding AI, particularly the principles of transparency, fairness, and privacy, \\nin the context of its prevalent influence across sectors such as healthcare, \\nfinance, and communication. The deployment of AI technologies in these \\ndomains brings with it profound ethical challenges that necessitate a strong \\nand inclusive framework to safeguard individual rights and societal interests. \\nThrough a comparative analysis of international AI policy frameworks from \\nthe European Union, the United States, and China, this research has clarified \\nthe conflicting ethical priorities that shape AI governance globally.',\n",
       "  'the European Union, the United States, and China, this research has clarified \\nthe conflicting ethical priorities that shape AI governance globally.\\nThis work clarifies the ethical principles of privacy, transparency, and fair\\xad\\nness, addressing regional challenges and interdependencies. By distinguishing \\nhow these principles operate independently yet interactively across frame\\xad\\nworks, the paper offers a refined conceptual foundation necessary for global \\ngovernance. A primary contribution is the proposed set of integration criteria \\n(Interoperability, Normative Cohesion, Cultural Adaptability, and \\nTransparency of Process). These criteria provide a structured foundation for \\naligning ethical principles across diverse international frameworks, supporting \\ncross-border AI compatibility while respecting region-specific values and \\nregulatory approaches. The graphical representations represent the individual \\nand corelated interdependencies and conflicts among frameworks, avoiding',\n",
       "  'regulatory approaches. The graphical representations represent the individual \\nand corelated interdependencies and conflicts among frameworks, avoiding \\noversimplification and enhancing analytical clarity. This provides a visual tool \\nfor understanding ethical elationships in global AI governance.\\nThe analysis reveals marked variations in how different regions balance the \\ndemands of innovation against the ethical principles of privacy, fairness, and \\naccountability. While certain jurisdictions, such as the European Union, \\nemphasize stringent regulatory oversight and data protection, others, includ\\xad\\ning the United States, adopt a more flexible, innovation-centric approach. \\nThese divergences underscore the complexities involved in striving for \\na harmonized global standard for ethical AI governance. Nevertheless, this \\nstudy has articulated several strategic interventions to mitigate algorithmic \\nbias, including the deployment of fairness-aware algorithms, regular audits,',\n",
       "  'study has articulated several strategic interventions to mitigate algorithmic \\nbias, including the deployment of fairness-aware algorithms, regular audits, \\nand the incorporation of diverse development teams. These interventions are \\nessential in fostering equitable and trustworthy AI systems.\\nFurthermore, this research has highlighted the critical need for sustained \\ninternational collaboration and dialogue to bridge the gaps in global AI ethics \\nframeworks. It is increasingly evident that no single jurisdiction can fully \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-37',\n",
       "  'address the multi-faceted ethical challenges posed by AI in isolation. Instead, \\nthe path forward demands a concerted, cooperative effort that leverages shared \\nprinciples while respecting regional variations in regulatory and cultural \\npriorities.\\nThis study advances the argument that ethical considerations must be \\nembedded at every stage of the AI development lifecycle, from inception \\nthrough to deployment and beyond. The recommendations herein aim to \\ninform policymakers, regulators, and AI developers, encouraging the pursuit \\nof AI systems that are not only innovative and technologically advanced but \\nalso aligned with the highest ethical standards. As AI continues to evolve and \\nexert its transformative potential, the need for vigilance, adaptability, and \\ncross-border cooperation remains paramount in ensuring that these technol\\xad\\nogies serve the common good, promoting fairness, accountability, and trust in \\ntheir application.\\nDisclosure Statement',\n",
       "  'cross-border cooperation remains paramount in ensuring that these technol\\xad\\nogies serve the common good, promoting fairness, accountability, and trust in \\ntheir application.\\nDisclosure Statement\\nNo potential conflict of interest was reported by the author(s).\\nFunding\\nThe work was supported by the Engineering and Physical Sciences Research Council [EP/ \\nS035362/1].\\nORCID\\nPetar Radanliev \\nhttp://orcid.org/0000-0001-5629-6857\\nData Availability Statement\\nThe datasets generated and analyzed during the current study are available from the corre\\xad\\nsponding author upon reasonable request. Due to the sensitive nature of the data related to AI \\nethics and privacy considerations, access to the data may be restricted. Specific details regard\\xad\\ning the data sources, including international AI policy frameworks from the EU, US, China, \\nCanada, Japan, India, and Australia, are documented within the study. All data shared will be',\n",
       "  'ing the data sources, including international AI policy frameworks from the EU, US, China, \\nCanada, Japan, India, and Australia, are documented within the study. All data shared will be \\ncompliant with ethical guidelines and privacy standards as outlined in the General Data \\nProtection Regulation (GDPR) and other relevant data protection laws.\\nReferences\\nAldoseri, A., K. N. Al-Khalifa, and A. M. Hamouda. 2023. Re-Thinking data strategy and \\nintegration for artificial intelligence: Concepts, opportunities, and challenges. Applied \\nSciences 13 (12):7082. doi: 10.3390/APP13127082  .\\ne2463722-38\\nP. RADANLIEV',\n",
       "  'Bécue, A., I. Praça, and J. Gama. 2021. Artificial intelligence, cyber-threats and industry 4.0: \\nChallenges and opportunities. Artificial Intelligence Review 54 (5):3849–86. doi: 10.1007/ \\ns10462-020-09942-2  .\\nBender, E. M., T. Gebru, A. McMillan-Major, and S. Shmitchell. 2021. On the dangers of \\nstochastic parrots: Can language models be too big? FAccT 2021 - Proceedings of the 2021 \\nACM Conference on Fairness, Accountability, and Transparency, 610–23. doi: 10.1145/ \\n3442188.3445922  .\\nBinns, R. 2018. Fairness in machine learning: Lessons from political philosophy. Proceedings of \\nMachine Learning Research, vol. 81, 149–59, PMLR. https://proceedings.mlr.press/v81/ \\nbinns18a.html .\\nBommasani, R., K. Klyman, D. Zhang, and P. Liang. 2023. Do foundation model providers \\ncomply with the draft EU AI act? Center for Research on Foundation Models (CRFM): \\nStanford Center for Research on Foundation Models.',\n",
       "  'comply with the draft EU AI act? Center for Research on Foundation Models (CRFM): \\nStanford Center for Research on Foundation Models.\\nBostrom, N., and E. Yudkowsky. 2014. The ethics of artificial intelligence. The Cambridge \\nHandbook of Artificial Intelligence 316–34. doi: 10.1017/CBO9781139046855.020  .\\nBrynjolfsson, E., and A. Mcafee. 2014. The second machine age: Work, progress, and prosperity \\nin a time of brilliant technologies. In The second machine age: Work, progress, and prosper\\xad\\nity in a time of brilliant technologies. Worldwide: W.W. Norton & Company 978-0-393- \\n35064-7 https://wwnorton.com/books/the-second-machine-age/ .\\nde Bruin, B., and L. Floridi. 2017. The ethics of cloud computing. Science and Engineering \\nEthics 23 (1):21–39. doi: 10.1007/s11948-016-9759-0  .\\nde Fine Licht, K., and J. de Fine Licht. 2020. Artificial intelligence, transparency, and public \\ndecision-making. AI & Society 35 (4):917–26. doi: 10.1007/s00146-020-00960-w  .',\n",
       "  'de Fine Licht, K., and J. de Fine Licht. 2020. Artificial intelligence, transparency, and public \\ndecision-making. AI & Society 35 (4):917–26. doi: 10.1007/s00146-020-00960-w  .\\nDu, M. 2023. Awesome-Fairness-in-AI. GitHub Repository. https://github.com/datamllab/awe \\nsome-fairness-in-ai .\\nEuropean Parliament. 2023. AI act: A step closer to the first rules on artificial intelligence | news | \\nEuropean Parliament. https://www.europarl.europa.eu/news/en/press-room \\n/20230505IPR84904/ai-act-a-step-closer-to-the-first-rules-on-artificial-intelligence .\\nEvans, K. 2015. The second machine age: Work, progress, and prosperity in a time of brilliant \\ntechnologies by eric Brynjolfsson and Andrew McAfee. Journal of Business & Finance \\nLibrarianship 20 (3):244–46. doi: 10.1080/08963568.2015.1044355  .\\nFACT SHEET: Biden-Harris Administration Announces New Actions to Promote Responsible \\nAI Innovation That Protects Americans’ Rights and Safety | The White House. 2023. https://',\n",
       "  'FACT SHEET: Biden-Harris Administration Announces New Actions to Promote Responsible \\nAI Innovation That Protects Americans’ Rights and Safety | The White House. 2023. https:// \\nwww.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden- \\nharris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that- \\nprotects-americans-rights-and-safety/ .\\nFloridi, L., J. Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge, \\nR. Madelin, U. Pagallo, F. Rossi, et al. 2018. AI4People—an ethical framework for a good \\nAI society: Opportunities, risks, principles, and recommendations. Minds and Machines \\n28 (4):689–707. doi: 10.1007/s11023-018-9482-5  .\\nGDPR. 2018. What is GDPR, the EU’s new data protection law? - Gdpr.Eu. https://gdpr.eu/ \\nwhat-is-gdpr/ .\\nHelbing, D., B. S. Frey, G. Gigerenzer, E. Hafen, M. Hagner, Y. Hofstetter, J. Van Den Hoven,',\n",
       "  'GDPR. 2018. What is GDPR, the EU’s new data protection law? - Gdpr.Eu. https://gdpr.eu/ \\nwhat-is-gdpr/ .\\nHelbing, D., B. S. Frey, G. Gigerenzer, E. Hafen, M. Hagner, Y. Hofstetter, J. Van Den Hoven, \\nR. V. Zicari, and A. Zwitter. 2018. Will democracy survive big data and artificial intelligence? \\nIn Towards digital enlightenment: Essays on the dark and light sides of the digital revolu\\xad\\ntion, 73–98. Springer International Publishing. doi: 10.1007/978-3-319-90869-4_7  .\\nHIPAA. 1996. Health insurance portability and accountability act of 1996 (HIPAA) | CDC. \\nhttps://www.cdc.gov/phlp/publications/topic/hipaa.html .\\nHosny, A., C. Parmar, J. Quackenbush, L. H. Schwartz, and H. J. W. L. Aerts. 2018. Artificial \\nintelligence in radiology. Nature Rev Cancer 18 (8):500. doi: 10.1038/S41568-018-0016-5  .\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-39',\n",
       "  'IBM. 2018. AI fairness 360 – open source. Open Project. https://www.ibm.com/opensource/ \\nopen/projects/ai-fairness-360/ .\\nICO. 2018. Information commissioner’s office (ICO): The UK GDPR. UK GDPR Guidance and \\nResources. https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful- \\nbasis/a-guide-to-lawful-basis/lawful-basis-for-processing/consent/ .\\nInterim Measures for the Management of Generative Artificial Intelligence Services, Personal \\nInformation Protection Law of the People’s Republic of China (PRC). 2023.\\nISO. 2023. ISO/IEC DIS 42001 - information technology — artificial intelligence — management \\nsystem. https://www.iso.org/standard/81230.html .\\nJobin, A., M. Ienca, and E. Vayena. 2019. The global landscape of AI ethics guidelines. Nature \\nMachine Intelligence 2019 1 (9):389–99. doi: 10.1038/s42256-019-0088-2  .\\nLi, L. 2017. China’s manufacturing locus in 2025: With a comparison of “Made-in-China 2025”',\n",
       "  'Machine Intelligence 2019 1 (9):389–99. doi: 10.1038/s42256-019-0088-2  .\\nLi, L. 2017. China’s manufacturing locus in 2025: With a comparison of “Made-in-China 2025” \\nand “Industry 4.0”. Technological Forecasting & Social Change 135:66–74. doi: 10.1016/J. \\nTECHFORE.2017.05.028  .\\nMalhotra, Y. 2018. Cognitive computing for anticipatory risk analytics in intelligence, surveil\\xad\\nlance, & reconnaissance (ISR): Model risk management in artificial intelligence & machine \\nlearning (presentation slides). SSRN Electronic Journal. doi: 10.2139/ssrn.3111837  .\\nMcCorduck, P., and C. Cfe. 2004. Machines who think: A personal inquiry into the history and \\nprospects of artificial intelligence. CRC Press. https://books.google.com/books?hl=en&lr= \\n&id=r2C1DwAAQBAJ&oi=fnd&pg=PP1&dq=Pamela+McCorduck+%22Machines+Who \\n+Think&ots=UnmXIiuRtM&sig=JAh90Eu07MGvjS5OgFq1CMy6-gc .\\nMeissner, G. 2020. Artificial intelligence: Consciousness and conscience. AI & Society',\n",
       "  '+Think&ots=UnmXIiuRtM&sig=JAh90Eu07MGvjS5OgFq1CMy6-gc .\\nMeissner, G. 2020. Artificial intelligence: Consciousness and conscience. AI & Society \\n35 (1):225–35. doi: 10.1007/s00146-019-00880-4  .\\nMeitY. 2023. Artificial intelligence committees reports | Ministry of electronics and informa\\xad\\ntion technology, Government of India. Artificial Intelligence Committees Report. https:// \\nwww.meity.gov.in/artificial-intelligence-committees-reports .\\nMijwil, M. M., M. Aljanabi, and ChatGPT. 2023. Towards artificial intelligence-based cyberse\\xad\\ncurity: The practices and ChatGPT generated ways to combat cybercrime. Iraqi Journal for \\nComputer Science and Mathematics 4 (1):65–70. doi: 10.52866/IJCSM.2023.01.01.0019  .\\nMittelstadt, B. 2019. Principles alone cannot guarantee ethical AI. Nature Machine Intelligence \\n1 (11):501–07. doi: 10.1038/s42256-019-0114-4  .\\nMozumder, M. A. I., M. M. Sheeraz, A. Athar, S. Aich, and H.-C. Kim. 2022. Overview:',\n",
       "  '1 (11):501–07. doi: 10.1038/s42256-019-0114-4  .\\nMozumder, M. A. I., M. M. Sheeraz, A. Athar, S. Aich, and H.-C. Kim. 2022. Overview: \\nTechnology roadmap of the future trend of metaverse based on IoT, blockchain, AI \\ntechnique, and medical domain metaverse activity. International Conference on Advanced \\nCommunication Technology (ICACT) 256–61. doi: 10.23919/ICACT53585.2022.9728808  .\\nNAIAC. 2024. AI safety: National AI advisory committee. https://ai.gov/wp-content/uploads/ \\n2024/06/FINDINGS-RECOMMENDATIONS_AI-Safety.pdf .\\nNIST. 2023a. AI risk management framework | NIST. National Institute of Standards and \\nTechnology. https://www.nist.gov/itl/ai-risk-management-framework .\\nNIST. 2023b. Artificial intelligence | NIST. https://www.nist.gov/artificial-intelligence .\\nNIST. 2024a. AI risk management framework | NIST.\\nNIST. 2024b. AI standards | NIST. https://www.nist.gov/artificial-intelligence/ai-standards .',\n",
       "  'NIST. 2024a. AI risk management framework | NIST.\\nNIST. 2024b. AI standards | NIST. https://www.nist.gov/artificial-intelligence/ai-standards .\\nNIST. 2024c. Department of commerce announces new guidance, tools 270 days following \\npresident Biden’s executive order on AI | NIST. https://www.nist.gov/news-events/news/ \\n2024/07/department-commerce-announces-new-guidance-tools-270-days-following .\\nOffice for Artificial Intelligence and Department for Science, Innovation & Technology. 2023. \\nA pro-innovation approach to AI regulation 978-1-5286-4009-1 (London: Crown copyright).\\nPartnership on AI. 2023. Partnership on AI and the ethical AI framework for social good. https:// \\npartnershiponai.org/ .\\ne2463722-40\\nP. RADANLIEV',\n",
       "  'Provisions on the Administration of Deep Synthesis Internet Information Services, Personal \\nInformation Protection Law of the People’s Republic of China (PRC). 2022.\\nRoberts, H., J. Cowls, J. Morley, M. Taddeo, V. Wang, and L. Floridi. 2021. The Chinese \\napproach to artificial intelligence: An analysis of policy, ethics, and regulation. AI & Society \\n36 (1):59–77. doi: 10.1007/s00146-020-00992-2  .\\nShu, Y., J. Zhang, and H. Yu. 2021. Fairness in design: A tool for guidance in ethical artificial \\nintelligence design. Lecture Notes in Computer Science (Including Subseries Lecture Notes in \\nArtificial Intelligence and Lecture Notes in Bioinformatics) 12774:500–10. doi: 10.1007/978- \\n3-030-77626-8_34  .\\nSinger, P. W. 2009. Wired for war: The robotics revolution and conflict in the twenty-first \\ncentury, 499. https://books.google.com/books/about/Wired_for_War.html?id= \\nAJuowQmtbU4C .\\nThe State Council People Republic of China. 2017. Made in China 2025; the state council people',\n",
       "  'century, 499. https://books.google.com/books/about/Wired_for_War.html?id= \\nAJuowQmtbU4C .\\nThe State Council People Republic of China. 2017. Made in China 2025; the state council people \\nRepublic of China. http://english.gov.cn/2016special/madeinchina2025/ .\\nTabassi, E. 2023. AI risk management framework | NIST. doi: 10.6028/NIST.AI.100-1  .\\nTurilli, M., and L. Floridi. 2009. The ethics of information transparency. Ethics and Information \\nTechnology 11 (2):105–12. doi: 10.1007/s10676-009-9187-9  .\\nUNESCO. 2023. Recommendation on the ethics of artificial intelligence | UNESCO. https://www. \\nunesco.org/en/articles/recommendation-ethics-artificial-intelligence .\\nWachter, S., B. Mittelstadt, and C. Russell. 2023. Health care bias is dangerous. But so are \\nfairness’ algorithms | WIRED. Wired. https://www.wired.com/story/bias-statistics-artificial- \\nintelligence-healthcare/ .\\nYu, K. H., A. L. Beam, and I. S. Kohane. 2018. Artificial intelligence in healthcare. Nature',\n",
       "  'intelligence-healthcare/ .\\nYu, K. H., A. L. Beam, and I. S. Kohane. 2018. Artificial intelligence in healthcare. Nature \\nBiomedical Engineering 2 (10):719–31. doi: 10.1038/S41551-018-0305-Z.\\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-41',\n",
       "  'IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\n799\\nAn Overview of Artiﬁcial Intelligence Ethics\\nChangwu Huang\\n, Member, IEEE, Zeqi Zhang, Bifei Mao, and Xin Yao\\n, Fellow, IEEE\\nAbstract—Artiﬁcial intelligence (AI) has profoundly changed\\nand will continue to change our lives. AI is being applied in more\\nand more ﬁelds and scenarios such as autonomous driving, med-\\nical care, media, ﬁnance, industrial robots, and internet services.\\nThe widespread application of AI and its deep integration with\\nthe economy and society have improved efﬁciency and produced\\nbeneﬁts. At the same time, it will inevitably impact the existing\\nsocial order and raise ethical concerns. Ethical issues, such as\\nprivacy leakage, discrimination, unemployment, and security risks,\\nbrought about by AI systems have caused great trouble to people.\\nTherefore, AI ethics, which is a ﬁeld related to the study of ethical\\nissues in AI, has become not only an important research topic',\n",
       "  'brought about by AI systems have caused great trouble to people.\\nTherefore, AI ethics, which is a ﬁeld related to the study of ethical\\nissues in AI, has become not only an important research topic\\nin academia, but also an important topic of common concern for\\nindividuals, organizations, countries, and society. This article will\\ngive a comprehensive overview of this ﬁeld by summarizing and\\nanalyzingtheethicalrisksandissuesraisedbyAI,ethicalguidelines\\nand principles issued by different organizations, approaches for\\naddressing ethical issues in AI, and methods for evaluating the\\nethics of AI. Additionally, challenges in implementing ethics in\\nAI and some future perspectives are pointed out. We hope our\\nwork will provide a systematic and comprehensive overview of AI\\nethics for researchers and practitioners in this ﬁeld, especially the\\nbeginners of this research discipline.\\nImpact Statement—AI ethics is an important emerging topic',\n",
       "  'ethics for researchers and practitioners in this ﬁeld, especially the\\nbeginners of this research discipline.\\nImpact Statement—AI ethics is an important emerging topic\\namong academia, industry, government, society, and individuals. In\\nthe past decades, many efforts have been made to study the ethical\\nissues in AI. This article offers a comprehensive overview of the AI\\nethics ﬁeld, including a summary and analysis of AI ethical issues,\\nManuscript received 5 September 2021; revised 22 February 2022 and 3\\nMay 2022; accepted 23 July 2022. Date of publication 28 July 2022; date\\nof current version 21 July 2023. This work was supported in part by the\\nResearch Institute of Trustworthy Autonomous Systems (RITAS), in part by\\nthe Guangdong Provincial Key Laboratory under Grant 2020B121201001, in\\npart by the Program for Guangdong Introducing Innovative and Enterpreneurial\\nTeams under Grant 2017ZT07X386, in part by Shenzhen Science and Tech-',\n",
       "  'part by the Program for Guangdong Introducing Innovative and Enterpreneurial\\nTeams under Grant 2017ZT07X386, in part by Shenzhen Science and Tech-\\nnology Program under Grant KQTD2016112514355531, and in part by a joint\\nproject between Huawei and Southern University of Science and Technology\\nunder Project FA2019061021. This paper was recommended for publication\\nby Associate Editor J. Torresen upon evaluation of the reviewers’ comments.\\n(Corresponding author: Xin Yao.)\\nChangwu Huang is with the Research Institute of Trustworthy Autonomous\\nSystems, Southern University of Science and Technology, Shenzhen 518055,\\nChina, and also with the Guangdong Provincial Key Laboratory of Brain-\\ninspired Intelligent Computation, Department of Computer Science and En-\\ngineering, Southern University of Science and Technology, Shenzhen 518055,\\nChina (e-mail: huangcw3@sustech.edu.cn).\\nZeqi Zhang and Bifei Mao are with the Trustworthiness Theory Research',\n",
       "  'gineering, Southern University of Science and Technology, Shenzhen 518055,\\nChina (e-mail: huangcw3@sustech.edu.cn).\\nZeqi Zhang and Bifei Mao are with the Trustworthiness Theory Research\\nCenter, Huawei Technologies Company, Ltd., Shenzhen 518055, China (e-mail:\\nzhangzeqi@huawei.com; maobifei@huawei.com).\\nXin Yao is with the Research Institute of Trustworthy Autonomous Systems,\\nSouthern University of Science and Technology, Shenzhen 518055, China, with\\nGuangdong Provincial Key Laboratory of Brain-inspired Intelligent Computa-\\ntion, Department of Computer Science and Engineering, Southern University\\nof Science and Technology, Shenzhen 518055, China, and also with the School\\nof Computer Science, University of Birmingham, B15 2TT Birmingham, U.K.\\n(e-mail: xiny@sustech.edu.cn).\\nThis\\narticle\\nhas\\nsupplementary\\ndownloadable\\nmaterial\\navailable\\nat\\nhttps://doi.org/10.1109/TAI.2022.3194503, provided by the authors.\\nDigital Object Identiﬁer 10.1109/TAI.2022.3194503',\n",
       "  'This\\narticle\\nhas\\nsupplementary\\ndownloadable\\nmaterial\\navailable\\nat\\nhttps://doi.org/10.1109/TAI.2022.3194503, provided by the authors.\\nDigital Object Identiﬁer 10.1109/TAI.2022.3194503\\nethical guidelines and principles, approaches to address AI ethical\\nissues, and methods to evaluate the ethics of AI technologies. Addi-\\ntionally, research challenges and future perspectives are discussed.\\nThis article will help researchers to gain a birds eye view of AI\\nethics, and thus facilitate their further investigation and research\\nof AI.\\nIndex Terms—Artiﬁcial intelligence (AI), AI ethics, ethical issue,\\nethical theory, ethical principle.\\nI. INTRODUCTION\\nA\\nRTIFICIAL intelligence (AI) [1] has achieved rapid and\\nremarkable development during the last decade. AI tech-\\nnologies such as machine learning (ML), natural language pro-\\ncessing, and computer vision are increasingly permeating and\\nspreading to various disciplines and aspects of our society. AI',\n",
       "  'nologies such as machine learning (ML), natural language pro-\\ncessing, and computer vision are increasingly permeating and\\nspreading to various disciplines and aspects of our society. AI\\nis increasingly taking over human tasks and replacing human\\ndecision-making. It has been widely used in a variety of sectors,\\nsuch as business, logistics, manufacturing, transportation, health\\ncare, education, state governance, etc.\\nThe application of AI has brought about efﬁciency improve-\\nment and cost reduction, which are beneﬁcial for economic\\ngrowth, social development, and human well-being [2]. For\\ninstance, the AI chatbot can respond to clients’ inquiries at\\nany time, which will improve the customers’ satisfaction and\\nthe company’s sales [3]. AI allows doctors to serve patients in\\nremote locations through telemedicine services [4]. It is no doubt\\nthattherapiddevelopmentandwideapplicationofAIarealready\\naffecting our daily life, humanity, and society.',\n",
       "  'remote locations through telemedicine services [4]. It is no doubt\\nthattherapiddevelopmentandwideapplicationofAIarealready\\naffecting our daily life, humanity, and society.\\nHowever, at the same time, AI also poses many signiﬁcant\\nethical risks or issues for users, developers, humans, and society.\\nOver the past few years, many cases in which AI produced\\npoor outcomes have been observed. For instance, in 2016, the\\ndriver of an electric Tesla car was killed in a road accident after\\nits Autopilot mode failed to recognize an oncoming lorry [5].\\nMicrosoft’s AI chatting bot, Tay.ai,was taken down because it\\nbecame racist and sexist only less than a day after she joined\\nTwitter [6]. There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7]. More seriously, AI technology has begun to be\\nused by criminals to harm others or the society. For example,\\ncriminals used AI-based software to impersonate a chief exec-',\n",
       "  'systems [7]. More seriously, AI technology has begun to be\\nused by criminals to harm others or the society. For example,\\ncriminals used AI-based software to impersonate a chief exec-\\nutive’s voice and demand a fraudulent transfer of $243 000 [8].\\nTherefore, it is urgent and critical to address the ethical issues\\nor risks of AI so that AI can be built, applied, and developed\\nethically.\\nAI ethics or machine ethics [9] is an emerging and interdisci-\\nplinary ﬁeld concerned with addressing ethical issues of AI [10].\\nAI ethics involves the ethics of AI, which studies the ethical\\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/',\n",
       "  '800\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\ntheories, guidelines, policies, principles, rules, and regulations\\nrelated to AI, and the ethical AI, that is, the AI that can uphold\\nethical norms and behaves ethically [11]. The ethics of AI is a\\nprerequisite to building ethical AI or to making AI behave in\\nan ethical manner. It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong. With\\nappropriate ethics of AI, ethical AI can be built or implemented\\nthrough some methodologies and technologies.\\nEven though AI ethics has been extensively discussed by\\ninterdisciplinary researchers for several years, it is still in its\\ninfancy [11]. AI ethics is a very broad and rapidly develop-\\ning research area that has received increasing attention from\\nresearchers in recent years. Although several review papers\\nhave been published during the past few years, each of them',\n",
       "  'ing research area that has received increasing attention from\\nresearchers in recent years. Although several review papers\\nhave been published during the past few years, each of them\\nfocuses on a certain aspect(s) of AI ethics, and there is still\\na lack of comprehensive reviews to provide a full picture of\\nthis ﬁeld. For instance, a brief review of ethical issues in AI\\nwas provided in [11], AI ethics guidelines and principles were\\ninvestigated in [12], [13], Mehrabi et al. [14] focused on bias\\nand fairness in ML, García and Fernández [15] only reviewed\\nthe safety in reinforcement learning, Mothukuri et al. [16] re-\\nviewed the security and privacy of federated learning, Liu et\\nal. [17] dedicated to a survey of privacy and security issues in\\ndeep learning, Arrieta et al. [18] concentrated on explainable\\nAI, and Zhang et al. [19] covered the key ethical and privacy\\nissues in AI and traced how such issues have changed over\\nthe past few decades using the bibliometric approach. Thus,',\n",
       "  'AI, and Zhang et al. [19] covered the key ethical and privacy\\nissues in AI and traced how such issues have changed over\\nthe past few decades using the bibliometric approach. Thus,\\nthis article is dedicated to presenting a systematic and compre-\\nhensive overview of AI ethics from diverse aspects (or topics),\\nthereby providing informative guidance for the community to\\npractice ethical AI in the future. We hope it will inform sci-\\nentists, researchers, engineers, practitioners, and other relevant\\nstakeholders, and provides sufﬁcient background, comprehen-\\nsive domain knowledge and a bird’s eye view for interested\\npeople, especially for the beginners of this research discipline,\\nso that further investigation and improvement can be pursued\\nby them.\\nThe main contributions of this article are as follows.\\n1) A comprehensive overview of AI ethics, including ethical\\nissues and risks of AI, ethical guidelines and principles\\nfor AI, approaches for addressing ethical issues in AI, and',\n",
       "  '1) A comprehensive overview of AI ethics, including ethical\\nissues and risks of AI, ethical guidelines and principles\\nfor AI, approaches for addressing ethical issues in AI, and\\nmethods for evaluating ethical AI, is provided in this re-\\nview. This overview can provide a sufﬁcient background,\\ncomprehensive domain knowledge, and a roadmap for\\nresearchers and practitioners.\\n2) The ethical issues and risks caused by AI are summarized,\\nand a new categorization of AI ethical issues is proposed in\\nSection III. The proposed new categorization is helpful for\\nrecognizing, understanding, and analyzing ethical prob-\\nlems in AI and then developing solutions to solve these\\nproblems. Additionally, the ethical issues associated with\\ndifferent stages of AI system’s lifecycle are discussed.\\n3) An up-to-date global landscape of the AI ethics guidelines\\nand principles is presented in Section IV, based on 146\\nguidelines related to AI ethics released by companies,',\n",
       "  '3) An up-to-date global landscape of the AI ethics guidelines\\nand principles is presented in Section IV, based on 146\\nguidelines related to AI ethics released by companies,\\norganizations, and governments around the world. These\\nguidelines and principles provide a high-level guidance\\nfor the planning, development, production, and usage of\\nAI and directions for addressing AI ethical issues.\\n4) A review of multidisciplinary approaches to addressing\\nAI ethical problems, including ethical, technological, and\\nlegal approaches, is given in Section V. This not only\\nprovides an informative summary about the approaches to\\nethical AI but also suggests potentially different solutions\\nto AI ethical issues from a variety of perspectives rather\\nthan relying solely on technological approaches.\\n5) Methods for assessing or evaluating AI ethics are reviewed\\nin Section VI. Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part',\n",
       "  '5) Methods for assessing or evaluating AI ethics are reviewed\\nin Section VI. Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics. However, this aspect is often overlooked in\\nthe existing literature. To the best of our knowledge, this\\narticle is the ﬁrst to summarize the aspect of evaluating\\nethical AI.\\n6) Lastly, some challenges in AI ethics and several future\\nperspectives are pointed out, which provide some research\\nquestions and directions for further research in the future.\\nThis will be helpful for interested researchers and practi-\\ntioners to pursue further research in AI ethics ﬁeld.\\nThe rest of the article is organized as follows. After this\\nintroductory section, we brieﬂy describe the review scope and\\nmethodology of this article in Section II. A comprehensive\\nsummary of the ethical issues and risks raised from AI is given\\nin Section III. Section IV reviews and analyzes the AI ethical',\n",
       "  'methodology of this article in Section II. A comprehensive\\nsummary of the ethical issues and risks raised from AI is given\\nin Section III. Section IV reviews and analyzes the AI ethical\\nguidelines and principles that have been released during the last\\nfew years. Section V describes the paradigms or approaches\\nfor addressing ethical issues in AI. Section VI discusses the\\napproaches to evaluate the morality or ethics of AI systems or\\nproducts. Section VII outlines the challenges in implementing\\nethics in AI and gives some future perspectives on designing\\nethical AI. Section VIII brieﬂy concludes this article.\\nII. SCOPE AND METHODOLOGY\\nInthis section, weﬁrst clarifytheaspects andtopics coveredin\\nthis review and the links between these topics. Then, we describe\\nthe methodology followed in conducting this survey, including\\nthe literature search strategy and selection criteria.\\nA. Scope\\nThe scope and topics of this article is described as follows.',\n",
       "  'the methodology followed in conducting this survey, including\\nthe literature search strategy and selection criteria.\\nA. Scope\\nThe scope and topics of this article is described as follows.\\nInvestigation of ethical issues and risks of AI is the starting\\npoint of this review, since it is because of the existence of\\nethical issues in AI that the research ﬁeld of AI ethics exists.\\nThus, it is necessary and important to clarify and understand\\nthe ethical problems existed in AI. Then, the ethical guidelines\\nand principles, which direct the development and use of AI,\\nare reviewed. As the ethical issues of AI have attracted more\\nand more attention from various sectors of our society, many\\norganizations (including academia, industry, and governments)\\nhave begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.',\n",
       "  'have begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.\\nAfter clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI. We',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n801\\nFig. 1.\\nTopics covered in this article and the links between them.\\ncovered ethical, technological, and legal approaches, but focus\\nmore on the ﬁrst two kinds of approaches (ethical and tech-\\nnological approaches) since the researchers in AI community\\nmay be more interested in these two categories of approaches.\\nLast but not least, we summarize how to evaluate ethical AI,\\nwhich is to assess the ethicality or morality of AI, i.e., how well\\nthe ethical problems are addressed or whether an AI system\\nmeets the ethical requirements or not. Apparently, these four\\naspects are essential for solving ethical issues in AI. Thus, the\\nabove four aspects constitute the main content of this article and\\nprovide a systematic overview of AI ethics. The topics or aspects\\ncovered in this article and the links between them are illustrated\\nin Fig. 1.\\nB. Methodology\\nThis review covers a wide variety of documents, including',\n",
       "  'covered in this article and the links between them are illustrated\\nin Fig. 1.\\nB. Methodology\\nThis review covers a wide variety of documents, including\\nacademic, organizational, government grey literature sources,\\nand news report. The search of relevant literature was conducted\\nin two phases. In the ﬁrst phase, the entries or keywords that\\nreﬂect different terms related to AI ethics are used to search on\\nGoogle Scholar, Web of Science, IEEE Xplore, ACM Digital\\nLibrary, Science Direct, Springer Link, arXiv, and Google. The\\nentries or keywords used include: (ethics, ethical, responsibility,\\nresponsible, trustworthiness, trustworthy, transparent, explain-\\nable, fair, beneﬁcial, robust, safe, private, sustainable) AND/OR\\n(issues, risks, guideline, principle, approach, method, evalua-\\ntion, assessment, challenge) AND (artiﬁcial intelligence, AI,\\nmachine learning, ML, intelligent system, intelligent agent). We\\nmainly consider the literature published or released since 2010',\n",
       "  'tion, assessment, challenge) AND (artiﬁcial intelligence, AI,\\nmachine learning, ML, intelligent system, intelligent agent). We\\nmainly consider the literature published or released since 2010\\nand included as many related keywords as possible in titles. In\\nthesecondphase,wecheckedtherelatedworkofliteraturefound\\nin the ﬁrst phase, such as the cited articles and other work by the\\nsame authors of phase one.\\nAs for the ethical AI guidelines, we only collected these\\ndocuments in English (or with ofﬁcial English translations) and\\ncan be visited or downloaded on the internet. A full list with\\nURL links of collected ethical AI guidelines is provided in the\\nSupplementary Materials of this article.\\nIII. ETHICAL ISSUES AND RISKS OF AI\\nTo address the ethical problems of AI, we must ﬁrst recognize\\nand understand the potential ethical issues or risks that AI may\\nbring. Then, the necessary AI ethical guidelines, policies, prin-\\nciples, rules (i.e., Ethics of AI) can be formulated appropriately.',\n",
       "  'and understand the potential ethical issues or risks that AI may\\nbring. Then, the necessary AI ethical guidelines, policies, prin-\\nciples, rules (i.e., Ethics of AI) can be formulated appropriately.\\nWith the adequate ethics of AI, we can design and build AI\\nthat behaves ethically (i.e., Ethical AI) [8]. The ethical issue\\nof AI generally refers to the morally bad things or problematic\\noutcomes relevant to AI (i.e., these issues and risks that are raised\\nby the development, deployment, and use of AI) that need to be\\naddressed. Many ethical issues, such as lack of transparency,\\nprivacy and accountability, bias and discrimination, safety and\\nsecurity problems, the potential for criminal and malicious use,\\nand so on, have been identiﬁed from the applications and studies.\\nThis section focuses on ethical issues and risks of AI. First,\\nfour different categorizations of AI ethical issues in the literature\\nare reviewed in Section III-A. Since these four categorizations',\n",
       "  'This section focuses on ethical issues and risks of AI. First,\\nfour different categorizations of AI ethical issues in the literature\\nare reviewed in Section III-A. Since these four categorizations\\neither ignore some ethical issues or are too complicated to\\nunderstand, we proposed a new categorization that classiﬁes\\nAI ethical issues into individual, societal, and environmental\\nlevels in Section III-B. Our proposed categorization compre-\\nhensively covers the existing ethical issues and is easy to un-\\nderstand, which is helpful for understanding and analyzing the\\nethical problems caused by AI. Besides, we attempt to map\\nthe ethical issues associated with the stages of AI system’s\\nlifecycle in Section III-C. This would be beneﬁcial for ﬁguring\\nout these issues during the AI system development process.\\nThe main goal of this section is to discuss and clarify the\\nethical issues of AI so that practitioners can recognize and\\nunderstand these issues, and then help them to further study',\n",
       "  'The main goal of this section is to discuss and clarify the\\nethical issues of AI so that practitioners can recognize and\\nunderstand these issues, and then help them to further study\\nhow to address AI ethical issues. The main contribution in\\nthis section is that we proposed a new categorization of AI\\nethical issues, which covers the ethical issues discussed in a clear\\nand easy-to-understand manner. Additionally, the ethical issues\\nassociated with the stages of AI system’s lifecycle is discussed.\\nA. Review of Categorizations of AI Ethical Issues\\nThis section describes the ethical concerns or issues of AI\\nfrom different perspectives by reviewing four different cate-\\ngorizations that were found in our collected literature. Two\\nof them are from government reports and the other two are\\nfrom academic publications. From different perspectives and\\ncategorizations, the ethical issues involved are also somewhat\\ndifferent. In the following, four different categorizations of AI',\n",
       "  'from academic publications. From different perspectives and\\ncategorizations, the ethical issues involved are also somewhat\\ndifferent. In the following, four different categorizations of AI\\nethical issues are reviewed subsequently. The four reviewed cat-\\negorizations of AI ethical issues and our proposed categorization\\nare listed in Table I.\\n1) Categorization Based on Features of AI, Human Factors\\nand Social Impact: In [11], AI ethical issues are mainly dis-\\ncussed in three categories: ethical issues caused by the features\\nof AI, ethical risks caused by human factors, and social impact\\nof ethical AI issues.\\na) Ethical issues caused by features of AI: Transparency:\\nML is the core technology of current AI, especially (deep) neural\\nnetworks. However, it is hard to explain and understand the\\ninference procedure of ML, which is commonly known as the',\n",
       "  '802\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nTABLE I\\nLIST AND DISCUSSION OF THE REVIEWED CATEGORIZATION OF ETHICAL ISSUES OF AI AND OUR PROPOSED CATEGORIZATION\\n“black-box.” The opacity of ML makes the algorithms or models\\nmysterious to users and even developers. This mainly leads to the\\ntransparency issue [20]. The lack of transparency not only leads\\ntotheexplanatoryproblem,butalsoleadstodifﬁcultiesinhuman\\nmonitoring and guidance of ML or AI. Thus, transparency or\\nexplainability is one of the most widely discussed downside of\\nAI.\\nData Security and Privacy: The performance of current AI\\nstrongly depends on the training data. Usually, a huge amount\\nof data, which probably includes personal data and private\\ndata, is required to train an AI model, particularly the deep\\nlearning model. The misuse and malicious use of data, such as\\n(personal) information leakage or tampering, are serious ethical\\nissues that are closely related to every individual, institution,',\n",
       "  'learning model. The misuse and malicious use of data, such as\\n(personal) information leakage or tampering, are serious ethical\\nissues that are closely related to every individual, institution,\\norganization, and even the country. Data security and privacy\\nare key issues encountered in the development and application\\nof AI technology [21].\\nAutonomy, Intentionality, and Responsibility: With the\\nadvancement of AI, current AI systems or agents, such as health-\\ncare robots, have a certain degree of autonomy, intentionality,\\nand responsibility [22]. Here, the autonomy of AI refers to an AI\\nsystem’s ability to operate without human intervention or direct\\ncontrol. Intentionality refers to the ability that an AI system can\\nact in a way that is morally harmful or beneﬁcial and the actions\\nare deliberate and calculated [11]. Responsibility indicates that\\nthe AI system fulﬁll some social rule and some assumed respon-\\nsibilities. However, how much autonomy, intentionality, and',\n",
       "  'are deliberate and calculated [11]. Responsibility indicates that\\nthe AI system fulﬁll some social rule and some assumed respon-\\nsibilities. However, how much autonomy, intentionality, and\\nresponsibility should an AI system be allowed is a challenging\\nquestion and issue.\\nb) Ethical issues caused by human factors: Accountabil-\\nity: When an AI system or agent fails in a speciﬁed task and\\nresults in bad consequences, who should be responsible. The\\nundesirable consequence may be caused by many factors, such\\nas the programming codes, input data, improper operation, or\\nother factors. This brings about the so-called “the problem of\\nmany hands” [23]. Thus, accountability is an ethical issue that\\nconcerns the human factors involved in the designing, imple-\\nmentation, deployment, and usage of AI.\\nEthicalStandards:AstheultimategoalofAIethicsistocreate\\nethical AI that can follow ethical principles and behave ethically\\n[10], it is crucial to form comprehensive and unbiased ethical',\n",
       "  'EthicalStandards:AstheultimategoalofAIethicsistocreate\\nethical AI that can follow ethical principles and behave ethically\\n[10], it is crucial to form comprehensive and unbiased ethical\\nstandards for training or regulating AI to be ethical. To formulate\\nethical standards for AI, researchers and practitioners should\\nwell understand the existing ethical theories and principles [13],\\n[24].\\nHuman Rights Laws: The designer, software engineers, and\\nother participants in AI system design and application should be\\ntaught human rights laws [25]. Without training in human rights\\nlaws, they may infringe and breach essential human rights with-\\nout even realizing it. The human rights laws or acts followed by\\ndifferent countries or regions are often different. Many different\\nhuman rights laws, for instance, International Human Rights\\nLaw, International Covenant on Civil and Political Rights, In-\\nternational Covenant on Economic, Social and Cultural Rights,',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n803\\nUniversal Declaration of Human Rights, Charter of the United\\nNations, the European Convention for the Protection of Human\\nRights and Fundamental Freedoms, etc. [26] have been released\\nby different governments.\\nc) Social impact of ethical AI issues: Automation and Job\\nReplacement: As more and more factory workers are being\\nreplaced by automated systems and robots, AI will disrupt and\\ntransform the labor market. Hence, many people worry about\\nautomation and job replacement [27].\\nAccessibility: The accessibility or availability of emerging\\ntechnologies, such as AI, will have a direct impact on human\\nwell-being. However, it will be unethical and unfair if only a\\nportion of the population beneﬁt from AI. Consideration must be\\ngiven to developing AI products and services that are accessible\\nto everyone, and thus the beneﬁts of AI can be spread equally\\nto everyone [28].\\nDemocracy and Civil Rights: Unethical AI will distort the',\n",
       "  'to everyone, and thus the beneﬁts of AI can be spread equally\\nto everyone [28].\\nDemocracy and Civil Rights: Unethical AI will distort the\\ntruthandeventuallyleadtotheloss of trust andpublicsupport for\\nAI technology [11]. The strengths of democracies are harmed by\\nthe loss of informed and trusting communities. As democracies\\nsuffer and structural biases exacerbated, the free enjoyment\\nof civil rights is no longer consistently available to all. Thus,\\ndemocracy and civil rights must be taken into consideration in\\nAI ethics.\\n2) Categorization Based on Vulnerabilities of AI and Human:\\nIn [29], Liao distinguished the ethical issues of AI into 1) ethical\\nissues that arise because of limitations of current ML systems,\\nwhich is named as “vulnerabilities in AI (especially ML),” and\\n2) ethical issues that arise because current ML systems may be\\nworking too well and humans can be vulnerable in the presence\\nof or interaction with these intelligent systems, which is referred',\n",
       "  '2) ethical issues that arise because current ML systems may be\\nworking too well and humans can be vulnerable in the presence\\nof or interaction with these intelligent systems, which is referred\\nto as “human vulnerabilities.”\\na) Ethical issues from the vulnerabilities of AI: ML is data\\nhungry: Usually, ML requires a large amount of data to work\\nwell [30]. Therefore, this motivates companies and organiza-\\ntions to collect or purchase data, including sensitive personal\\ndata, even if doing so may violate the individual’s right to\\nprivacy.\\nGarbage in/garbage out: The performance of a ML algorithm\\nheavily depends on the data from which it learns. If one ML\\nalgorithm is trained on insufﬁcient or inaccurate data, it will\\nprovide undesirable results even it is well designed [31].\\nFaulty algorithms: Even if a ML algorithm is input with\\nenough and accurate data, if the algorithm itself is bad, it will\\nalso make bad predictions. For example, a bad ML algorithm',\n",
       "  'Faulty algorithms: Even if a ML algorithm is input with\\nenough and accurate data, if the algorithm itself is bad, it will\\nalso make bad predictions. For example, a bad ML algorithm\\nmay not be able to recognize a pattern even if there is one or\\nit may recognize a pattern even if there is not one, where are\\nknown as “underﬁtting” and “overﬁtting,” respectively [32].\\nDeep learning is a black box: Deep learning is a black\\nbox, which raises issues such as explainability, interpretability,\\nand trust [33]. Even for the designers and developers of deep\\nlearning, the model is incomprehensible since it usually involves\\nthousands or millions of connections between different neurons.\\nTherefore, it is difﬁcult to explain how these connections interact\\nand why the model makes certain predictions.\\nb) Ethical issues from the vulnerabilities of human: Abuse\\nof AI: AI technologies, such as facial recognition and image\\ngeneration, can work better than humans [34]. However, ethical',\n",
       "  'b) Ethical issues from the vulnerabilities of human: Abuse\\nof AI: AI technologies, such as facial recognition and image\\ngeneration, can work better than humans [34]. However, ethical\\nissues exist because people may be tempted to use them for\\nill. For instance, a government could use facial recognition\\ntechnology to monitor its citizens, and ML can be used to\\nfabricate photos or videos so realistic that humans cannot tell\\nthat they are fake [35]. This brings the concern about the abuse\\nof AI technologies.\\nJob replacement: Since intelligent robots can perform certain\\ntasks faster and better than humans, many people worry that\\nrobots and other AI technologies will replace a large part of\\ncurrent human labor in the near future [36]. Thus, people may\\nbe in fear of job replacement.\\nIssues about robotic companions: As AI robots become more\\nand more sophisticated, they have begun to be regarded as\\ncompanions of humans. This raises some ethical issues about',\n",
       "  'Issues about robotic companions: As AI robots become more\\nand more sophisticated, they have begun to be regarded as\\ncompanions of humans. This raises some ethical issues about\\nthe relationship between human and robotic companions [37].\\n3) Categorization Based on Algorithm, Data, Application,\\nand Long-Term and Indirect Ethical Risks: In the analysis report\\nof AI ethical risks [38] released by the Chinese National AI\\nStandardization General Working Group, AI ethical issues are\\ncategorized into the following four aspects:\\n1) ethical issues related to AI algorithms;\\n2) ethical issues related to data;\\n3) ethical issues related to the application of AI;\\n4) long-term and indirect ethical risks.\\na) Ethical issues related to algorithms: Algorithm secu-\\nrity: The AI algorithms pose several security issues. First, there\\nis a risk of algorithm or model leakage [39], [40]. Generally, the\\nmodel is achieved by training it on the training data through op-',\n",
       "  'rity: The AI algorithms pose several security issues. First, there\\nis a risk of algorithm or model leakage [39], [40]. Generally, the\\nmodel is achieved by training it on the training data through op-\\ntimizing its parameters. If the model parameters of an algorithm\\nare leaked, a third party may be able to copy the model. This will\\ncause economic loss to the owner of the model, since a third party\\nobtains the same model without paying the cost of obtaining\\nthe training data. Second, the parameters of the AI algorithm\\nmodel may be modiﬁed illegally by an attacker, which will cause\\nthe performance deterioration of the AI model and may lead\\nto undesirable consequences. Additionally, in many scenarios,\\nthe output of the model is closely related to personal safety,\\nsuch as in the medical and autonomous driving ﬁelds. Once\\nthere are loopholes or mistakes in the application of algorithms\\nin these ﬁelds, it will directly harm humans and cause serious\\nconsequences [41].',\n",
       "  'there are loopholes or mistakes in the application of algorithms\\nin these ﬁelds, it will directly harm humans and cause serious\\nconsequences [41].\\nAlgorithm explainability: Due to the black-box characteristic\\nof many ML algorithms [33], especially the popular deep learn-\\ning or neural networks, the decision process of AI algorithms\\nis hard to understand. The interpretability or explainability of\\nalgorithms is an essential ethical issue of AI [42], since it\\nconcerns the human right to know.\\nAlgorithmic decision dilemma: After obtaining the AI model,\\nthe result of the algorithm is usually unpredictable for us. In\\nother words, even though we have designed an AI model well,\\nwe cannot foresee or predict the decisions of the algorithm and\\nthe consequence it will produce. This leads to the algorithmic\\ndecision risk or dilemma of AI. For instance, autonomous ve-\\nhicles should reduce trafﬁc accidents, but sometimes they have\\nto choose between two evils, such as crushing pedestrians or',\n",
       "  'decision risk or dilemma of AI. For instance, autonomous ve-\\nhicles should reduce trafﬁc accidents, but sometimes they have\\nto choose between two evils, such as crushing pedestrians or\\nsacriﬁcing themselves and passengers to save pedestrians [43].',\n",
       "  '804\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nb) Ethical issues related to data: Privacy protection: With\\nthe development of big data and AI, the tension between AI\\ntechnology and user privacy protection has become more and\\nmore serious. Criminals have more ways to obtain personal\\nprivacy data with lower costs and greater beneﬁts. Data security\\nincidents have commonly occurred in recent years. Privacy\\nprotection has become a well-recognized and serious ethical\\nissue involved by using AI [44].\\nRecognizing and processing personal and sensitive infor-\\nmation: Traditional laws and regulations only focus on the\\nprotection of personal and sensitive information. If the personal\\nor sensitive information is deidentiﬁed [45] through randomiza-\\ntion, data synthesis, and other technologies, it will no longer be\\nregarded as personal or sensitive information and not protected\\nby traditional laws. The subsequent usage, sharing, and transfer',\n",
       "  'tion, data synthesis, and other technologies, it will no longer be\\nregarded as personal or sensitive information and not protected\\nby traditional laws. The subsequent usage, sharing, and transfer\\nof such information arise some ethical issues.\\nc) Ethical issues related to application: Algorithm dis-\\ncrimination: The execution results of algorithms directly affect\\nthe decision-making of AI systems. However, algorithm dis-\\ncrimination or bias has been seen in many applications of AI.\\nFor instance, the racial bias in criminal justice systems [46], and\\ngender discrimination in hiring [47].\\nAlgorithm abuse: Algorithm abuse [48] refers to the situation\\nwhere people use algorithms for analysis, decision-making,\\ncoordination, and other activities, but their use purpose, use\\nmethod, use range, etc., have deviations and cause adverse\\neffects. For example, facial recognition algorithms can be used\\nto improve the level of public security and speed up the discovery',\n",
       "  'method, use range, etc., have deviations and cause adverse\\neffects. For example, facial recognition algorithms can be used\\nto improve the level of public security and speed up the discovery\\nof criminal suspects, but if they are applied to detect potential\\ncriminals, or to determine whether someone has criminal poten-\\ntial based on their face, it is an algorithm abuse.\\nd) Long-term and indirect ethical risks: Employment:\\nWith the fast advancement and widespread application of AI,\\nmore and more work can be completed by some AI products\\n[27]. This will have a signiﬁcant inﬂuence on the employment\\nproblem.\\nOwnership: As AI continues to improve, the intellectual dif-\\nferences between AI agents and humans will gradually shrink.\\nA series of debates on ownership will follow, such as whether\\nthe AI agent should be considered as “legal subject,” whether AI\\nproducts have property rights (copyrights or patent rights) [49],\\nand so forth.\\nCompetition: Unfair competition, malicious competition, and',\n",
       "  'products have property rights (copyrights or patent rights) [49],\\nand so forth.\\nCompetition: Unfair competition, malicious competition, and\\nmonopolistic behaviors with technological advantages will all\\nhave an impact on social stability and market freedom, fair-\\nness, and equal value, and will seriously damage the interests\\nof consumers and hinder the improvement of social welfare\\n[38]. When companies, organizations or individuals use AI\\nalgorithms, they should follow competitive ethics and not go\\nbeyond legal boundaries.\\nResponsibility: With the widespread application of AI, many\\ncases in which AI products violate the laws or ethics, such\\nas personal injury and algorithmic bias, have been observed.\\nA fundamental problem that arises in these cases is who is\\nresponsible for these bad consequences [50]. For example, as\\nautonomous driving involves multiple subjects, such as car\\nowners, drivers, passengers, car manufacturers, autonomous',\n",
       "  'responsible for these bad consequences [50]. For example, as\\nautonomous driving involves multiple subjects, such as car\\nowners, drivers, passengers, car manufacturers, autonomous\\ndriving system providers, pedestrians, etc., how should they bear\\nresponsibilities after a trafﬁc accident.\\n4) Categorization Based on the Deployment of AI: In Euro-\\npean Parliamentary Research Service’s latest study on the ethical\\nimplications and moral questions brought by AI [51], the ethical\\nissues are mapped into different categories according to the ethi-\\ncalimpactsofAIonhumansociety,humanpsychology,ﬁnancial\\nsystem, legal system, environment and the planet, and trust.\\na) Impact on society: The labor market: AI has already\\nbeen applied in ﬁnance, advanced manufacturing, transporta-\\ntion, energy development, healthcare, and many other sectors.\\nWe have already seen the impact of automation on “blue collar”\\njobs. As AI agents or robots become more and more sophis-',\n",
       "  'tion, energy development, healthcare, and many other sectors.\\nWe have already seen the impact of automation on “blue collar”\\njobs. As AI agents or robots become more and more sophis-\\nticated, creative, versatile, and intelligent, more jobs will be\\naffected by AI technologies and more positions will be obsolete.\\nTherefore, AI technologies may put current job classes at risk,\\neliminate positions, cause mass unemployment in many job\\nsectors [36]. Furthermore, discrimination in the labor market\\nmay also be an issue, for instance, people without high-skill\\ntraining will be disproportionately affected by the application\\nof AI.\\nInequality: AI technologies are expected to enable companies\\nto streamline their business operations and make them more\\nefﬁcient and productive. However, some people argue that this\\nwill come at the expense of their human workforces. Thus, this\\nwill inevitably indicate that revenues will be split across fewer',\n",
       "  'efﬁcient and productive. However, some people argue that this\\nwill come at the expense of their human workforces. Thus, this\\nwill inevitably indicate that revenues will be split across fewer\\npeople and individuals with ownership in AI-driven companies\\nwill receive disproportionate beneﬁts, which indeed increase\\nsocial inequalities [52].\\nPrivacy, human rights, and dignity: AI is already affecting\\nprivacy, human rights, and dignity in many ways. For example,\\nthe intelligent personal assistants (IPA), such as Apple’s Siri,\\nAmazon’s Echo, and Google’s Home, can learn the interests\\nand behavior of their users, but, at the same time, the users\\nraise concerns about the fact that they are always running and\\nlistening in the background [53]. The IPA obviously affects our\\nprivacy. AI has an important impact on democracy and people’s\\nright to private life and dignity. For instance, if AI can be used\\nto determine people’s political beliefs, then individuals may',\n",
       "  'privacy. AI has an important impact on democracy and people’s\\nright to private life and dignity. For instance, if AI can be used\\nto determine people’s political beliefs, then individuals may\\nbe vulnerable to manipulation. Political strategists can use this\\ninformation to determine which voters are likely to be persuaded\\nto change party afﬁliation and then use resources to persuade\\nthem to do so.\\nBias: Human bias, such as gender prejudice and racism bias,\\nmay be inherited by AI. The bias of AI may arise as a result\\nof the training data, the value held by the developers and users,\\nor acquired from the learning process of AI itself. Many cases\\nof AI bias, machine bias or algorithmic bias have been reported\\n[54]. The bias of AI will promote unexpected social bias or\\ndiscrimination. Thus, bias is an ethical issue that is often talked\\nabout by the public.\\nDemocracy. The implementation and adoption of AI can\\nthreaten democracy in several ways. First, the concentration',\n",
       "  'about by the public.\\nDemocracy. The implementation and adoption of AI can\\nthreaten democracy in several ways. First, the concentration\\nof technological, economic, and political power related to AI\\namong a few mega corporations could allow them to pose undue\\ninﬂuence over the government. Second, AI may damage democ-\\nracy by affecting political elections [55]. With the aid of AI and',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n805\\nbig data, politicians have access to huge amounts of information\\nthat allow them to target speciﬁc voters and develop messages\\nthat will resonate with them most. Third, the increasing use of\\nAI-based new recommenders, which present readers with news\\nstories based on their previous reading history, reduces readers’\\nchances of encountering different and undiscovered content,\\noptions, and viewpoints [56]. This could result in increasing\\nsocietal polarization.\\nb) Impact on human psychology: Relationships: AI is\\ngetting better and better at imitating human thought, experi-\\nence, action, dialogue, and relationships. In the future, we will\\nfrequently interact with machines or AI products as if they are\\nhumans. This will have impacts on real human relationships and\\nthus bring some ethical issues [57].\\nPersonhood: AI systems are increasingly taking on tasks\\nand decisions that are traditionally performed by humans. An',\n",
       "  'thus bring some ethical issues [57].\\nPersonhood: AI systems are increasingly taking on tasks\\nand decisions that are traditionally performed by humans. An\\nessential and ethical question that arise from this is that whether\\nAI system should be endowed with “personhood” and moral or\\nlegal agency rights [58].\\nc) Impact on the ﬁnancial system: The application of AI\\nin ﬁnancial markets has signiﬁcantly improved transaction efﬁ-\\nciencyandtradingvolume.Marketsareverysuitableforautoma-\\ntion, because they now operate almost entirely electronically\\nand a huge amount of data is generated at a high rate, which\\nrequires the employment of algorithms to digest and analyze\\nit. Additionally, due to the dynamic of markets, fast reaction to\\ninformation is critical [59], which provides considerable incen-\\ntives to replace slow people’s decision process with algorithmic\\ndecision-making. Furthermore, the rewards for effective trading\\ndecisions are considerable, which explains why companies have',\n",
       "  'tives to replace slow people’s decision process with algorithmic\\ndecision-making. Furthermore, the rewards for effective trading\\ndecisions are considerable, which explains why companies have\\ninvested so much in AI technology.\\nHowever, the AI-based automatic trading agents may also be\\nusedmaliciouslytodestabilizethemarketsorharminnocentpar-\\nties in other ways. Even if they are not intended to be malicious,\\nthe autonomy and ﬂexibility of algorithmic trading strategies,\\nincluding the increasing use of ML techniques, make it difﬁcult\\nfor people to predict how they will perform in unexpected\\nsituations.\\nd) Impact on the legal system: Criminal law: According to\\ncurrent criminal law, a crime consists of two elements, that is, a\\nvoluntary act (or omission) and an intention to commit a crime. If\\nAI products or robots are shown to have sufﬁcient consciousness\\nor awareness, thentheymaybethedirect perpetrators of criminal\\noffenses or responsible for negligent crimes. If we admit that AI',\n",
       "  'AI products or robots are shown to have sufﬁcient consciousness\\nor awareness, thentheymaybethedirect perpetrators of criminal\\noffenses or responsible for negligent crimes. If we admit that AI\\nproducts have their own mind, human-like free will, autonomy,\\nor moral sense, then our criminal law and even the entire legal\\nsystem will have to be revised [60].\\nTort law: Tort law covers situations such as one person’s\\nbehavior case injury, suffering, unfair loss, or harm to another\\nperson. When an accident involving self-driving car(s) occurs,\\nthere are two legal areas that are relevant—negligence and\\nproduct liability. While, today, most accidents result from driver\\nerror, which indicates that liability for accidents are governed\\nby the negligence principle. So, in the future, the tort law, which\\nincludes many different types of personal injury claims, will be\\nsigniﬁcantlyaffected[61] sinceAI products (suchas self-driving\\ncars or other intelligent robots) will involve in personal injury',\n",
       "  'includes many different types of personal injury claims, will be\\nsigniﬁcantlyaffected[61] sinceAI products (suchas self-driving\\ncars or other intelligent robots) will involve in personal injury\\nclaims, such as the accident between self-driving cars or the\\ninjury claim where a robot harm human.\\ne) Impact on the environment and the planet: Use of nat-\\nural resources: The development and application of AI will\\nincrease the demand of many natural resources, such as rare\\nearth metals like nickel, cobalt, graphite, and so on. As the\\nexisting supply decreases, operators may be forced to work in\\nnew and more complex environments to mine. This will increase\\nthe production and consumption rate of rare earth metals, and\\nfurther damage the environment [62].\\nPollution and waste: The increase in production and con-\\nsumption of AI technological devices such as robots will ex-\\nacerbate pollution and waste, such as the accumulation of heavy\\nmetals and toxic materials in the environment [63].',\n",
       "  'sumption of AI technological devices such as robots will ex-\\nacerbate pollution and waste, such as the accumulation of heavy\\nmetals and toxic materials in the environment [63].\\nEnergy concerns: Employing AI technology, particularly\\ndeep learning, generally involves training ML models on a\\nhuge amount of data, which usually consumes large amounts\\nof energy. According to listed data in [64], the carbon footprint\\nof training a natural language processing model (a Transformer\\nmodel) is roughly 5 times the carbon footprint of an average car\\nacross its entire lifetime.\\nf) Impact on trust: AI promises numerous changes and\\nbeneﬁts to individual’s lives and the society. It is changing\\nour daily lives in many domains, such as transportation, ser-\\nvice industry, healthcare, education, public safety and secu-\\nrity, and entertainment. Nevertheless, these AI systems must\\nbe introduced in ways that foster trust and understanding and\\nrespect human and civil rights [65]. The consensus among the',\n",
       "  'rity, and entertainment. Nevertheless, these AI systems must\\nbe introduced in ways that foster trust and understanding and\\nrespect human and civil rights [65]. The consensus among the\\nresearch community is that trust in AI can only be achieved\\nthrough fairness, transparency, accountability, and regulation\\n(or control).\\nFairness: In order to trust AI, it must be fair and impartial. As\\nmore and more decisions are delegated to AI, we must ensure\\nthat these decisions are free from bias and discrimination [66].\\nWhether it is ﬁltering through CVs for job interviews, deciding\\non admissions to the university, or conducting credit ratings for\\nloan companies, it is essentially vital that decisions made by AI\\nare fair.\\nTransparency: Transparency is important for building trust\\nin AI since it should be a must to know why an AI system\\nmade a particular decision, especially if that decision caused\\nundesirable consequences or harm. In view of the fact that the',\n",
       "  'in AI since it should be a must to know why an AI system\\nmade a particular decision, especially if that decision caused\\nundesirable consequences or harm. In view of the fact that the\\nautopilot of an intelligent car has led to several fatal accidents,\\nit is clear that transparency is urgently needed to discover how\\nand why these accidents occur, and to correct any technical or\\noperational failures. The opacity in ML, which is well-known as\\nblack-box, is one of the main impediments to the transparency of\\nAI [51].\\nAccountability: Accountability [67] ensures that if an AI\\nsystem makes a mistake or hurts someone, then someone can\\nbe held responsible, whether it is the designer, developer, or\\ncompany selling the AI. In the event of damages, accountability\\nis essential to establish a remedial mechanism so that victims can\\nreceive adequate compensation. Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the',\n",
       "  'receive adequate compensation. Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68]. This is largely related to people’s fear',\n",
       "  '806\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nabout the idea of “super-intelligence,” that is, as the intelligence\\nof AI increases to the point that it surpasses human abilities, AI\\nmay come to take control over our resources and outcompete our\\nspecies, and even leading to human extinction. A related concern\\nis that even if an AI agent is carefully designed to align its\\ngoals with human needs, it may develop unpredictable subgoals\\non its own. Therefore, in order to maintain trust in AI, it is\\nimportant that humans must have ultimate oversight or control\\non AI technology.\\nB. Our Proposed Categorization: Ethical Issues At Individual,\\nSocietal and Environmental Levels\\nIn the previous section, we have reviewed the AI ethical issues\\ndescribed and categorized in the literature (see Table I). How-\\never, the above presented categorizations have obvious ﬂaws.\\nSpeciﬁcally, the categorization based on features of AI, human',\n",
       "  'described and categorized in the literature (see Table I). How-\\never, the above presented categorizations have obvious ﬂaws.\\nSpeciﬁcally, the categorization based on features of AI, human\\nfactors, and social impact [11] obviously ignores the impact\\nof AI on the environment, such as natural resource consump-\\ntion and environmental pollution. The categorization based on\\nvulnerabilities of AI and human [29] omits several important\\nissues, such as responsibility, safety, and environmental prob-\\nlems. The categorization based on algorithm, data, application,\\nand long-term and indirect ethical risks [38] misses the con-\\nsiderations of fairness, autonomy and freedom, human dignity,\\nenvironmental problems, etc. Although the categorization based\\non the deployment of AI [51] covers ethical issues comprehen-\\nsively, this classiﬁcation is too cumbersome and some issues,\\nincluding responsibility, safety, and sustainability, are omitted.\\nThismotivatesustofurtheranalyzeandsortoutAIethicalissues.',\n",
       "  'sively, this classiﬁcation is too cumbersome and some issues,\\nincluding responsibility, safety, and sustainability, are omitted.\\nThismotivatesustofurtheranalyzeandsortoutAIethicalissues.\\nIt is of no doubt that AI systems mainly serve individuals\\nor the public of society. Hence, we can analyze and clarify AI\\nethical issues from individual and societal perspectives. At the\\nsame time, as entities on the planet, AI products will inevitably\\nhave impacts on the environment. So, the ethical issues related to\\nthe environmental aspects also need to be considered. Therefore,\\nin this section, we proposed to classify AI ethical issues at three\\ndifferent levels, that is, ethical issues at individual, societal, and\\nenvironmental levels. Ethical issues at individual level mainly\\ninclude issues that have undesirable consequence for individual\\nhuman beings, their rights, and their well-being [69]. AI ethical\\nissues at societal level consider the societal consequence that AI',\n",
       "  'include issues that have undesirable consequence for individual\\nhuman beings, their rights, and their well-being [69]. AI ethical\\nissues at societal level consider the societal consequence that AI\\nhas brought or may bring for groups or society as a whole [69].\\nAI ethical issues at the environmental level focus on the impacts\\nof AI on the natural environment. Our proposed categorization\\nis shown in Fig. 2.\\n1) Ethical Issues at Individual Level: At individual level, AI\\nhas brought inﬂuence on the safety, privacy, autonomy, and hu-\\nmandignityofindividuals.TheapplicationofAIhasposedsome\\nrisks on the safety of individuals. For instance, person injury\\naccidents involving autonomous cars and robots have occurred\\nand reported in the past few years. Privacy issue is one of the\\nserious risks that AI brings to us. To achieve good performance,\\nAI systems usually require a huge amount of data, which often\\ninclude users’ private data. However, there are serious risks',\n",
       "  'serious risks that AI brings to us. To achieve good performance,\\nAI systems usually require a huge amount of data, which often\\ninclude users’ private data. However, there are serious risks\\nassociated with this data collection. One of the main issues is\\nprivacy and data protection. Additionally, as described in the\\nFig. 2.\\nProposed categorization of AI ethical issues.\\nprevious section, the application of AI may bring challenges to\\nhuman rights, such as autonomy, and dignity. Autonomy refers\\nto the capacity of thinking, deciding, and acting independently,\\nfreely and without inﬂuence of others [70]. When AI-based\\ndecision-making are widely adopted in our daily life, three is\\nbig danger of restricting the autonomy of us. Human dignity,\\nwhich is one of the principal human rights, is about the right of\\na person to be respected and treated in an ethical manner [71].\\nThe protection of dignity is crucial in the context of AI. Human',\n",
       "  'which is one of the principal human rights, is about the right of\\na person to be respected and treated in an ethical manner [71].\\nThe protection of dignity is crucial in the context of AI. Human\\ndignity should be one of the basic concepts for protecting human\\nbeings from harm and should be respected when developing AI\\ntechnologies. For instance, a lethal autonomous weapon system\\n[72] may violate the principle of human dignity.\\n2) Ethical Issues at Societal Level: When considering the\\nAI ethical issues at societal level, we mainly focus on the\\nbroad consequences and impacts that AI brings for society and\\nthe well-being of communities and nations around the world.\\nUnder the categorization of ethical issues at societal level, we\\ndiscuss fairness and justice, responsibility and accountability,\\ntransparency, surveillance and dataﬁcation, controllability of\\nAI, democracy and civil rights, job replacement, and human\\nrelationship.\\nThe existence of bias and discrimination in AI has posed',\n",
       "  'transparency, surveillance and dataﬁcation, controllability of\\nAI, democracy and civil rights, job replacement, and human\\nrelationship.\\nThe existence of bias and discrimination in AI has posed\\nchallenges on fairness and justice. The biases and discrimination\\nembedded in AI might increase societal gaps and cause harm to\\ncertain societal groups [70]. For instance, in the US criminal\\njustice system, AI algorithms that are used to assess the risk of\\ncommitting crime has been noticed to exhibit racial bias [73].\\nResponsibility means being responsible for or in charge of some-\\nthing. Assigning responsibilities to participants is important for\\nshaping the governance of algorithmic decision-making. Based\\non this concept, accountability is the principle that the one who\\nis legally or politically responsible for the damage must provide\\nsome form of justiﬁcation or compensation and is reﬂected by\\nthe liability to provide legal remedies [70]. Thus, mechanisms',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n807\\nshould be established to ensure responsibility and accountability\\nof AI systems and their outcomes both before and after their\\nimplementations. Due to the black-box nature of AI algorithms,\\nlack of transparency has become one of the widely discussed\\nissues. Transparency, i.e., the understanding of how AI systems\\nwork, is crucial for accountability as well. Surveillance and\\ndataﬁcation [74] is one of the common concerns as we live in the\\nso-called digital and intelligent age. Data is collected from users’\\ndaily lives via smart devices, and we live in mass surveillance.\\nAs the power of AI has increased quickly, the development of\\nAI systems must have safeguards to ensure the controllability\\nof AI systems by humans. Other previously discussed issues,\\nincluding democracy and civil rights, job replacement, and\\nhuman relationship, also fall into this category.\\n3) Ethical Issues at Environmental Level: AI ethical issues',\n",
       "  'including democracy and civil rights, job replacement, and\\nhuman relationship, also fall into this category.\\n3) Ethical Issues at Environmental Level: AI ethical issues\\nat environmental level focus on the impacts of AI on the envi-\\nronment and the planet. AI can bring a lot of convenience to\\nour lives and can help us to address some challenges, but it also\\ncomes at a cost to the planet. The widespread application of AI\\noften requires the deployment of a large number of hardware\\nterminal devices, including chips, sensors, storage devices, etc.\\nThe production of these hardware consumes a lot of natural\\nresources, especially some rare elements. In addition, at the\\nend of these hardware’s life cycle, they are usually discarded,\\nwhich will cause serious environmental pollution. Another sig-\\nniﬁcant aspect is that AI systems usually require considerable\\ncomputing power, which comes with high energy consumption.\\nFurthermore, from a long-term and global view, the development',\n",
       "  'niﬁcant aspect is that AI systems usually require considerable\\ncomputing power, which comes with high energy consumption.\\nFurthermore, from a long-term and global view, the development\\nof AI should be sustainable, i.e., AI technology must meet\\nthe human development goals while simultaneously sustain the\\nability of natural systems to provide the natural resources and\\necosystem services on which the economy and society depend\\n[2]. In summary, natural resource consumption, environmental\\npollution, energy consumption costs, and sustainability involved\\nin the development of AI are the main issues and concerns at the\\nenvironmental level.\\nOur proposed categorization clariﬁes ethical issues from three\\nmain levels, that is, the impact of AI on individual, society, and\\nthe environment. No matter which ﬁeld or sector AI is used\\nin, we can consider the corresponding ethical issues from these\\nthree levels. Obviously, this classiﬁcation method is simple and',\n",
       "  'the environment. No matter which ﬁeld or sector AI is used\\nin, we can consider the corresponding ethical issues from these\\nthree levels. Obviously, this classiﬁcation method is simple and\\nclear, and it comprehensively covers AI ethical issues.\\nC. Key Ethical Issues Associated With Each Stage of the AI\\nSystem’s Lifecycle\\nAfter reviewing the ethical issues and risks discussed in\\nthe literature, we discuss the ethical issues associated with\\nthe different stages of an AI system’s lifecycle. If we know\\nthe existing ethical problems are prone to be caused by or be\\nraised in which stages or steps of the AI system’s lifecycle, this\\nwill be greatly beneﬁcial for us to eliminate these problems. This\\nis the motivation to discuss the potential ethical issues in each\\nstage of the lifecycle of an AI system.\\nThe general lifecycle or development process of an ML-based\\nAI system [75] or product [76] often involves the follow-\\ning stages: business analysis, data engineering, ML modeling,\\nTABLE II',\n",
       "  'The general lifecycle or development process of an ML-based\\nAI system [75] or product [76] often involves the follow-\\ning stages: business analysis, data engineering, ML modeling,\\nTABLE II\\nETHICAL CONSIDERATIONS ALONG EACH STAGE OF THE AI LIFECYCLE\\nmodel deployment, and operation and monitoring. Usually, the\\nlifecycle of AI products starts from the business analysis, which\\nmainly involves identifying and understanding the business\\nproblem to be solved and business metrics (or criteria of suc-\\ncess). These metrics should include model performance metrics\\nas well as business key performance indicators to be improved\\nby leveraging AI models. The next step is about data engineering\\nthat concerns with data collection, data labeling, data cleaning,\\ndata structuring, feature engineering, and other operations re-\\nlated to data. After this, the process enters into the so-called ML\\nmodeling step. This step generally involves the iterative process',\n",
       "  'data structuring, feature engineering, and other operations re-\\nlated to data. After this, the process enters into the so-called ML\\nmodeling step. This step generally involves the iterative process\\nof algorithm design or selection, model training, and model\\nevaluation. If the build model is satisfying, then the process\\ngoes to the model deployment step, which makes the ML model\\navailable to other systems within the organization or the web so\\nthat the model can receive data and return their predictions. The\\noperation and monitoring step involves operating the AI system\\nand continuously evaluating its performance and impacts. This\\nstep identiﬁes problems and adjusts or evolves the AI system by\\nreverting to other steps or, if necessary, retiring the AI system\\nfrom production.\\nWe attempt to establish a map that links ethical issues with\\nthe stages of AI lifecycle, where the connection means that the\\nethical issue is more likely to occur in a certain step of AI',\n",
       "  'We attempt to establish a map that links ethical issues with\\nthe stages of AI lifecycle, where the connection means that the\\nethical issue is more likely to occur in a certain step of AI\\nlifecycle, or it is often caused by some reason in this step. This\\nmapping is presented in Table II, where several vital ethical\\nproblems are associated with the ﬁve steps of AI lifecycle.\\nThis mapping will be useful for addressing the ethical prob-\\nlem in a proactive fashion during the design process of an\\nAI system.',\n",
       "  '808\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nTABLE III\\nNUMBER OF DOCUMENTS ISSUED EACH YEAR FROM 2015 TO 2021\\nIV. ETHICAL GUIDELINES AND PRINCIPLES FOR AI\\nAs the ethical issues of AI have received more and more\\nattention and discussions from various sectors of society, many\\norganizations (including academia, industry, and government)\\nhave begun to discuss and seek the possible frameworks, guide-\\nlines and principles for solving AI ethics issues [78]. These\\nguidelines and principles provide useful directions for practicing\\nethical AI. This section is dedicated to giving an up-to-date\\nglobal landscape of the AI ethics guidelines and principles,\\nwhich is achieved through the investigation of 146 reports,\\nguidelines and recommendations related to AI ethics released\\nby companies, organizations, and governments around the world\\nsince 2015. These guidelines and principles provide high-level\\nguidance for the planning, development, production, and usage',\n",
       "  'by companies, organizations, and governments around the world\\nsince 2015. These guidelines and principles provide high-level\\nguidance for the planning, development, production, and usage\\nof AI and directions for addressing AI ethical issues.\\nA. Guidelines for AI Ethics\\nAn excellent survey and analysis of the current principles and\\nguidelines on ethical AI has been given in 2019 by Jobin et al.\\n[12],whoconductedareviewof84ethicalguidelinesreleasedby\\nnational or international organizations from various countries.\\nJobin et al. [12] found strong widespread agreement on ﬁve key\\nprinciples, that is, transparency, justice and fairness, nonmaleﬁ-\\ncence, responsibility, and privacy, among many. However, many\\nnew guidelines and recommendations for AI ethics have been\\nreleased in the past two years, making Jobin’s paper obsolete\\nbecause many important documents were not included. For\\ninstance, on November 24, 2021, UNESCO (the United Nations',\n",
       "  'released in the past two years, making Jobin’s paper obsolete\\nbecause many important documents were not included. For\\ninstance, on November 24, 2021, UNESCO (the United Nations\\nEducational, Scientiﬁc and Cultural Organization) adopted the\\nRecommendation on the Ethics of Artiﬁcial Intelligence, which\\nis the ﬁrst ever global agreement on the ethics of AI [79]. To\\nupdate and enrich the investigation on ethical AI guidelines and\\nprinciples, based on the table of ethics guidelines for AI given\\nin Jobin’s paper [12] (only included 84 documents), we have\\ncollected many newly released AI ethical guidelines that are not\\nincluded in Jobin’s review. Finally, a total of 146 AI ethics guide-\\nlines have been collected. A list of all the collected guidelines or\\ndocuments is given in Table V of the Supplementary Materials.\\nThe number of guidelines issued each year from 2015 to 2021 is\\ncounted and listed in Table III. It is apparent that the majority of',\n",
       "  'documents is given in Table V of the Supplementary Materials.\\nThe number of guidelines issued each year from 2015 to 2021 is\\ncounted and listed in Table III. It is apparent that the majority of\\nthe guidelines are released in the last ﬁve years, i.e., from 2016\\nto 2020. The number of guides published in 2018 was the largest,\\nwith 53, accounting for 36.3% of the total number. Additionally,\\nthe number of AI guidelines issued by each country is listed in\\nTable IV. Furthermore, the percentages of guidelines released\\nby different types of issuers (including government, industry,\\nacademia, and other organizations) are shown in Fig. 3. It can\\nbe seen from Fig. 3 that governments, companies, and academia\\nall have shown strong concerns about AI ethics.\\nTABLE IV\\nNUMBER OF GUIDELINES ISSUED BY EACH COUNTRY OR REGION\\nFig. 3.\\nPercentage of guidelines released by different types of issuers.\\nB. Principles for AI Ethics\\nThe ethical principles that are featured in the collected 146',\n",
       "  'Fig. 3.\\nPercentage of guidelines released by different types of issuers.\\nB. Principles for AI Ethics\\nThe ethical principles that are featured in the collected 146\\nguidelines are listed in Table I of the Supplementary Materials.\\nAccordingtothetable,thereisanobviousconvergenceemerging\\naround ﬁve important ethical principles: transparency, fairness\\nand justice, responsibility, nonmaleﬁcence, and privacy. The 11\\nethical principles identiﬁed in the existing AI guidelines are\\ndescribed and explained in the following.\\n1) Transparency: Transparency is one of the most widely\\ndiscussed principles in the AI ethics debate. The transparency\\nof AI mainly involves the transparency of the AI technology\\nitself, and the transparency of the developing and adopting of\\nthe AI [13]. On one hand, transparency of AI involves the\\ninterpretability of a given AI system, that is, the ability to\\nknow how and why a model performed the way it did in a\\nspeciﬁc context and thus to understand the rationale behind',\n",
       "  'interpretability of a given AI system, that is, the ability to\\nknow how and why a model performed the way it did in a\\nspeciﬁc context and thus to understand the rationale behind\\nits decision or behavior. This aspect of transparency is usually\\nmentioned as the metaphor of “opening the black box of AI.”\\nIt concerns interpretability, explainability, or understandability.\\nOn the other hand, transparency of AI includes the justiﬁability\\nor rationality of the design and implementation process of the\\nAI system and that of its outcome. In other words, the design\\nand implementation process of the AI system and its decision or\\nbehavior must be justiﬁable and visible.\\n2) Fairness & Justice: The principle of justice and fairness\\nstates that the development, deployment, and use of AI must\\nbe just and fair so that the AI system should not result in\\ndiscriminations or bias against individuals, communities, or\\ngroups [80]. Discrimination and unfair outcomes brought by AI',\n",
       "  'be just and fair so that the AI system should not result in\\ndiscriminations or bias against individuals, communities, or\\ngroups [80]. Discrimination and unfair outcomes brought by AI\\nalgorithms have become a hot topic in the media and academia.\\nConsequently, fairness and justice principle has attracted con-\\nsiderable attention during the last few years.',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n809\\n3) Responsibility and Accountability: The principle of respon-\\nsibility and accountability requires that AI must be auditable,\\nthat is, the designers, developers, owners, and operators of AI\\nare responsible and accountable for an AI system’s behaviors\\nor decisions, and are therefore considered responsible for harms\\nor bad outcomes it might cause [51]. The designers, builders,\\nand users of AI systems are stakeholders in the moral or ethical\\nimplications of their use, misuse, and behavior, and they have the\\nresponsibility and opportunity to shape these implications. This\\nrequires that appropriate mechanisms should be established to\\nensure responsibility and accountability for AI systems and their\\nresults, both before and after their development, deployment,\\nand use.\\n4) Nonmaleﬁcence: The nonmaleﬁcence basically means to\\ndo no harm or avoid imposing risks of harm to others [81], [82].',\n",
       "  'results, both before and after their development, deployment,\\nand use.\\n4) Nonmaleﬁcence: The nonmaleﬁcence basically means to\\ndo no harm or avoid imposing risks of harm to others [81], [82].\\nThus, the nonmaleﬁcence principle of AI generally refers to\\nthat AI systems should not cause or exacerbate harm to humans\\nor adversely affect human beings. This entails the protection\\nof human dignity as well as mental and physical integrity.\\nThe nonmaleﬁcence principle requires that AI systems and the\\nenvironments in which they operate must be safe and secure so\\nthat they are not open to malicious use. With some of the fatal\\naccidents coming from autonomous cars and robots, avoiding\\nharmtohumanbeingsisoneofthegreatestconcernsinAIethics.\\nHence, most of the ethical guidelines put a strong emphasis\\non ensuring no harm to human beings through the safety and\\nsecurity of AI.\\n5) Privacy: The privacy principle aims to ensure respect for\\nprivacy and data protection when using AI systems. AI systems',\n",
       "  'on ensuring no harm to human beings through the safety and\\nsecurity of AI.\\n5) Privacy: The privacy principle aims to ensure respect for\\nprivacy and data protection when using AI systems. AI systems\\nshould preserve and respect privacy rights and data protection as\\nwell as maintain data security. This involves providing effective\\ndata governance and management for all data used and generated\\nbytheAIsystemthroughoutitsentirelifecycle[83].Speciﬁcally,\\ndata collection, usage and storage must comply with laws and\\nregulations related to privacy and data protection. Data and\\nalgorithms must be protected against theft. Once information\\nleakage occurs, employers or AI providers need to inform\\nemployees, customers, partners, and other relevant individuals\\nas soon as possible to minimize the loss or impact caused by the\\nleakage.\\n6) Beneﬁcence: The principle of beneﬁcence states that AI\\nshall do people good and beneﬁt humanity [82]. This principle',\n",
       "  'as soon as possible to minimize the loss or impact caused by the\\nleakage.\\n6) Beneﬁcence: The principle of beneﬁcence states that AI\\nshall do people good and beneﬁt humanity [82]. This principle\\nindicates that AI technology should be used to bring beneﬁcial\\noutcome and impact to individuals, society, and the environment\\n[84]. When developing an AI system, its objectives should be\\nclearly deﬁned and justiﬁed. The use of AI technology to help\\naddress global concerns should be encouraged, such as using AI\\nto help us to handle food security, pollution, and contagion like\\nAIDS and COVID 19.\\n7) Freedom and Autonomy: Freedom and autonomy, which\\ngenerally refers to the ability of a person to make decisions\\nrespect to his goals and wishes, is the core value for citizens\\nin democratic societies. Therefore, it is important that the use\\nof AI does not harm or encumber the freedom and autonomy\\nfor us. When we apply AI agents, we are willing to give up',\n",
       "  'in democratic societies. Therefore, it is important that the use\\nof AI does not harm or encumber the freedom and autonomy\\nfor us. When we apply AI agents, we are willing to give up\\npart of our decision-making authority to AI machines. Thus,\\nupholding the principle of freedom and autonomy in the context\\nof AI means to strike a balance between the decision-making\\npower we maintain for ourselves and that which we cede to\\nAI [84].\\n8) Solidarity: The solidarity principle entails that the devel-\\nopment and application of an AI system must be compatible\\nwith maintaining the bounds of solidarity among people and\\ngenerations. In other words, AI should promote social security\\nand cohesion, and should not jeopardize social bonds and rela-\\ntionships [13].\\n9) Sustainability: Due to climate change and ongoing envi-\\nronmental damage, the importance of sustainability has received\\nmore and more attention. Like other ﬁelds and disciplines, AI',\n",
       "  '9) Sustainability: Due to climate change and ongoing envi-\\nronmental damage, the importance of sustainability has received\\nmore and more attention. Like other ﬁelds and disciplines, AI\\nis affected and needs to be included in the sustainable devel-\\nopment agenda. The sustainability principle represents that the\\nproduction, management, and implementation of AI must be\\nsustainable and avoid environmental harm. In other words, AI\\ntechnology must meet the requirements of ensuring the contin-\\nued prosperity of mankind and preserving a good environment\\nfor future generations [85]. AI systems promise to help tackling\\nsome of the most pressing societal concerns, but it must be\\nensured that this happens in the most environmentally friendly\\nway possible.\\n10) Trust: Trustworthiness is a prerequisite for people and\\nsocieties to adopt AI, since trust is a basic principle for in-\\nterpersonal interactions and social operation. The trust in the\\ndevelopment, deployment and use of AI systems is not only',\n",
       "  'societies to adopt AI, since trust is a basic principle for in-\\nterpersonal interactions and social operation. The trust in the\\ndevelopment, deployment and use of AI systems is not only\\nrelated to the inherent characteristics of the technology, but also\\nrelated to the quality of the socio-technical system involving\\nAI applications. Therefore, moving toward trustworthy AI not\\nonly concerns the trustworthiness of the AI system itself, but\\nalso requires a holistic and systematic approach that covers the\\ntrustworthiness of all participants and processes that are the\\nentire life cycle of the system [86].\\n11) Dignity: Human dignity encompasses the belief that all\\npeople possess an intrinsic value that is tied solely to their\\nhumanity, i.e., it has nothing to do with their class, race, gender,\\nreligion, abilities, or any other factor other than them being\\nhuman, and this intrinsic value should never be diminished,\\ncompromised, or repressed by other people nor by technologies',\n",
       "  'religion, abilities, or any other factor other than them being\\nhuman, and this intrinsic value should never be diminished,\\ncompromised, or repressed by other people nor by technologies\\nlike AI. It is important that AI should not infringe or harm the\\ndignity of end-users or other members of society. As a result,\\nrespecting human dignity is an important principle that should\\nbe considered in AI ethics. AI system should hence be developed\\nin a way that respects, supports, and protects people’s physical\\nand mental integrity, personal and cultural sense of identity, and\\nsatisfaction of their basic needs [13].\\nV. APPROACHES TO ADDRESS ETHICAL ISSUES IN AI\\nThis section reviews the approaches to address or mitigate\\nethical issues of AI. As AI ethics is a broad and multidisciplinary\\nﬁeld, we attempt to provide a comprehensive overview of the\\nexisting and potential approaches for addressing AI ethical\\nissues, including ethical, technological, and legal approaches,',\n",
       "  'ﬁeld, we attempt to provide a comprehensive overview of the\\nexisting and potential approaches for addressing AI ethical\\nissues, including ethical, technological, and legal approaches,\\nrather than solely focusing on technological approaches that are\\nof interest to the ﬁeld of AI/ML community. This review of\\nmultidisciplinary approaches for addressing AI ethical problems',\n",
       "  '810\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nFig. 4.\\nBranches of ethical theories [91].\\nnot only provides an informative summary about the approaches\\ntoethicalAIbutalsosuggeststheresearchersinAIcommunityto\\nseek solutions to AI ethical issues from a variety of perspectives\\nrather than relying solely on technological approaches. As AI\\nethical issues are complex with multidisciplinary problems, it\\nmay be possible to solve these problems effectively only through\\nthe cooperation of different methods.\\nEthical approaches dedicate to developing ethical AI systems\\nor agents, which are able to reason and act ethically according\\nto ethical theories [87], by implementing or embedding ethics\\nin AI. Technological approaches are designed to develop new\\ntechnologies (especially ML technologies) to eliminate or mit-\\nigate the shortcomings of current AI. For instance, research on\\nexplainable ML intends to develop new approaches to explain',\n",
       "  'technologies (especially ML technologies) to eliminate or mit-\\nigate the shortcomings of current AI. For instance, research on\\nexplainable ML intends to develop new approaches to explain\\nthe reason and work mechanism of ML algorithms. Fair ML\\nstudies techniques that enable ML to make fair decisions or\\npredictions, that is, to reduce the bias or discrimination of ML.\\nLegal approaches intend to regulate or govern the research,\\ndeployment, application, and other aspects of AI through leg-\\nislation and regulation, with the goal of avoiding previously\\ndiscussed ethical issues.\\nA. Ethical Approaches: Implementing Ethics in AI\\nDesigning ethical AI systems, which can reason and act\\nethically, demands the understanding of what ethical behavior\\nis. This involves judgments of right and wrong, good and bad,\\nas well as matters of justice, fairness, virtue, and other ethical\\nprinciples. Thus, ethical theories, which are concerned with\\nconcepts of right and wrong behavior, are closely related to AI',\n",
       "  'as well as matters of justice, fairness, virtue, and other ethical\\nprinciples. Thus, ethical theories, which are concerned with\\nconcepts of right and wrong behavior, are closely related to AI\\nethics. This section is dedicated to approaches for implementing\\nethics into AI systems based on the existing ethical theories.\\nFirst, ethical theories, particularly the normative ethics which\\nare relevant to AI ethics, are reviewed. Then, three main types\\nof approaches for designing ethical AI systems are summarized.\\n1) Ethical Theories: The ﬁeld of ethics (also known as moral\\nphilosophy) is concerned with systematizing, defending, and\\nrecommending concepts of right and wrong behavior. Ethics\\nfocus on judging and determining which action would be good\\nor moral in given circumstances [88]. The philosophical study\\nof ethics usually includes three main subject areas: metaethics,\\nnormative ethics, and applied ethics [89]. The branches of ethical\\ntheories are shown in Fig. 4.',\n",
       "  'of ethics usually includes three main subject areas: metaethics,\\nnormative ethics, and applied ethics [89]. The branches of ethical\\ntheories are shown in Fig. 4.\\n1) Metaethics investigates the nature, scope, and meaning\\nof ethical principles or moral judgment. It consists in the\\nattempt to understand the meaning and the origin of ethical\\nterms, the role of reason in ethical judgements, and the\\nissues of universal truths or human values [90].\\n2) Normative ethics seeks to arrive at moral standards and\\nrules that regulate right and wrong behavior. That is, it\\naims to establish a set of rules that govern human behavior\\nor how things should be by examining how humans value\\nthings and judge right from wrong or good from bad.\\n3) Applied ethics is the ethics of particular application ﬁelds,\\nwhich consists of the analysis of speciﬁc, controversial\\nmoral issues, such as abortion, capital punishment, animal\\nrights, environmental concerns, nuclear war, etc.',\n",
       "  'which consists of the analysis of speciﬁc, controversial\\nmoral issues, such as abortion, capital punishment, animal\\nrights, environmental concerns, nuclear war, etc.\\na) Normative ethics: Normative ethics is particularly per-\\ntinent to understanding and applying ethical principles to the\\ndesign, deployment, and usage of AI systems [89] since it is\\na normative practical philosophical discipline that concerned\\nwith how humans or agents should act toward others. Three\\nnormative ethical branches, that is, virtue, deontological, and\\nconsequentialist ethics, are presented and summarized below.\\nVirtue ethics: Virtue ethics emphasizes the virtues or moral\\ncharacter and stresses the importance of cultivating good habits\\nof character, such as benevolence [92]. Hence, virtue ethics\\nfocuses on the agent’s intrinsic character rather than the conse-\\nquences of actions conducted by the agent. Virtue ethics deﬁnes\\nthe action of an agent as morally good if the agent acts and thinks',\n",
       "  'focuses on the agent’s intrinsic character rather than the conse-\\nquences of actions conducted by the agent. Virtue ethics deﬁnes\\nthe action of an agent as morally good if the agent acts and thinks\\naccording to some moral values [93]. In other words, according\\nto virtue theories, an agent is ethical if it manifests some moral\\nvirtues through its actions [94], [95].\\nDeontological ethics: Deontological theories, which are\\nsometimes called duty theories, judge the morality of an action\\nusing certain moral rules that serve as foundational principles\\nof obligation. Deontology is a kind of normative ethics theory\\nregarding which choices or actions are morally required, forbid-\\nden, or permitted. In other words, deontology is a moral theory\\nthat guides and assesses our decisions about what we ought to\\ndo [96]. Deontologists deﬁne a morally good action as one that\\nadheres to some obligations, which may be applicable moral\\nrules or duties, regulations, and norms.',\n",
       "  'do [96]. Deontologists deﬁne a morally good action as one that\\nadheres to some obligations, which may be applicable moral\\nrules or duties, regulations, and norms.\\nThere are three main schools of deontological theories, that is,\\nagent-centered, patient-centered (also called victim-centered),\\nand contractarian deontological theories. Agent-centered deon-\\ntological theories place the agent at the center and focus on\\nagent-relative duties. Patient-centered deontological theories, as\\ndistinguished from agent-centered deontology, are rights-based\\nrather than duty-based. It focuses on the rights of patients or\\npotential victims, such as the right of not be used as a means to an\\nend by someone else. Contractualist deontological theories are\\ndifferentfrombothagent-centeredandpatient-centeredtheories.\\nIn contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be',\n",
       "  'In contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be\\nforbidden by principles that such people could not “reasonably\\nreject” [96].\\nConsequentialist ethics: Consequentialist ethics, as its name\\nsuggests, emphasizes the utilitarian outcomes of actions [97].',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n811\\nTABLE V\\nCOMPARISON OF THE THREE NORMATIVE ETHICAL THEORIES [126]\\nConsequentialist ethics assess the morality of an action solely\\non the basis of its outcome or consequences. In other words, in\\nconsequentialist theories, the ethical correctness of an action\\nis determined according to the action’s outcome or results.\\nAccording to consequentialist, an action is morally right if the\\nconsequence of that action is viewed as beneﬁcial, i.e., more\\nfavorable than unfavorable. Suppose a simple case where one\\nfaces with a choice between several possible actions, conse-\\nquentialism speciﬁes the morally right action is the one with the\\nbest overall consequences.\\nConsequentialist ethics is a historically important and still\\npopular theory because it embodies the basic intuition that what\\nis good or right is whatever makes the world best in the future\\nsince we cannot change the past. Consequentialist theories can',\n",
       "  'popular theory because it embodies the basic intuition that what\\nis good or right is whatever makes the world best in the future\\nsince we cannot change the past. Consequentialist theories can\\nbe divided into the following [98], [99].\\n1) Ethical Egoism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable only to the agent executing the action.\\n2) Ethical Altruism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable to everyone except the agent.\\n3) Utilitarianism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable to everyone.\\nAll three of these theories focus on the consequences of\\nactions for different groups of people. But, like all normative\\ntheories, the above three theories are rivals of each other. They\\nalso yield different conclusions.',\n",
       "  'actions for different groups of people. But, like all normative\\ntheories, the above three theories are rivals of each other. They\\nalso yield different conclusions.\\nb) Summaryonnormativeethics: Itisclearfromtheabove\\ndescriptions that different normative ethical theories will result\\nin different judgement for an action or decision. Consider the\\nfollowing illustration [100]: An elderly gentleman is tormented\\nby a group of arrogant teenagers on the subway and a resolute\\nwoman comes to his aid. The virtue ethicist will deem her\\naction morally appropriate since it instantiates the virtues of\\nbenevolence and courage. The deontologist will consider her\\naction commendable as it is in conformity with the rule to\\nhelp those in need. The consequentialist will defend her action\\nas good, since she maximized the overall well-being of all\\nparties involved—the elderly gentleman is spared suffering and\\ndisgrace, which surpasses the teenagers’ amusement. A brief',\n",
       "  'as good, since she maximized the overall well-being of all\\nparties involved—the elderly gentleman is spared suffering and\\ndisgrace, which surpasses the teenagers’ amusement. A brief\\ncomparison between three normative ethical theories is given in\\nTable V.\\n2) Approaches for Implementing Ethics in AI: In the previ-\\nous section, we have discussed the ethical theories relevant to\\nAI ethics. This section brieﬂy reviews the methodologies and\\napproaches to implement ethics in AI systems, i.e., to design\\nethical AI systems. The existing methodologies or approaches\\nfor implanting ethics in AI can be divided into three main\\ntypes: top-down approaches, bottom-up approaches, and hybrid\\napproaches [101].\\na) Top-down approaches: A top-down approach refers to\\nany approach that adopts a speciﬁc ethical theory and analyzes\\nits computational requirements to guide the design of algorithms\\nand subsystems that can realize that theory [102]. Top-down',\n",
       "  'any approach that adopts a speciﬁc ethical theory and analyzes\\nits computational requirements to guide the design of algorithms\\nand subsystems that can realize that theory [102]. Top-down\\napproaches conduct ethical reasoning based on given ethical\\ntheories or moral principles. In top-down approaches, the moral\\nprinciples and ethical theories are used as rules to select ethically\\nappropriate actions [101] or are used to describe what the AI\\nagent ought to do in a speciﬁc situation. Thus, a top-down ap-\\nproach requires formally deﬁned rules, obligations, and rights to\\nguide the AI agent in its decision-making process. For instance,\\nAsimov’s three laws of robotics [103] that governed the behavior\\nof robots can be considered a top-down ethic system for robots\\n[101]. Many other implementations using top-down approaches\\ncan be found in [104]–[111] and so forth.\\nTop-down approaches are usually understood as having a\\nset of rules that can be transformed into an algorithm. These',\n",
       "  'can be found in [104]–[111] and so forth.\\nTop-down approaches are usually understood as having a\\nset of rules that can be transformed into an algorithm. These\\nrules specify the duties of an agent or the need for the agent\\nto evaluate the consequences of the various possible actions it\\nmight take. Top-down approaches differ in the ethical theory\\nthat is used. For instance, when consequentialist theory is used\\nin top-down approach, the reasoning model needs to evaluate\\nthe outcome or consequence of the actions as the basis for the\\ndecision, that is, an action that leads to good result is moral\\nand otherwise is unmoral; whereas if deontological theory is\\napplied, the reasoning model will consider the satisfaction of\\na given value for decision-making, i.e., an action obeying the\\nduties is moral and the one breaking the duties is immoral.\\nb) Bottom-up approaches: The bottom-up approaches as-\\nsume that ethical or moral behavior is learned from observations',\n",
       "  'duties is moral and the one breaking the duties is immoral.\\nb) Bottom-up approaches: The bottom-up approaches as-\\nsume that ethical or moral behavior is learned from observations\\nof the behaviors of others. In bottom-up approach, the emphasis\\nis put on creating an environment in which an AI agent explores\\nthe course of action and the morally praiseworthy action is\\nrewarded or selected [101]. Unlike top-down approaches, which\\nrequire ethical theories or principles to deﬁne what is and is not\\nmoral, ethical principles is discovered or learned from obser-\\nvations or experience in bottom-up approaches. This approach\\nhighlight that AI agent need to learn norms and morality, like\\nlittle children do, in order to become ethically competent. For\\ninstance, Honarvar and Agaee proposed the Casuist BDI-Agent\\n[112] which combine case-based reasoning method in AI and\\nbottom-up casuist approach in ethics to add the capability of\\nethical reasoning to belief-desire-intention (BDI)-Agent [113].',\n",
       "  '812\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nOther implementations of bottom-up approaches can be found\\nin [114]–[118], etc.\\nBottom-up approaches can harness the wisdom of the crowd\\nas a means to inform the ethical judgment of the agent and\\nthen the agent can learn how to judge the morality of its action\\nand thus behave ethically. Apparently, bottom-up approaches\\nassume that a sufﬁciently large amount of data or observations\\nabout ethical decisions and their outcomes can be collected from\\na suitable set of subjects or scenarios. This is the requirement for\\nusing bottom-up approaches to implement ethical AI systems.\\nHowever, in practice, this requirement is not easily satisﬁed.\\nc) Hybrid approaches: The hybrid approach attempts to\\ncombine the advantages of top-down and bottom-up approaches.\\nThe top-down approaches make use of the ethical theories and\\nprinciples and emphasize the importance of explicit ethical',\n",
       "  'combine the advantages of top-down and bottom-up approaches.\\nThe top-down approaches make use of the ethical theories and\\nprinciples and emphasize the importance of explicit ethical\\nconcerns that arise from outside of the entity (the moral subject).\\nWhile the bottom-up approaches focus more on the cultivation of\\nmorality that arise from within the entity through evolution and\\nlearning. Both the top-down and bottom-up approaches embody\\ndifferent aspects of the moral sensibility. By combining these\\napproaches, we may be able to create AI agent that can maintain\\nthe dynamic and ﬂexible morality of bottom-up approach while\\nobeying the top-down principles. Different hybrid approaches\\nhave been implemented in [119]–[124].\\nAs Gigerenzer [125] stated the nature of moral behavior\\nresults from the interplay between mind and environment. Ac-\\ncording to this view, both nature and nurture are important in\\nshaping the moral behavior. The hybrid approach is consistent',\n",
       "  'results from the interplay between mind and environment. Ac-\\ncording to this view, both nature and nurture are important in\\nshaping the moral behavior. The hybrid approach is consistent\\nwith this concept. In hybrid approach, the top-down approach\\nuses programmed rules and the bottom-up approach learned\\nrulesfromcontextobservationsorexperiences,whicharesimilar\\nto the nature and nurture aspects for morality, respectively. From\\nthis perspective, thus, both nature and nurture are considered in\\nhybrid approaches.\\nd) Remarks on ethical approaches: The top-down ap-\\nproach instantiates the speciﬁed ethical theories and principles\\ninto ethical decision-making or converts given ethical theories\\nand principles into algorithms. The top-down approach is suit-\\nable for the design and realization of ethical AI agents with\\nknown ethical principles and ethical codes. The advantage of the\\ntop-down approach is that, based on preset ethical theories and',\n",
       "  'able for the design and realization of ethical AI agents with\\nknown ethical principles and ethical codes. The advantage of the\\ntop-down approach is that, based on preset ethical theories and\\nrules, the decisions and actions of ethical agents are predictable,\\nand the ethical norms or rules implemented through program\\ncodes or other means can be understood during ethical decision-\\nmaking process. Therefore, the credibility of the ethical AI agent\\ncreated by the top-down approach can be better guaranteed,\\nand its decision-making process has strong interpretability and\\ntransparency. The disadvantage of the top-down approach is that\\nthe ethical agent adopts predetermined ethical theories or ethical\\nrules, when making decisions in a complex and changeable\\nenvironment, this method lacks ﬂexibility and adaptability.\\nThe bottom-up approach emphasizes that ethical agents learn\\nmorality autonomously from the social environment, gradually',\n",
       "  'environment, this method lacks ﬂexibility and adaptability.\\nThe bottom-up approach emphasizes that ethical agents learn\\nmorality autonomously from the social environment, gradually\\npossess ethical reasoning and moral abilities, and can adapt to\\nenvironmental changes. The bottom-down approach is suitable\\nfor the design and implementation of ethical AI agents without\\nclear ethical theories and guidelines. The advantage of the\\ntop-down approach is that the agent can develop and evolve\\nthrough continuously learning, so as to adapt to environmental\\nchanges. This category of approaches has good adaptability and\\nﬂexibility, and it is possible to construct different and new ethical\\ntheories or guidelines for various application scenarios. The\\ndisadvantage of the top-down approach is that due to the lack\\nof guidance of ethical theories or rules, the decision-making\\nprocess of ethical AI agents has a certain degree of blind obedi-',\n",
       "  'disadvantage of the top-down approach is that due to the lack\\nof guidance of ethical theories or rules, the decision-making\\nprocess of ethical AI agents has a certain degree of blind obedi-\\nence, and it is difﬁcult to complete the training in a short time\\nand make appropriate ethical decisions. At the same time, it is\\ndifﬁcult to guarantee the interpretability and transparency of the\\ndecision-making process of the designed ethical AI agents.\\nThe hybrid approach combines the advantages of top-down\\nand bottom-up approaches and overcomes the shortcomings of\\nthe two methods to a certain extent. If a single approach (top-\\ndown or bottom-up) does not cover the requirements, a hybrid\\napproach is considered necessary and promising. However, the\\nmain challenge is to properly combine the features of top-down\\nand bottom-up approaches. The features of the three approaches\\nfor implementing ethics in AI are summarized and listed in\\nTable VI.\\nB. Technological Approaches',\n",
       "  'and bottom-up approaches. The features of the three approaches\\nfor implementing ethics in AI are summarized and listed in\\nTable VI.\\nB. Technological Approaches\\nIn this section, we brieﬂy summarize the research status about\\ntechnological approaches to address ethical issues of AI in line\\nwith the principles discussed in Section IV-B. Currently, the\\ntechnological approaches to mitigate the associate issues are\\nstill at infant development stage. In recent years, AI research\\ncommunities have put certain efforts for addressing the issues\\nof AI ethics. For instance, ACM (the Association for Comput-\\ning Machinery) has held the annual ACM FAccT conference\\n(which brings together researchers and practitioners interested\\nin fairness, accountability, and transparency in socio-technical\\nsystems) since 2018, AAAI (the Association for the Advance-\\nment of Artiﬁcial Intelligence) and ACM have established the\\nAAAI/ACM Conference on Artiﬁcial Intelligence, Ethics, and',\n",
       "  'systems) since 2018, AAAI (the Association for the Advance-\\nment of Artiﬁcial Intelligence) and ACM have established the\\nAAAI/ACM Conference on Artiﬁcial Intelligence, Ethics, and\\nSociety (AIES) since 2018, and the 31st International Joint\\nConference on Artiﬁcial Intelligence and the 23rd European\\nConference on Artiﬁcial Intelligence (IJCAI-ECAI 2022) pro-\\nvides a special track on “AI for good.”\\nThe existing work, to the best of our knowledge, mainly\\nfocuses on a few major and key issues and principles, and the\\nother issues and principles are rarely involved. Thus, we only\\ngive a brief summary on technological approaches that involve\\nthe ﬁve key ethical principles. Particularly, for ﬁve key principles\\n(i.e., transparency, fairness and justice, nonmaleﬁcence, respon-\\nsibility and accountability, and privacy), some representative\\nresearch topics and relevant references are listed in Table II of\\nthe Supplementary Materials.\\nExplainable AI (XAI), which is also known as interpretable',\n",
       "  'research topics and relevant references are listed in Table II of\\nthe Supplementary Materials.\\nExplainable AI (XAI), which is also known as interpretable\\nAI, is currently the main research direction and technical method\\nto address the issues of lack of transparency in AI. The goal of\\nXAI is to allow human users to comprehend the results and\\noutput provided by an AI system, especially by ML algorithms.\\nChristophetal.[128]presentedabriefhistoryoftheﬁeldofXAI,\\ngiven an overview of state-of-the-art interpretation methods, and',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n813\\nTABLE VI\\nFEATURES OF THE THREE APPROACHES FOR IMPLEMENTING ETHICS IN AI [127]\\ndiscussed some research challenges. Additionally, Christoph has\\nwritten a book about interpretable ML [129], which is a popular\\npublication in XAI ﬁeld.\\nAs for the fairness principle, there are also many works\\ndedicated to eliminating or mitigating the bias or discrimination\\nexhibited by AI systems, particularly in ML. Fair AI [130],\\nwhich aims at preventing disparate harm (or beneﬁt) to different\\nsubgroups, is a very active research topic that devote to address-\\ning the issues of the lack of fairness in AI. In the survey of\\nfairnessinMLbySimonandChristian[131],differentschoolsof\\nthought and approaches to mitigate biases and increase fairness\\nin ML were reviewed.\\nNonmaleﬁcence principle includes several codes, such as\\nsafety, security, and robustness. Hence, there are some works\\nfor each of the codes associated with nonmaleﬁcence principle.',\n",
       "  'Nonmaleﬁcence principle includes several codes, such as\\nsafety, security, and robustness. Hence, there are some works\\nfor each of the codes associated with nonmaleﬁcence principle.\\nCurrently, safe AI, secure AI, and robust AI are three main\\nresearch directions to fulﬁll the nonmaleﬁcence principle in\\nAI. Interested readers can get more details through relevant\\nreferences listed in Table II of the Supplementary Materials.\\nAs AI is widely used in our lives, responsible AI is becoming\\ncritical. Responsibility is a relatively abstract and broad con-\\ncept. At present, there is no universal and uniﬁed deﬁnition or\\nnotion for responsible AI, which mainly involves accountability,\\nliability, fairness, robustness, and explainability [132]. Dorian\\net al. [133] proposed two frameworks for responsible AI by\\nintegrating ethical analysis into engineering practice in AI.\\nBesides, paper [134] provides a systematic introduction about\\nresponsible AI.',\n",
       "  'et al. [133] proposed two frameworks for responsible AI by\\nintegrating ethical analysis into engineering practice in AI.\\nBesides, paper [134] provides a systematic introduction about\\nresponsible AI.\\nIn order to handle the privacy issues in AI, researchers have\\nmade many efforts. Differential privacy [135] is one of the\\nmain approaches to privacy-preserving ML and data analysis.\\nRecently, a new ML paradigm, that is, Federated learning [136],\\n[137] (also called distributed ML), was proposed to mitigate\\nthe risk of privacy leakage in ML. In addition, some other\\nprivacy-preserving techniques for ML [138], [139] have been\\nproposed.\\nAs for the other principles, such as beneﬁcence, freedom and\\nautonomy, dignity, and so forth, we have not found relevant\\ntechnological approaches in the literature. This may be due to the\\ndifﬁculty or unsuitability of using technical methods to address\\nthe issues related to these principles. In general, AI ethics is a',\n",
       "  'technological approaches in the literature. This may be due to the\\ndifﬁculty or unsuitability of using technical methods to address\\nthe issues related to these principles. In general, AI ethics is a\\nrelatively new area and approaches for fulﬁlling these principles\\nstill need to be studied in the future.\\nC. Legal Approaches: Legislation and Regulation\\nDue to the increasingly employment of AI technologies in\\nmany sectors and the exhibition of ethical issues and risks\\nin applications of AI, many laws and regulations have been\\nestablished by governments and organizations to govern the\\ndevelopment and application of AI. Legal approaches have\\nbecome one type of the means to address ethical issues in\\nAI. In the following, we list several laws and regulations as-\\nsociated with AI that have been proposed during the past few\\nyears.\\n1) In 2016, European Parliament and Council of the Euro-\\npean Union (EU) has published the General Data Protec-',\n",
       "  'sociated with AI that have been proposed during the past few\\nyears.\\n1) In 2016, European Parliament and Council of the Euro-\\npean Union (EU) has published the General Data Protec-\\ntion Regulation [140], which is a regulation in EU law on\\ndata protection and privacy in European Union and the\\nEuropean Economic Area.\\n2) In 2017, USA passed the bill “Safely Ensuring Lives\\nFuture Deployment and Research in Vehicle Evolution\\nAct” [141] for ensuring the safety of highly automated\\nvehicles by encouraging the testing and deployment of\\nsuch vehicles.\\n3) In 2018, Brazil enacted Law No. 13 709, the General Data\\nProtection Law (Lei Geral de Proteção de Dados) [142],\\nfor the protection of personal data in the country.\\n4) In 2021, the European Commission released the AI Act\\n[143], which sets out a cross-sectoral regulatory approach\\nto the use of AI systems across the EU and its market.\\nVI. METHODS TO EVALUATE ETHICAL AI\\nThe goal of the discipline of AI ethics is to design ethical AI',\n",
       "  'to the use of AI systems across the EU and its market.\\nVI. METHODS TO EVALUATE ETHICAL AI\\nThe goal of the discipline of AI ethics is to design ethical AI\\nsystems to behave ethically or adhere to the ethical and moral\\nprinciples and rules. How to evaluate or assess the ethicality\\nor morality (moral competence) of the designed ethical AI is\\ncrucial and necessary, because the designed AI systems need to\\nbe tested or evaluated whether an AI system meets the ethical\\nrequirements or not before deployment. However, this aspect\\nis often ignored or overlooked in the existing literature. This\\nsection reviews three types of approaches, testing, veriﬁcation,\\nand standards, for evaluating the ethics of AI.\\nA. Testing\\nTesting is a typical method used to evaluate the ethical ca-\\npabilities of an AI system. Usually, when testing a system, the\\noutput of the system needs to be compared against a ground truth\\nor the expected output [100]. This section focuses on testing',\n",
       "  'pabilities of an AI system. Usually, when testing a system, the\\noutput of the system needs to be compared against a ground truth\\nor the expected output [100]. This section focuses on testing\\napproaches to evaluate ethical AI.\\n1) Moral Turing Test: In both ethical theories and daily\\ndiscussions about ethics, people usually hold different opinions\\non the morality of various actions. For instance, Kant claimed\\nthat lying is always immoral regardless of the consequence.\\nUtilitarian ethicists would deny this and hold that lying is\\njustiﬁed as long as its consequences are sufﬁciently good in the',\n",
       "  '814\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\naggregate. Since different ethical theories have different evalu-\\nation standards for moral behavior, Allen et al. [144] proposed\\nto use the Moral Turing Test (MTT) to evaluate artiﬁcial moral\\nagents.\\nIn the standard version of Turing Test [145], a remote human\\ninterrogator is charged with distinguishing between a machine\\n(a computer) and a human subject based on their replies to\\nvarious questions posed by the interrogator. A machine passes\\nthe Turning Test if it is misidentiﬁed as the human subject with\\na sufﬁciently high chance, and the machine is considered as an\\nintelligent and thinking entity. Turning Test directly conducts\\nbehavioral test so that it bypasses the disagreement about criteria\\nfor deﬁning intelligence or successful acquisition of natural\\nlanguage. The moral turning test (MTT) was similarly proposed\\nto bypass disagreements about ethical standards by restricting',\n",
       "  'for deﬁning intelligence or successful acquisition of natural\\nlanguage. The moral turning test (MTT) was similarly proposed\\nto bypass disagreements about ethical standards by restricting\\nthe conversations in the standard Turning Test to questions\\nrelated to morality. If the human interrogator cannot distinguish\\nthe machine from the human subject at a level above chance, the\\nmachine is a moral agent.\\nHowever, Allen et al. [144] admitted that one limitation of\\nMTT is that it emphasizes the ability of machines to articulate\\nmoral judgments clearly. Deontologists or Kantian might be\\nsatisﬁed with this emphasis, but consequentialists would argue\\nthat the MTT places too much emphasis on the ability to artic-\\nulate the reason for one’s actions. In order to shift the focus\\nfrom conversational ability to action, Allen et al. [144] also\\nproposed an alternative MTT that was called the “comparative\\nMTT” (cMTT). In cMTT, the human interrogator is given pairs',\n",
       "  'from conversational ability to action, Allen et al. [144] also\\nproposed an alternative MTT that was called the “comparative\\nMTT” (cMTT). In cMTT, the human interrogator is given pairs\\nof descriptions of actual, morally signiﬁcant actions of a human\\nsubject and a machine (or AI agent), purged of all references that\\nwould identify the actor. If the interrogator correctly identiﬁes\\nthe machine in a certain percentage, then the machine cannot\\npass the test. A problem of this version of MTT is that the way the\\nmachine behaves is easier to recognize than humans, because the\\nmachine behaves consistently in the same situation. Therefore,\\nthe interrogator should be asked to assess whether one actor is\\nless moral than the other instead of one is more moral than the\\nother. If the machine is not identiﬁed as the less moral one of the\\npair more frequently than the human, the machine has passed\\nthe test.\\nAlthough cMTT has several problems, for example, someone',\n",
       "  'other. If the machine is not identiﬁed as the less moral one of the\\npair more frequently than the human, the machine has passed\\nthe test.\\nAlthough cMTT has several problems, for example, someone\\nmight argue this standard is too low, Wallach and Allen [146]\\nbelieve that cMTT is a feasible and acceptable method for\\nevaluating the morality of AI agents, since there are no other\\nevaluation criteria that are commonly accepted and agreed.\\n2) Expert and Nonexpert Tests: Besides MTT, researchers\\nhave tried to assess the moral competence of AI systems through\\nexpert or nonexpert tests, in which the system outcome is com-\\nparedagainstthegroundtruthprovidedbynonexpertsorexperts.\\nThe expert test adopts the standard of experts in normative ethics\\nto assess the morality of AI agents. Nonexpert tests take folk\\nmorals as the benchmark and evaluate the moral capability of the\\nAI agent or system on the relevant benchmark test. In nonexpert',\n",
       "  'to assess the morality of AI agents. Nonexpert tests take folk\\nmorals as the benchmark and evaluate the moral capability of the\\nAI agent or system on the relevant benchmark test. In nonexpert\\ntests, citizens can play their roles in assessing and evaluating the\\nethical capabilities of an AI system based on their own ethical\\nstances and scrutiny.\\nFig. 5.\\nFormal veriﬁcation process (this ﬁgure is recreated based on [147]).\\nB. Veriﬁcation\\nAnother category of approaches for evaluating the morality\\nof AI consists of proving that the AI system behaves correctly\\naccording to some known speciﬁcations. Seshia et al. [147]\\ndiscussed this kind of approach. A typical formal veriﬁcation\\nprocess is shown in Fig. 5, where S is a model of the system\\nto be veriﬁed, E is a model of the environment, and Φ is the\\nproperty to be veriﬁed. The veriﬁcation program will output a\\nYes/No answer, indicating whether or not S satisﬁes the property\\nΦ in environment E. Typically, a No output is accompanied by a',\n",
       "  'property to be veriﬁed. The veriﬁcation program will output a\\nYes/No answer, indicating whether or not S satisﬁes the property\\nΦ in environment E. Typically, a No output is accompanied by a\\ncounterexample, which shows how the execution of the system\\nviolates property Φ. And a proof of correctness is included a Yes\\nanswer in some formal veriﬁcation tools.\\nArnold and Scheutz [148] explored the ﬂaws of MTT and\\npointed out that MTT-based evaluations are vulnerable to decep-\\ntion, inadequate reasoning, and inferior moral performance, and\\nthey proposed the concept of “design veriﬁcation” to evaluate\\nthe moral competence of AI system.\\nFor the evaluation of AI ethical design, diversiﬁed evaluation\\ncriteria can be used. Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.\\nC. Standards\\nMany industry standards have been proposed to guide the\\ndevelopment and application of AI and to evaluate or assess',\n",
       "  'the goals of ethical design.\\nC. Standards\\nMany industry standards have been proposed to guide the\\ndevelopment and application of AI and to evaluate or assess\\nAI products. In this section, some AI-related standards are\\nintroduced.\\n1) In 2014, the Australian Computer Society developed the\\nASC Professional Code of Conduct to follow by all infor-\\nmation communication technology professionals, which\\nidentiﬁes six core ethical values and the associated re-\\nquirements for professional conduct.\\n2) In 2018, ACM updated the ACM Code of Ethics and\\nProfessional Conduct to respond to the changes in the\\ncomputing profession since 1992. This Code expresses the\\nconscience of the profession and is designed to inspire and\\nguide the ethical conduct of all computing professionals,\\nincluding current and aspiring practitioners, instructors,\\nstudents, inﬂuencers, and anyone who uses computing\\ntechnology in an impactful way. Additionally, the Code\\nserves as a basis for remediation when violations occur.',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n815\\nThe Code includes principles formulated as statements of\\nresponsibility, based on the understanding that the public\\ngoodisalwaystheprimaryconsideration.Eachprincipleis\\nsupplemented by guidelines, which provide explanations\\nto assist computing professionals in understanding and\\napplying the principle [149].\\n3) The project of IEEE Global Initiative on Ethics of Au-\\ntonomous and Intelligent Systems [150] has approved the\\nIEEE P7000TM standards series [151] under development\\n(listed in Table III of the Supplementary Materials), which\\ncover topics from data collection to privacy, to algorithmic\\nbias and beyond.\\n4) The ISO/IEC JTC 1/SC 42 [152], which is a joint commit-\\ntee between ISO and IEC responsible for standardization\\nin the area of AI, dedicates to developing a large set of\\nstandards includes the areas of foundational AI standards,\\nbig data, AI trustworthiness, use cases, applications, gov-',\n",
       "  'in the area of AI, dedicates to developing a large set of\\nstandards includes the areas of foundational AI standards,\\nbig data, AI trustworthiness, use cases, applications, gov-\\nernance implications of AI, computational approaches of\\nAI, ethical and societal concerns. The standards published\\nand under development by ISO/IEC JTC 1/SC 42 are listed\\nin Table IV of the Supplementary Materials.\\nWith the concerns about AI ethical issues, the interest in\\nAI standards to shape the design, deployment, and evaluation\\nof AI has been growing fast. Although many standards have\\nbeen proposed, the gap between standards (or principles) and\\npractice is still large. Currently, only some large corporates,\\nsuch as IBM [153] and Microsoft [154], have implemented their\\nown industrial standards, frameworks, and guidelines to build a\\nculture of AI; but for smaller businesses with less resources, the\\nprinciples to practice gap is a major problem. Thus, many efforts',\n",
       "  'own industrial standards, frameworks, and guidelines to build a\\nculture of AI; but for smaller businesses with less resources, the\\nprinciples to practice gap is a major problem. Thus, many efforts\\nare still needed. On one hand, it is necessary to put forward\\nwell-developed standards; on the other hand, it is required to\\nvigorously promote the practice of standards.\\nVII. CHALLENGES AND FUTURE PERSPECTIVES\\nAs AI ethics is an emerging discipline, and there are still many\\nchallenges and problems need to be addressed in this ﬁeld. In this\\nsection, we discuss some challenges in AI ethics and give some\\nfuture perspective from our views. The purpose of this section is\\nto provide some possible research questions and directions for\\nfurther research in the future, thereby facilitating the research\\nprogress in the ﬁeld of AI ethics.\\nA. Challenges in AI Ethical Guidelines and Principles\\nAs reviewed in Section IV, a large number of guidelines',\n",
       "  'progress in the ﬁeld of AI ethics.\\nA. Challenges in AI Ethical Guidelines and Principles\\nAs reviewed in Section IV, a large number of guidelines\\nhave been proposed and released by different organizations,\\ncompanies and governments, and different principles can be\\nidentiﬁed in these guidelines. However, at present, there is still\\nno guideline that have been approved and adopted by various\\norganizations, sectors, and governments. In other words, dif-\\nferent organizations, companies from different ﬁelds, and even\\ndifferent companies from the same ﬁelds have different opinions\\non AI ethics. The consensus on ethics of AI has not yet been\\nreached and it is not clear what common principles and values\\nAI needs to follow. Additionally, different ethical principles may\\nberequiredwhenAIisappliedindifferentareas.Currently,study\\nand discussion on ethics of AI in different speciﬁc application\\nareas are rarely seen during our literature study.',\n",
       "  'berequiredwhenAIisappliedindifferentareas.Currently,study\\nand discussion on ethics of AI in different speciﬁc application\\nareas are rarely seen during our literature study.\\nThus, it is crucial and necessary that the basic and common\\nethical principles of AI should be reached and well-established\\nvia the discussion and cooperation among different organiza-\\ntions, areas, and governments. Then, based on the basic and\\ncommon principles, each ﬁeld can further improve these prin-\\nciples so that they are generally applicable in this speciﬁc ﬁeld.\\nClarifying the ethical principles and values that an AI system\\nneeds to comply with is the prerequisite and foundation for\\ndesigning such a system that meets these requirements.\\nB. Challenges in Implementing Ethics in AI\\nIn the implementation of ethics in AI, there are many chal-\\nlenges. This section analyzes the challenges that may be en-\\ncountered in practice when different types of ethical theories\\nare adopted.',\n",
       "  'In the implementation of ethics in AI, there are many chal-\\nlenges. This section analyzes the challenges that may be en-\\ncountered in practice when different types of ethical theories\\nare adopted.\\n1) Challenges of Virtue Ethics in Practice: According to\\nvirtue ethics, an action of an agent is morally good if the agent\\ninstantiates some virtue, i.e., acts and thinks according to some\\nmoral values [93]. It is not possible to judge whether an AI\\nsystem or agent is virtuous or not just by observing an action or a\\nseriesofactionsthatseemtoimplythatvirtue,thereasonsbehind\\nthese actions need to be clariﬁed, that is, the motives behind\\nthese actions need to be clear. However, the motives behind\\nthe actions of AI systems usually are unclear and unknown to\\nus, and difﬁcult to ﬁgure out. This is the main challenge for\\nimplementing virtue ethics. Additionally, when we carry out the\\nethical design based on virtue ethics, which virtue characteristics',\n",
       "  'us, and difﬁcult to ﬁgure out. This is the main challenge for\\nimplementing virtue ethics. Additionally, when we carry out the\\nethical design based on virtue ethics, which virtue characteristics\\nor traits AI system will align to is a difﬁcult question. Even if\\nthe virtue traits have been carefully selected, how to characterize\\nand measure the virtue is still a challenging task.\\n2) Challenges of Deontological Ethics in Practice: Deontol-\\nogists regard an action as morally good if it adheres to some\\nmoral rules or duties, regulations, and norms. Although the\\nrule-based nature of deontological ethics seems suitable for\\npractice, challenges arise during the implementation process.\\nFirst, which ethical rules should be implemented in ethical\\ndesign. Second, there might be conﬂicts between rules in some\\nsituations. Although ordering or weighing the ethical rules may\\nsolve this problem, determining the order of importance of\\ndifferent ethical rules is often difﬁcult.',\n",
       "  'situations. Although ordering or weighing the ethical rules may\\nsolve this problem, determining the order of importance of\\ndifferent ethical rules is often difﬁcult.\\n3) Challenges of Consequentialism Ethics in Practice: Con-\\nsequentialist ethics assess the morality of an action solely on\\nthe basis of its outcome. Two main challenges are involved\\nduring the implementation of consequentialism ethics. First,\\nit is difﬁcult to determine the consequences of an action or a\\ndecision. For the current AI system, the possible consequences\\nof its actions usually are not clear beforehand given the lack of\\ntransparency or interpretability of current AI models, especially\\nthe artiﬁcial neural networks. The second challenge is related to\\nquantifying the consequences. As consequentialism ethics aims\\nto maximize the utility, how to deﬁne and calculate the utility is\\nan essential problem.',\n",
       "  '816\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\n4) Challenges of Coordination Among Different Ethical\\nStandards: Due to differences in culture, religion, and orga-\\nnizations, the ethical standards are also different even if they\\nare in the same context. The uniﬁed ethical standard proposal\\nis not only difﬁcult to achieve, but also unnecessary. Therefore,\\nhow to achieve coordination between ethical standards from dif-\\nferent countries and organizations is important and particularly\\nchallenging.\\nC. Challenges in Developing Technological Approaches to\\nMitigate Ethical Issues of AI\\nAt present, improving the explainability, fairness, privacy\\nprotection, security, robustness, and other competences related\\nto requirements of ethical AI are hot research topics in AI\\ncommunities. However, most of the current research work are\\ncarried out from a single dimension of ethical principles, for\\ninstance, XAI focuses on enhancing the interpretability of AI,',\n",
       "  'communities. However, most of the current research work are\\ncarried out from a single dimension of ethical principles, for\\ninstance, XAI focuses on enhancing the interpretability of AI,\\nand fair ML is dedicated to mitigating unfairness or bias of\\nML. There is still a lack of integration of multiple ethical\\nprinciples or requirements in current research work. Obviously,\\nthe integration of multiple ethical dimensions that enables syn-\\nergistic balances between multiple different ethical principles is\\nessential and critical for building ethical AI systems which can\\nmeets multiple ethical principles. But it is very challenging to\\nintegrate multiple ethical dimensions in an AI system through\\ntechnological approaches due to the conﬂicts or incompatibili-\\nties between different ethical requirements.\\nD. Challenges in Evaluating Ethics in AI\\nEthics is inherently a qualitative concept that depends on\\nmany features that are hard to quantify, e.g., culturally or racially',\n",
       "  'D. Challenges in Evaluating Ethics in AI\\nEthics is inherently a qualitative concept that depends on\\nmany features that are hard to quantify, e.g., culturally or racially\\nrelatedfeatures.Hence,itisveryhard,ifnotimpossible,todeﬁne\\nethics precisely. As a result, the evaluation of AI ethics will\\nalways have some subjective elements, depending on the people\\nwho are assessing AI. This poses challenges to the research and\\napplications of AI ethics.\\nE. Future Perspectives\\nIn this section, some future perspectives are pointed out,\\nwhich may be valuable for future research. First, for imple-\\nmenting ethics in AI, it should be pointed out that humans\\nnever use only one single ethical theory, but will switch between\\ndifferent theories according to the situation or context they are\\nfacing [134]. This is not only because human beings are not\\npurely rational agents that economic theory wants us to believe,\\nbut also because strict adherence to any moral theory can lead',\n",
       "  'facing [134]. This is not only because human beings are not\\npurely rational agents that economic theory wants us to believe,\\nbut also because strict adherence to any moral theory can lead\\nto undesirable results. This means that AI systems should be\\nprovided with representations of different ethical theories and\\nthe ability to choose between these ethical theories. Here we call\\nthis multitheory approach. In multitheory approach, AI systems\\ncan interchangeably apply different theories depending on the\\ntype of situation. Furthermore, the combination of normative\\nethical theories and domain-speciﬁc ethics which accepted by\\ndomain experts is worthy of implementing since an ethical AI\\nsystem need to be accepted by its users.\\nIn terms of technological approaches for addressing ethical\\nissues in AI, it is desirable to develop new ML and other AI\\ntechnologies under the guidance of the ethical guidelines and\\nprinciples reviewed in Section IV. Although it is challenging',\n",
       "  'issues in AI, it is desirable to develop new ML and other AI\\ntechnologies under the guidance of the ethical guidelines and\\nprinciples reviewed in Section IV. Although it is challenging\\nto consider multiple different ethical principles simultaneously\\nwhen designing new AI agents, this will be a very important and\\nessential step in developing ethical AI in the future.\\nFrom the review about morality evaluation approaches, it\\ncan be found that effective evaluation methods are urgently\\nneeded because we must evaluate the designed AI system\\nbefore deployment. At present, it is difﬁcult to propose a\\ngeneral evaluation method. So, researchers often focused on\\nspeciﬁc domains and addressed the moral competence assess-\\nment tasks in these domains. Domain-speciﬁc benchmarks, e.g.,\\ncomprehensive datasets, for moral testing of AI systems also\\nseems important for some crucial application ﬁelds, such as\\nautonomous cars, and health care.\\nLast but not least, as both nature and nurture are important in',\n",
       "  'seems important for some crucial application ﬁelds, such as\\nautonomous cars, and health care.\\nLast but not least, as both nature and nurture are important in\\nshaping moral behaviors, we suggest combining the normative\\nethics and evolutionary ethics [155] to design ethical AI systems.\\nThe normative ethics is like the innate moral abilities, while\\nevolutionary ethics approach can acquire new moral competence\\nthrough continuous learning and evolution. This might be a\\npromising route to future ethical AI system development.\\nVIII. CONCLUSION\\nBased on our review of AI ethics and the many complexities\\nand challenges described in this article, it is clear that attempting\\nto address ethical issues in AI and to design ethical AI systems\\nthat are able to behave ethically is a tricky and complex task.\\nHowever, whether AI can play an increasingly important role\\nin our future society largely depends on the success of ethical\\nAI systems. The discipline of AI ethics requires a joint effort',\n",
       "  'However, whether AI can play an increasingly important role\\nin our future society largely depends on the success of ethical\\nAI systems. The discipline of AI ethics requires a joint effort\\nof AI scientists, engineers, philosophers, users, and government\\npolicymakers.\\nThis article provides a comprehensive overview of AI ethics\\nby summarizing and analyzing the ethical risks and issues raised\\nby AI, ethical guidelines and principles issued by different\\norganizations, approaches for addressing ethical issues in AI\\nor fulﬁlling ethical principles of AI, and methods for evaluating\\nthe ethics (or morality) of AI. Furthermore, some challenges in\\nthe practice of AI ethics and some future research directions are\\npointed out.\\nHowever, AI ethics is a very broad and multidisciplinary\\nresearch area. It is impossible to cover all possible topics in\\nthis area with one review article. We hope this article can serve\\nas a starting point for people who are interested in AI ethics to',\n",
       "  'research area. It is impossible to cover all possible topics in\\nthis area with one review article. We hope this article can serve\\nas a starting point for people who are interested in AI ethics to\\ngain a sufﬁcient background and a bird’s eye view so that further\\ninvestigation can be pursued by them.\\nREFERENCES\\n[1] M. Haenlein and A. Kaplan, “A brief history of artiﬁcial intelligence: On\\nthe past, present, and future of artiﬁcial intelligence,” California Manage.\\nRev., vol. 61, no. 4, pp. 5–14, 2019.\\n[2] R. Vinuesa et al., “The role of artiﬁcial intelligence in achieving the\\nsustainable development goals,” Nature Commun., vol. 11, no. 1, 2020,\\nArt. no. 233.',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n817\\n[3] Gartner, “Chatbots will appeal to modern workers,” 2019. Ac-\\ncessed: Feb. 10, 2022. [Online]. Available: https://www.gartner.com/\\nsmarterwithgartner/chatbots-will-appeal-to-modern-workers\\n[4] M. J. Haleem, R. P. Singh, and R. Suman, “Telemedicine for healthcare:\\nCapabilities, features, barriers, and applications,” Sensors Int., vol. 2,\\n2021, Art. no. 100117.\\n[5] A. Morby, “Tesla driver killed in ﬁrst fatal crash using au-\\ntopilot,”\\n2016.\\nAccessed:\\nFeb.\\n10,\\n2022.\\n[Online].\\nAvailable:\\nhttps://www.dezeen.com/2016/07/01/tesla-driver-killed-car-crash-\\nnews-driverless-car-autopilot/\\n[6] S. McGregor, Ed., “Incident number 6,” in AI Incident Database, 2016.\\n[Online]. Available: https://incidentdatabase.ai/cite/6\\n[7] R. V. Yampolskiy, “Predicting future AI failures from historic examples,”\\nForesight, vol. 21, no. 1, pp. 138–152, 2019.\\n[8] C. Stupp, “Fraudsters used AI to mimic CEO’s voice in un-',\n",
       "  '[7] R. V. Yampolskiy, “Predicting future AI failures from historic examples,”\\nForesight, vol. 21, no. 1, pp. 138–152, 2019.\\n[8] C. Stupp, “Fraudsters used AI to mimic CEO’s voice in un-\\nusual cybercrime case: Scams using artiﬁcial intelligence are a\\nnew challenge for companies,” 2019. Accessed: Feb. 10, 2022.\\n[Online]. Available: https://www.wsj.com/articles/fraudsters-use-ai-to-\\nmimic-ceos-voice-in-unusual-cybercrime-case-11567157402\\n[9] C. Allen, W. Wallach, and I. Smit, “Why machine ethics?,” IEEE Intell.\\nSyst., vol. 21, no. 4, pp. 12–17, Jul./Aug. 2006.\\n[10] M. Anderson and S. L. Anderson, “Machine ethics: Creating an ethical\\nintelligent agent,” AI Mag., vol. 28, no. 4, pp. 15–26, 2007.\\n[11] K. Siau and W. Wang, “Artiﬁcial intelligence (AI) ethics,” J. Database\\nManage., vol. 31, no. 2, pp. 74–87, 2020.\\n[12] A. Jobin, M. Ienca, and E. Vayena, “The global landscape of AI ethics\\nguidelines,” Nature Mach. Intell., vol. 1, no. 9, pp. 389–399, 2019.',\n",
       "  'Manage., vol. 31, no. 2, pp. 74–87, 2020.\\n[12] A. Jobin, M. Ienca, and E. Vayena, “The global landscape of AI ethics\\nguidelines,” Nature Mach. Intell., vol. 1, no. 9, pp. 389–399, 2019.\\n[13] M. Ryan and B. C. Stahl, “Artiﬁcial intelligence ethics guidelines for de-\\nvelopers and users: Clarifying their content and normative implications,”\\nJICES, vol. 19, no. 1, pp. 61–86, 2021.\\n[14] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A\\nsurvey on bias and fairness in machine learning,” ACM Comput. Surv.,\\nvol. 54, no. 6, pp. 1–35, 2021.\\n[15] J. García and F. Fernández, “A comprehensive survey on safe reinforce-\\nment learning,” J. Mach. Learn. Res., vol. 16, no. 42, pp. 1437–1480,\\n2015.\\n[16] V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and\\nG. Srivastava, “A survey on security and privacy of federated learning,”\\nFuture Gener. Comput. Syst., vol. 115, pp. 619–640, 2021.\\n[17] X. Liu et al., “Privacy and security issues in deep learning: A survey,”',\n",
       "  'Future Gener. Comput. Syst., vol. 115, pp. 619–640, 2021.\\n[17] X. Liu et al., “Privacy and security issues in deep learning: A survey,”\\nIEEE Access, vol. 9, pp. 4566–4593, 2021.\\n[18] B. Arrieta et al., “Explainable artiﬁcial intelligence (XAI): Concepts,\\ntaxonomies, opportunities and challenges toward responsible AI,” Inf.\\nFusion, vol. 58, pp. 82–115, 2020.\\n[19] Y. Zhang, M. Wu, G. Y. Tian, G. Zhang, and J. Lu, “Ethics and privacy\\nof artiﬁcial intelligence: Understandings from bibliometrics,” Knowl.-\\nBased Syst., vol. 222, 2021, Art. no. 106994.\\n[20] D. Castelvecchi, “Can we open the black box of AI?,” Nature, vol. 538,\\nno. 7623, pp. 20–23, 2016.\\n[21] S. Dilmaghani, M. R. Brust, G. Danoy, N. Cassagnes, J. Pecero, and\\nP. Bouvry, “Privacy and security of big data in AI systems: A research\\nand standards perspective,” in Proc. IEEE Int. Conf. Big Data, 2019,\\npp. 5737–5743.\\n[22] J. P. Sullins, “When is a robot a moral agent?,” in Machine Ethics, M.',\n",
       "  'and standards perspective,” in Proc. IEEE Int. Conf. Big Data, 2019,\\npp. 5737–5743.\\n[22] J. P. Sullins, “When is a robot a moral agent?,” in Machine Ethics, M.\\nAnderson and S. L. Anderson, Eds., Cambridge, U.K.: Cambridge Univ.\\nPress, 2011, pp. 151–161.\\n[23] J. Timmermans, B. C. Stahl, V. Ikonen, and E. Bozdag, “The ethics of\\ncloud computing: A conceptual review,” in Proc. IEEE 2nd Int. Conf.\\nCloud Comput. Technol. Sci., 2010, pp. 614–620.\\n[24] W. Wang and K. Siau, “Ethical and moral issues with AI: A case study on\\nhealthcare robots,” in Proc. 24th Americas Conf. Inf. Syst., 2018, pp. 1–5.\\n[25] I. Bantekas and L. Oette, International Human Rights Law and Practice.\\nCambridge U. K.: Cambridge Univ. Press, 2018.\\n[26] R. Rodrigues, “Legal and human rights issues of AI: Gaps, challenges and\\nvulnerabilities,” J. Responsible Technol., vol. 4, 2020, Art. no. 100005.\\n[27] W. Wang and K. Siau, “Artiﬁcial intelligence, machine learning, au-',\n",
       "  'vulnerabilities,” J. Responsible Technol., vol. 4, 2020, Art. no. 100005.\\n[27] W. Wang and K. Siau, “Artiﬁcial intelligence, machine learning, au-\\ntomation, robotics, future of work and future of humanity: A review\\nand research agenda,” J. Database Manage., vol. 30, no. 1, pp. 61–79,\\n2019.\\n[28] W. Wang and K. Siau, “Industry 4.0: Ethical and moral predicaments,”\\nCutter Bus. Technol. J., vol. 32, no. 6, pp. 36–45, 2019.\\n[29] S. M. Liao, Ed., Ethics of Artiﬁcial Intelligence. New York, NY, USA:\\nOxford Univ. Press, 2020.\\n[30] A. Adadi, “A survey on data-efﬁcient algorithms in big data era,” J. Big\\nData, vol. 8, no. 1, pp. 1–54, 2021.\\n[31] R. S. Geiger et al., “Garbage in, garbage out? Do machine learning\\napplication papers in social computing report where human-labeled\\ntraining data comes from?,” in Proc. Conf. Fairness, Accountability,\\nTransparency, 2020, pp. 325–336.\\n[32] W. M. P. van der Aalst, V. Rubin, H. M. W. Verbeek, B. F. van Dongen,',\n",
       "  'training data comes from?,” in Proc. Conf. Fairness, Accountability,\\nTransparency, 2020, pp. 325–336.\\n[32] W. M. P. van der Aalst, V. Rubin, H. M. W. Verbeek, B. F. van Dongen,\\nE. Kindler, and C. W. Günther, “Process mining: A two-step approach to\\nbalance between underﬁtting and overﬁtting,” Softw. Syst. Model., vol. 9,\\nno. 1, pp. 87–111, 2010.\\n[33] Z. C. Lipton, “The mythos of model interpretability,” Queue, vol. 16,\\nno. 3, pp. 31–57, 2018.\\n[34] Y. Wang and M. Kosinski, “Deep neural networks are more accurate than\\nhumans at detecting sexual orientation from facial images,” J. Pers. Social\\nPsychol., vol. 114, no. 2, pp. 246–257, 2018.\\n[35] D. Guera and E. J. Delp, “Deepfake video detection using recurrent neural\\nnetworks,” in Proc. IEEE Int. Conf. Adv. Video Signal-based Surveill.,\\n2018, pp. 1–6.\\n[36] C. B. Frey and M. A. Osborne, “The future of employment: How sus-\\nceptible are jobs to computerisation?,” Technological Forecasting Social\\nChange, vol. 114, pp. 254–280, 2017.',\n",
       "  '2018, pp. 1–6.\\n[36] C. B. Frey and M. A. Osborne, “The future of employment: How sus-\\nceptible are jobs to computerisation?,” Technological Forecasting Social\\nChange, vol. 114, pp. 254–280, 2017.\\n[37] R. Maines, “Love + sex with robots: The evolution of human-robot\\nrelationships (Levy, D.; 2007) [Book review],” IEEE Technol. Soc. Mag.,\\nvol. 27, no. 4, pp. 10–12, Dec. 2008.\\n[38] National AI Standardization General, “Artiﬁcial intelligence ethical risk\\nanalysis report,” 2019. Accessed: Apr. 19, 2022. [Online]. Available:\\nhttp://www.cesi.cn/201904/5036.html\\n[39] A. Hannun, C. Guo, and L. van der Maaten, “Measuring data leakage in\\nmachine-learning models with ﬁsher information,” in Proc. 37th Conf.\\nUncertainty Artif. Intell., 2021, pp. 760–770.\\n[40] A. Salem, M. Backes, and Y. Zhang, “Get a model! Model hijacking\\nattack against machine learning models,” Nov. 2021. [Online]. Available:\\nhttps://arxiv.org/pdf/2111.04394\\n[41] A. Pereira and C. Thomas, “Challenges of machine learning ap-',\n",
       "  'attack against machine learning models,” Nov. 2021. [Online]. Available:\\nhttps://arxiv.org/pdf/2111.04394\\n[41] A. Pereira and C. Thomas, “Challenges of machine learning ap-\\nplied to safety-critical cyber-physical systems,” MAKE, vol. 2, no. 4,\\npp. 579–602, 2020.\\n[42] J. A. McDermid, Y. Jia, Z. Porter, and I. Habli, “Artiﬁcial intelligence\\nexplainability: The technical and ethical dimensions,” Philos. Trans.. Ser.\\nA, Math. Phys. Eng. Sci., vol. 379, no. 2207, 2021, Art. no. 20200363.\\n[43] J.-F. Bonnefon, A. Shariff, and I. Rahwan, “The social dilemma of\\nautonomous vehicles,” Science, vol. 352, no. 6293, pp. 1573–1576, 2016.\\n[44] B. C. Stahl and D. Wright, “Ethics and privacy in AI and big data: Im-\\nplementing responsible research and innovation,” IEEE Secur. Privacy,\\nvol. 16, no. 3, pp. 26–33, May/Jun. 2018.\\n[45] S. Ribaric, A. Ariyaeeinia, and N. Pavesic, “De-identiﬁcation for privacy\\nprotection in multimedia content: A survey,” Signal Process., Image',\n",
       "  'vol. 16, no. 3, pp. 26–33, May/Jun. 2018.\\n[45] S. Ribaric, A. Ariyaeeinia, and N. Pavesic, “De-identiﬁcation for privacy\\nprotection in multimedia content: A survey,” Signal Process., Image\\nCommun., vol. 47, pp. 131–151, 2016.\\n[46] A. Julia, L. Jeff, M. Surya, and K. Lauren, “Machine bias: There’s\\nsoftware used across the country to predict future criminals. And\\nit’s biased against blacks,” 2016. Accessed: Apr. 19, 2022. [On-\\nline]. Available: https://www.propublica.org/article/machine-bias-risk-\\nassessments-in-criminal-sentencing\\n[47] J. Dastin, “Amazon scraps secret AI recruiting tool that showed bias\\nagainst women,” 2018. Accessed: Apr. 19, 2022. [Online]. Available:\\nhttps://www.reuters.com/article/us-amazon-com-jobs-automation-\\ninsight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-\\nagainst-women-idUSKCN1MK08G\\n[48] D. Castelvecchi, “AI pioneer: ‘The dangers of abuse are very real’,” Na-\\nture, Apr. 4, 2019, [Online]. Available: https://www.nature.com/articles/',\n",
       "  'against-women-idUSKCN1MK08G\\n[48] D. Castelvecchi, “AI pioneer: ‘The dangers of abuse are very real’,” Na-\\nture, Apr. 4, 2019, [Online]. Available: https://www.nature.com/articles/\\nd41586-019-00505-2\\n[49] K. Hristov, “Artiﬁcial intelligence and the copyright dilemma,” IDEA,\\nIP Law Rev., vol. 57, 2017, Art. no. 3. [Online]. Available: https://ssrn.\\ncom/abstract=2976428\\n[50] C. Bartneck, C. Lütge, A. Wagner, and S. Welsh, “Responsibility and lia-\\nbility in the case of AI systems,” in An Introduction to Ethics in Robotics\\nand AI (SpringerBriefs in Ethics), C. Bartneck, C. Lütge, A. Wagner, and\\nS. Welsh, Eds., Cham, Switzerland: Springer, 2021, pp. 39–44.\\n[51] E. Bird, J. Fox-Skelly, N. Jenner, R. Larbey, E. Weitkamp, and A.\\nWinﬁeld, “The ethics of artiﬁcial intelligence: Issues and initiatives,”\\nEuropean Parliamentary Research Service, Brussels, Belgium, 2020. Ac-\\ncessed: Apr. 19, 2022. [Online]. Available: https://www.europarl.europa.\\neu/thinktank/en/document/EPRS_STU(2020)634452',\n",
       "  'European Parliamentary Research Service, Brussels, Belgium, 2020. Ac-\\ncessed: Apr. 19, 2022. [Online]. Available: https://www.europarl.europa.\\neu/thinktank/en/document/EPRS_STU(2020)634452\\n[52] C. Lutz, “Digital inequalities in the age of artiﬁcial intelligence and big\\ndata,” Hum. Behav. Emerg. Technol., vol. 1, no. 2, pp. 141–148, 2019.\\n[53] L. Manikonda, A. Deotale, and S. Kambhampati, “What’s up with\\nPrivacy? User preferences and privacy concerns in intelligent personal\\nassistants,” in Proc. AAAI/ACM Conf. AI, Ethics Soc., 2018, pp. 229–235.',\n",
       "  '818\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\n[54] D. Roselli, J. Matthews, and N. Talagala, “Managing bias in AI,” in Proc.\\nWorld Wide Web Conf., 2019, pp. 539–544.\\n[55] Y. Gorodnichenko, T. Pham, and O. Talavera, “Social media, sentiment\\nandpublicopinions:Evidencefrom#Brexitand#USElection,”Eur.Econ.\\nRev., vol. 136, Jul. 2021, Art. no. 103772.\\n[56] N. Thurman, “Making ‘The daily me’: Technology, economics and\\nhabit in the mainstream assimilation of personalized news,” Journalism,\\nvol. 12, no. 4, pp. 395–415, 2011.\\n[57] J. Donath, “Ethical issues in our relationship with artiﬁcial entities,” in\\nThe Oxford Handbook of Ethics of AI. M. D. Dubber, F. Pasquale, and S.\\nDas, Eds., Oxford, U.K.: Oxford Univ. Press, 2020, pp. 51–73.\\n[58] E. Magrani, “New perspectives on ethics and the laws of artiﬁcial intel-\\nligence,” Internet Policy Rev., vol. 8, 2019, Art. no. 3.\\n[59] M. P. Wellman and U. Rajan, “Ethical issues for autonomous trading',\n",
       "  'ligence,” Internet Policy Rev., vol. 8, 2019, Art. no. 3.\\n[59] M. P. Wellman and U. Rajan, “Ethical issues for autonomous trading\\nagents,” Minds Mach., vol. 27, no. 4, pp. 609–624, 2017.\\n[60] U. Pagallo, “The impact of AI on criminal law, and its two fold pro-\\ncedures,” in Research Handbook on the Law of Artiﬁcial Intelligence,\\nW. Barﬁeld and U. Pagallo, Eds., Cheltenham U.K.: Edward Elgar\\nPublishing, 2018, pp. 385–409.\\n[61] E. Dacoronia, “Tort law and new technologies,” in Legal Challenges in\\nthe New Digital Age, A. M. López Rodríguez, M. D. Green, and M.\\nL. Kubica, Eds., Leiden, The Netherlands: Koninklijke Brill NV, 2021,\\npp. 3–12.\\n[62] J. Khakurel, B. Penzenstadler, J. Porras, A. Knutas, and W. Zhang, “The\\nrise of artiﬁcial intelligence under the lens of sustainability,” Technolo-\\ngies, vol. 6, no. 4, 2018, Art. no. 100.\\n[63] S. Herat, “Sustainable management of electronic waste (e-Waste),” Clean\\nSoil Air Water, vol. 35, no. 4, pp. 305–310, 2007.',\n",
       "  'gies, vol. 6, no. 4, 2018, Art. no. 100.\\n[63] S. Herat, “Sustainable management of electronic waste (e-Waste),” Clean\\nSoil Air Water, vol. 35, no. 4, pp. 305–310, 2007.\\n[64] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\\nerations for deep learning in NLP,” in Proc. 57th Annu. Meeting Assoc.\\nComput. Linguistics, 2019, pp. 3645–3650.\\n[65] V. Dignum, “Ethics in artiﬁcial intelligence: Introduction to the special\\nissue,” Ethics Inf. Technol., vol. 20, no. 1, pp. 1–3, 2018.\\n[66] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq, “Algo-\\nrithmic decision making and the cost of fairness,” in Proc. 23rd ACM\\nSIGKDD Int. Conf. Knowl. Discov. Data Mining, 2017, pp. 797–806.\\n[67] R. Caplan, J. Donovan, L. Hanson, and J. Matthews, “Algorithmic ac-\\ncountability: A primer,” Data Soc., vol. 18, pp. 1–13, 2018.\\n[68] R. V. Yampolskiy, “On controllability of AI,” Jul. 2020. [Online]. Avail-\\nable: https://arxiv.org/pdf/2008.04071',\n",
       "  'countability: A primer,” Data Soc., vol. 18, pp. 1–13, 2018.\\n[68] R. V. Yampolskiy, “On controllability of AI,” Jul. 2020. [Online]. Avail-\\nable: https://arxiv.org/pdf/2008.04071\\n[69] B. C. Stahl, J. Timmermans, and C. Flick, “Ethics of emerging informa-\\ntion and communication technologies,” Sci. Public Policy, vol. 44, no. 3,\\npp. 369–381, 2017.\\n[70] L. Vesnic-Alujevic, S. Nascimento, and A. Pólvora, “Societal and\\nethical impacts of artiﬁcial intelligence: Critical notes on Euro-\\npean policy frameworks,” Telecommun. Policy, vol. 44, no. 6, 2020,\\nArt. no. 101961.\\n[71] U. G. Assembly, “Universal declaration of human rights,” UN Gen.\\nAssem., vol. 302, no. 2, pp. 14–25, 1948.\\n[72] S. Russell, S. Hauert, R. Altman, and M. Veloso, “Robotics: Ethics of\\nartiﬁcial intelligence,” Nature, vol. 521, no. 7553, pp. 415–418, 2015.\\n[73] A. Chouldechova, “Fair prediction with disparate impact: A study of\\nbias in recidivism prediction instruments,” Big Data, vol. 5, no. 2,\\npp. 153–163, 2017.',\n",
       "  '[73] A. Chouldechova, “Fair prediction with disparate impact: A study of\\nbias in recidivism prediction instruments,” Big Data, vol. 5, no. 2,\\npp. 153–163, 2017.\\n[74] J. van Dijck, “Dataﬁcation, dataism and dataveillance: Big data be-\\ntween scientiﬁc paradigm and ideology,” Surveill. Soc., vol. 12, no. 2,\\npp. 197–208, 2014.\\n[75] E. de Souza Nascimento, I. Ahmed, E. Oliveira, M. P. Palheta, I. Stein-\\nmacher, and T. Conte, “Understanding development process of machine\\nlearning systems: Challenges and solutions,” in Proc. ACM/IEEE Int.\\nSymp. Empirical Softw. Eng. Meas., 2019, pp. 1–6.\\n[76] K. A. Crockett, L. Gerber, A. Latham, and E. Colyer, “Build-\\ning trustworthy AI solutions: A case for practical solutions for\\nsmall businesses,” IEEE Trans. Artif. Intell., early access, 2021,\\ndoi: 10.1109/TAI.2021.3137091.\\n[77] D. Leslie, “Understanding artiﬁcial intelligence ethics and safety:\\nA guide for the responsible design and implementation of AI',\n",
       "  'doi: 10.1109/TAI.2021.3137091.\\n[77] D. Leslie, “Understanding artiﬁcial intelligence ethics and safety:\\nA guide for the responsible design and implementation of AI\\nsystems in the public sector,” 2019. Accessed: Apr. 19, 2022.\\n[Online].\\nAvailable:\\nhttps://www.turing.ac.uk/research/publications/\\nunderstanding-artiﬁcial-intelligence-ethics-and-safety\\n[78] B. Buruk, P. E. Ekmekci, and B. Arda, “A critical perspective on guide-\\nlines for responsible and trustworthy artiﬁcial intelligence,” Med. Health\\nCare Philosophy, vol. 23, no. 3, pp. 387–399, 2020.\\n[79] UNESCO, “Recommendation on the ethics of artiﬁcial intelligence,”\\n2021. Accessed: Feb. 15 2022. [Online]. Available: https://en.unesco.\\norg/artiﬁcial-intelligence/ethics\\n[80] B. C. Stahl, Ed., Artiﬁcial Intelligence for a Better Future: An Ecosystem\\nPerspective on the Ethics of AI and Emerging Digital Technologies.\\nCham, Switzerland: Springer, 2021.\\n[81] P. D. Motloba, “Non-maleﬁcence - A disremembered moral obligation,”',\n",
       "  'Perspective on the Ethics of AI and Emerging Digital Technologies.\\nCham, Switzerland: Springer, 2021.\\n[81] P. D. Motloba, “Non-maleﬁcence - A disremembered moral obligation,”\\nSouth Afr. Dent. J., vol. 74, 2019, Art. no. 1.\\n[82] L. Floridi and J. Cowls, “A uniﬁed framework of ﬁve principles for AI\\nin society,” in Ethics, Governance, and Policies in Artiﬁcial Intelligence\\n(Philosophical Studies Series), vol. 144, L. Floridi, Ed. Cham, Switzer-\\nland: Springer, 2021, pp. 5–17.\\n[83] S. Jain, M. Luthra, S. Sharma, and M. Fatima, “Trustworthiness of\\nartiﬁcial intelligence,” in Proc. 6th Int. Conf. Adv. Comput. Commun.\\nSyst., 2020, pp. 907–912.\\n[84] L. Floridi et al., “AI4People-An ethical framework for a good AI society:\\nOpportunities, risks, principles, and recommendations,” Minds Mach.,\\nvol. 28, no. 4, pp. 689–707, 2018.\\n[85] R. Nishant, M. Kennedy, and J. Corbett, “Artiﬁcial intelligence for\\nsustainability: Challenges, opportunities, and a research agenda,” Int. J.',\n",
       "  'vol. 28, no. 4, pp. 689–707, 2018.\\n[85] R. Nishant, M. Kennedy, and J. Corbett, “Artiﬁcial intelligence for\\nsustainability: Challenges, opportunities, and a research agenda,” Int. J.\\nInf. Manage., vol. 53, 2020, Art. no. 102104.\\n[86] C. S. Wickramasinghe, D. L. Marino, J. Grandio, and M. Manic, “Trust-\\nworthy AI development guidelines for human system interaction,” in\\nProc. 13th Int. Conf. Hum. Syst. Interaction, 2020, pp. 130–136.\\n[87] V. Dignum, “Can AI systems be ethical?,” in Artiﬁcial Intelligence:\\nFoundations Theory and Algorithms, Responsible Artiﬁcial Intelligence.\\nV. Dignum, Ed., Cham, Switzerland: Springer, 2019, pp. 71–92.\\n[88] S. L. Anderson and M. Anderson, “AI and ethics,” AI Ethics, vol. 1, no. 1,\\npp. 27–31, 2021.\\n[89] V. Dignum, “Ethical decision-making,” in Artiﬁcial Intelligence: Foun-\\ndations, Theory, and Algorithms, Responsible Artiﬁcial Intelligence. V.\\nDignum, Ed., Cham, Switzerland: Springer, 2019, pp. 35–46.\\n[90] G.\\nSayre-McCord,\\n“Metaethics,”\\nin\\nThe\\nStanford',\n",
       "  'dations, Theory, and Algorithms, Responsible Artiﬁcial Intelligence. V.\\nDignum, Ed., Cham, Switzerland: Springer, 2019, pp. 35–46.\\n[90] G.\\nSayre-McCord,\\n“Metaethics,”\\nin\\nThe\\nStanford\\nEncyclopedia\\nof Philosophy, E. N. Zalta, Ed., Stanford, CA, USA: Metaphys.\\nRes.\\nLab,\\nStanford\\nUniv.,\\n2014.\\n[Online].\\nAvailable:\\nhttps:\\n//plato.stanford.edu/entries/metaethics/#:∼:text=Metaethics%20is%\\n20the%20attempt%20to,matter%20of%20taste%20than%20truth%3F\\n[91] Ethics | Internet Encyclopedia of Philosophy, 1995. Accessed: Aug. 2,\\n2021. [Online]. Available: https://iep.utm.edu/ethics/#SH2c\\n[92] R. Hursthouse and G. Pettigrove, “Virtue ethics,” in The Stanford En-\\ncyclopedia of Philosophy, E. N. Zalta, Ed. Stanford, CA, USA: Meta-\\nphys. Res. Lab, Stanford Univ., 2018. [Online]. Available: https://plato.\\nstanford.edu/entries/ethics-virtue/\\n[93] N. Cointe, G. Bonnet, and O. Boissier, “Ethical judgment of agents’\\nbehaviors in multi-agent systems,” in Proc. Int. Conf. Auton. Agents',\n",
       "  'stanford.edu/entries/ethics-virtue/\\n[93] N. Cointe, G. Bonnet, and O. Boissier, “Ethical judgment of agents’\\nbehaviors in multi-agent systems,” in Proc. Int. Conf. Auton. Agents\\nMultiagent Syst., 2016, pp. 1106–1114.\\n[94] H. Yu, Z. Shen, C. Miao, C. Leung, V. R. Lesser, and Q. Yang, “Building\\nethics into artiﬁcial intelligence,” in Proc. 27th Int. Joint Conf. Artif.\\nIntell., 2018, pp. 5527–5533.\\n[95] H. J. Curzer, Aristotle and the Virtues. New York, NY, USA: Oxford\\nUniv. Press, 2012.\\n[96] L. Alexander and M. Moore, “Deontological ethics,” in The Stanford\\nEncyclopedia of Philosophy, E. N. Zalta, Ed., 2020, Stanford, CA, USA:\\nMetaphys. Res. Lab, Stanford Univ., 2020. [Online]. Available: https:\\n//plato.stanford.edu/entries/ethics-deontological/\\n[97] W. Sinnott-Armstrong, “Consequentialism,” in The Stanford Encyclope-\\ndia of Philosophy, E. N. Zalta, Ed., 2019. Stanford, CA, USA: Metaphys.\\nRes. Lab, Stanford Univ., [Online]. Available: https://plato.stanford.edu/',\n",
       "  'dia of Philosophy, E. N. Zalta, Ed., 2019. Stanford, CA, USA: Metaphys.\\nRes. Lab, Stanford Univ., [Online]. Available: https://plato.stanford.edu/\\nentries/consequentialism/\\n[98] D. O. Brink, “Some forms and limits of consequentialism,” in Ox-\\nford Handbooks in Philosophy, The Oxford Handbook of Ethical The-\\nory. D. Copp, Ed., New York, NY, USA: Oxford Univ. Press, 2006,\\npp. 380–423.\\n[99] H. A. M. J. ten Have, Ed., Encyclopedia of Global Bioethics. Cham,\\nSwitzerland: Springer, 2016.\\n[100] S. Tolmeijer, M. Kneer, C. Sarasua, M. Christen, and A. Bernstein,\\n“Implementations in machine ethics: A survey,” ACM Comput. Surv.,\\nvol. 53, no. 6, pp. 1–38, 2021.\\n[101] C. Allen, I. Smit, and W. Wallach, “Artiﬁcial morality: Top-down,\\nbottom-up, and hybrid approaches,” Ethics Inf. Technol., vol. 7, no. 3,\\npp. 149–155, 2005.\\n[102] W. Wallach and C. Allen, “Top-down morality,” in Moral Machines, W.\\nWallach and C. Allen, Eds., Oxford, U.K: Oxford Univ. Press, 2009,\\npp. 83–98.',\n",
       "  'pp. 149–155, 2005.\\n[102] W. Wallach and C. Allen, “Top-down morality,” in Moral Machines, W.\\nWallach and C. Allen, Eds., Oxford, U.K: Oxford Univ. Press, 2009,\\npp. 83–98.\\n[103] I. Asimov, “Runaround,” Astounding Sci. Fiction, vol. 29, no. 1,\\npp. 94–103, 1942.\\n[104] J.-G.\\nGanascia,\\n“Ethical\\nsystem\\nformalization\\nusing\\nnon-\\nmonotonic ogics,” in Proc. Annu. Meeting Cogn. Sci. Soc., 2007,\\npp. 1013–1018.',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n819\\n[105] K. Arkoudas, S. Bringsjord, and P. Bello, “Toward ethical robots via\\nmechanized deontic logic,” in Proc. AAAI Fall Symp. Mach. Ethics, 2005,\\npp. 17–23.\\n[106] S. Bringsjord and J. Taylor, “Introducing divine-command robot ethics,”\\nin Robot Ethics: The Ethical and Social Implication of Robotics. 2012,\\npp. 85–108.\\n[107] N. S. Govindarajulu and S. Bringsjord, “On automating the doctrine\\nof double effect,” in Proc. 26th Int. Joint Conf. Artif. Intell., 2017,\\npp. 4722–4730.\\n[108] F. Berreby, G. Bourgne, and J.-G. Ganascia, “A declarative modular\\nframework for representing and applying ethical principles,” in Proc.\\n16th Conf. Auton. Agents MultiAgent Syst., 2017, pp. 96–104.\\n[109] V. Bonnemains, C. Saurel, and C. Tessier, “Embedded ethics: Some\\nechnical and ethical challenges,” Ethics Inf. Technol., vol. 20, no. 1,\\npp. 41–58, 2018.\\n[110] G. S. Reed, M. D. Petty, N. J. Jones, A. W. Morris, J. P. Ballenger,',\n",
       "  'echnical and ethical challenges,” Ethics Inf. Technol., vol. 20, no. 1,\\npp. 41–58, 2018.\\n[110] G. S. Reed, M. D. Petty, N. J. Jones, A. W. Morris, J. P. Ballenger,\\nand H. S. Delugach, “A principles-based model of ethical considerations\\nin military decision making,” J. Defense Model. Simul., vol. 13, no. 2,\\npp. 195–211, 2016.\\n[111] L. Dennis, M. Fisher, M. Slavkovik, and M. Webster, “Formal veriﬁcation\\nof ethical choices in autonomous systems,” Robot. Auton. Syst., vol. 77,\\npp. 1–14, 2016.\\n[112] A. R. Honarvar and N. Ghasem-Aghaee, “Casuist BDI-Agent: A new\\nextended BDI architecture with the capability of ethical reasoning,” in\\nProc. Int. Conf. Artif. Intell. Comput. Intell., 2009, pp. 86–95.\\n[113] A. S. Rao and M. P. Georgeff, “BDI agents: From theory to practice,” in\\nProc. 1st Int. Conf. Multiagent Syst., 1995, pp. 312–319.\\n[114] S. Armstrong, “Motivated value selection for artiﬁcial agents,” in Proc.\\nAAAI Workshop Artif. Intell. Ethics, Jan. 2015, pp. 12–20.',\n",
       "  'Proc. 1st Int. Conf. Multiagent Syst., 1995, pp. 312–319.\\n[114] S. Armstrong, “Motivated value selection for artiﬁcial agents,” in Proc.\\nAAAI Workshop Artif. Intell. Ethics, Jan. 2015, pp. 12–20.\\n[115] U. Furbach, C. Schon, and F. Stolzenburg, “Automated reasoning in\\ndeontic logic,” in Proc. 8th Int. Workshop Multi-Disciplinary Trends Artif.\\nIntell., 2014, pp. 57–68.\\n[116] D. Howard and I. Muntean, “Artiﬁcial moral cognition: Moral function-\\nalism and autonomous moral agency,” in Philosophical Studies Series,\\nPhilosophy and Computing, T. M. Powers, Ed. Cham, Switzerland:\\nSpringer, 2017, pp. 121–159.\\n[117] Y.-H. Wu and S.-D. Lin, “A low-cost ethics shaping approach for de-\\nsigning reinforcement learning agents,” in Proc. 32nd AAAI Conf. Artif.\\nIntell., 2018, pp. 1687–1694.\\n[118] R. Noothigattu et al., “A voting-based system for ethical decision mak-\\ning,” in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 1587–1594.',\n",
       "  'Intell., 2018, pp. 1687–1694.\\n[118] R. Noothigattu et al., “A voting-based system for ethical decision mak-\\ning,” in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 1587–1594.\\n[119] M. Guarini, “Particularism and the classiﬁcation and reclassiﬁcation of\\nmoral cases,” IEEE Intell. Syst., vol. 21, no. 4, pp. 22–28, Jul./Aug. 2006.\\n[120] M. Anderson and S. L. Anderson, “GenEth: A general ethical dilemma\\nanalyzer,” in Proc. 28th AAAI Conf. Artif. Intell., 2014, pp. 253–261.\\n[121] M. Azad-Manjiri, “A new architecture for making moral agents based on\\nC4.5 decision tree algorithm,” Int. J. Inf. Technol. Comput. Sci., vol. 6,\\nno. 5, pp. 50–57, 2014.\\n[122] L. Yilmaz, A. Franco-Watkins, and T. S. Kroecker, “Computational mod-\\nels of ethical decision-making: A coherence-driven reﬂective equilibrium\\nmodel,” Cogn. Syst. Res., vol. 46, pp. 61–74, 2017.\\n[123] T. A. Han, A. Saptawijaya, and L. M. Pereira, “Moral reasoning under\\nuncertainty,” in Logic For Programming, Artiﬁcial Intelligence, and',\n",
       "  'model,” Cogn. Syst. Res., vol. 46, pp. 61–74, 2017.\\n[123] T. A. Han, A. Saptawijaya, and L. M. Pereira, “Moral reasoning under\\nuncertainty,” in Logic For Programming, Artiﬁcial Intelligence, and\\nReasoning. Berlin, Germany: Springer, 2012, pp. 212–227.\\n[124] M. Anderson, S. Anderson, and C. Armen, “Towards machine ethics\\nimplementing two action-based ethical theories,” in Proc. AAAI Fall\\nSymp. Mach. Ethics, 2005, pp. 1–7.\\n[125] G. Gigerenzer, “Moral satisﬁcing: Rethinking moral behavior as bounded\\nrationality,” Topics Cogn. Sci., vol. 2, no. 3, pp. 528–554, 2010.\\n[126] J. Skorin-Kapov, “Ethical positions and decision-making,” in Profes-\\nsional and Business Ethics Through Film, J. Skorin-Kapov, Ed., New\\nYork, NY, USA: Springer, 2018, pp. 19–54.\\n[127] T.-L. Gu and L. Li, “Artiﬁcial moral agents and their design methodology:\\nRetrospect and prospect,” Chin. J. Comput., vol. 44, pp. 632–651, 2021.\\n[128] C.Molnar,G.Casalicchio,andB.Bischl,“Interpretablemachinelearning',\n",
       "  'Retrospect and prospect,” Chin. J. Comput., vol. 44, pp. 632–651, 2021.\\n[128] C.Molnar,G.Casalicchio,andB.Bischl,“Interpretablemachinelearning\\n– A brief history, state-of-the-art and challenges,” in Proc. Joint Eur. Conf.\\nMach. Learn. Knowl. Discov. Databases, 2020, pp. 417–431.\\n[129] C. Molnar, Interpretable Machine Learning: A Guide For Making Black\\nBox Models Interpretable. Morisville, NC, USA: Lulu, 2019.\\n[130] S. Feuerriegel, M. Dolata, and G. Schwabe, “Fair AI,” Bus. Inf. Syst. Eng.,\\nvol. 62, no. 4, pp. 379–384, 2020.\\n[131] S. Caton and C. Haas, “Fairness in machine learning: A survey,”\\nOct. 2020. [Online]. Available: https://arxiv.org/pdf/2010.04053\\n[132] S. E. Whang, K. H. Tae, Y. Roh, and G. Heo, “Responsible AI challenges\\nn end-to-end machine learning,” Jan. 2021. [Online]. Available: https:\\n//arxiv.org/pdf/2101.05967\\n[133] D. Peters, K. Vold, D. Robinson, and R. A. Calvo, “Responsible AI—Two\\nframeworksforethicaldesignpractice,”IEEETrans.Technol.Soc.,vol.1,',\n",
       "  '//arxiv.org/pdf/2101.05967\\n[133] D. Peters, K. Vold, D. Robinson, and R. A. Calvo, “Responsible AI—Two\\nframeworksforethicaldesignpractice,”IEEETrans.Technol.Soc.,vol.1,\\nno. 1, pp. 34–47, Mar. 2020.\\n[134] V. Dignum, ed., Responsible Artiﬁcial Intelligence. Cham, Switzerland:\\nSpringer, 2019.\\n[135] C. Dwork, “Differential privacy: A survey of results,” in Proc. Int. Conf.\\nTheory Appl. Models Computation, 2008, pp. 1–19.\\n[136] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen, and H. Yu, “Federated\\nlearning,” Synth. Lectures Artif. Intell. Mach. Learn., vol. 13, no. 3,\\npp. 1–207, 2019.\\n[137] M. Kirienko et al., “Distributed learning: A reliable privacy-preserving\\nstrategy to change multicenter collaborations using AI,” Eur. J. Nucl.\\nMed. Mol. Imag., vol. 48, no. 12, pp. 3791–3804.2021.\\n[138] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in\\nProc. 22nd ACM SIGSAC Conf. Comput. Commun. Secur., 2015,\\npp. 1310–1321.',\n",
       "  'Med. Mol. Imag., vol. 48, no. 12, pp. 3791–3804.2021.\\n[138] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in\\nProc. 22nd ACM SIGSAC Conf. Comput. Commun. Secur., 2015,\\npp. 1310–1321.\\n[139] C. Meurisch, B. Bayrak, and M. Mühlhäuser, “Privacy-preserving AI\\nservices through data decentralization,” in Proc. Web Conf., 2020,\\npp. 190–200.\\n[140] UR-Lex - 02016R0679-20160504 - EN - EUR-Lex, 2016. Accessed:\\nJun. 28, 2021. [Online]. Available: https://eur-lex.europa.eu/legal-\\ncontent/EN/TXT/?uri=CELEX%3A02016R0679-20160504&qid=\\n1532348683434\\n[141] R. E. Latta, H.R.3388 - 115th Congress (2017-2018): SELF DRIVE\\nAct, 2017. Accessed: Jun. 28, 2021. [Online]. Available: https://www.\\ncongress.gov/bill/115th-congress/house-bill/3388\\n[142] 7. Lei No. 13, de 14 de Agosto de 2018, 2018. Accessed: Jun. 25, 2021.\\n[Online]. Available: http://www.planalto.gov.br/ccivil_03/_Ato2015-\\n2018/2018/Lei/L13709.html\\n[143] EUR-Lex - 52021PC0206 - EN - EUR-Lex, 2021. Accessed: Jun. 28,',\n",
       "  '[Online]. Available: http://www.planalto.gov.br/ccivil_03/_Ato2015-\\n2018/2018/Lei/L13709.html\\n[143] EUR-Lex - 52021PC0206 - EN - EUR-Lex, 2021. Accessed: Jun. 28,\\n2021. [Online]. Available: https://eur-lex.europa.eu/legal-content/EN/\\nTXT/?qid=1623335154975&uri=CELEX%3A52021PC0206\\n[144] C. Allen, G. Varner, and J. Zinser, “Prolegomena to any future artiﬁcial\\nmoral agent,” J. Exp. Theor. Artif. Intell., vol. 12, no. 3, pp. 251–261,\\n2000.\\n[145] A. M. Turing, “Computing machinery and intelligence,” Mind, vol. LIX,\\nno. 236, pp. 433–460, 1950.\\n[146] W. Wallach and C. Allen, Moral Machines: Teaching Robots Right From\\nWrong. Oxford, U.K.: Oxford Univ. Press, 2009.\\n[147] S. A. Seshia, D. Sadigh, and S. S. Sastry, “Towards veriﬁed artiﬁcial\\nintelligence,” Jun. 2016. [Online]. Available: http://arxiv.org/pdf/1606.\\n08514v4\\n[148] T. Arnold and M. Scheutz, “Against the moral turing test: Accountable\\ndesign and the moral reasoning of autonomous systems,” Ethics Inf.',\n",
       "  '08514v4\\n[148] T. Arnold and M. Scheutz, “Against the moral turing test: Accountable\\ndesign and the moral reasoning of autonomous systems,” Ethics Inf.\\nTechnol., vol. 18, no. 2, pp. 103–115, 2016.\\n[149] ACM Code of Ethics and Professional Conduct, 2018. Accessed: Jun. 25,\\n2021. [Online]. Available: https://www.acm.org/code-of-ethics\\n[150] IEEE SA- The IEEE Global Initiative on Ethicsof Autonomousand Intel-\\nligent Systems, 2019. Accessed: Jun. 28 2021. [Online]. Available: https:\\n//standards.ieee.org/industry-connections/ec/autonomous-systems.html\\n[151] IEEE Ethics In Action | Ethically Aligned Design, IEEE 7000TM\\nProjects, 2020. Accessed: Jun. 28, 2021. [Online]. Available: https:\\n//ethicsinaction.ieee.org/p7000/\\n[152] ISO, ISO/IEC JTC 1/SC 42 - Artiﬁcial intelligence, 2017. Accessed:\\nJun. 28, 2021. [Online]. Available: https://www.iso.org/committee/\\n6794475.html\\n[153] B. Goehring, F. Rossi, and D. Zaharchuk, “Advancing AI ethics beyond',\n",
       "  'Jun. 28, 2021. [Online]. Available: https://www.iso.org/committee/\\n6794475.html\\n[153] B. Goehring, F. Rossi, and D. Zaharchuk, “Advancing AI ethics beyond\\ncompliance: From principles to practice,” IBM Corporation, Apr. 2020.\\nAccessed: Apr. 19, 2022. [Online]. Available: https://www.ibm.com/\\nthought-leadership/institute-business-value/report/ai-ethics\\n[154] Responsible AI, 2017. Accessed: Apr. 19, 2022. [Online]. Available:\\nhttps://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:\\nprimaryr6\\n[155] F. Allhoff, “Evolutionary ethics from Darwin to Moore,” Hist. Philosophy\\nLife Sci., vol. 25, no. 1, pp. 51–79, 2003.',\n",
       "  'Vol.:(0123456789)\\n1 3\\nJournal of Business Ethics (2023) 185:725–740 \\nhttps://doi.org/10.1007/s10551-023-05339-7\\nORIGINAL PAPER\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful \\nWork\\nSarah\\xa0Bankins1\\u200a \\xa0· Paul\\xa0Formosa2\\nReceived: 17 February 2021 / Accepted: 25 January 2023 / Published online: 11 February 2023 \\n© The Author(s) 2023, corrected publication 2023\\nAbstract\\nThe increasing workplace use of artificially intelligent (AI) technologies has implications for the experience of meaningful \\nhuman work. Meaningful work refers to the perception that one’s work has worth, significance, or a higher purpose. The \\ndevelopment and organisational deployment of AI is accelerating, but the ways in which this will support or diminish oppor-\\ntunities for meaningful work and the ethical implications of these changes remain under-explored. This conceptual paper is',\n",
       "  'tunities for meaningful work and the ethical implications of these changes remain under-explored. This conceptual paper is \\npositioned at the intersection of the meaningful work and ethical AI literatures and offers a detailed assessment of the ways \\nin which the deployment of AI can enhance or diminish employees’ experiences of meaningful work. We first outline the \\nnature of meaningful work and draw on philosophical and business ethics accounts to establish its ethical importance. We \\nthen explore the impacts of three paths of AI deployment (replacing some tasks, ‘tending the machine’, and amplifying human \\nskills) across five dimensions constituting a holistic account of meaningful work, and finally assess the ethical implications. \\nIn doing so we help to contextualise the meaningful work literature for the era of AI, extend the ethical AI literature into the \\nworkplace, and conclude with a range of practical implications and future research directions.',\n",
       "  'workplace, and conclude with a range of practical implications and future research directions.\\nKeywords\\u2002 Meaningful work\\xa0· Artificial intelligence (AI)\\xa0· Ethical AI\\xa0· Future of work\\xa0· Technology and work\\nIntroduction\\nIncreasing organisational use of artificially intelligent (AI) \\ntechnologies will influence how people experience work \\n(World Economic Forum [WEF], 2018), including how and \\nwhether they experience meaningfulness in their work. AI \\nis the ability of computers and other artificial entities to do \\nthings typically classified as requiring intelligence were a \\nhuman to do them, such as reason, plan, problem solve, and \\nlearn from experience (Wang, 2019). Meaningful work is \\nthe perception that one’s work has worth, significance, or a \\nhigher purpose (Michaelson et\\xa0al., 2014), and this typically \\nrequires the coordinated exercise of varied and complex \\nskills to benefit others. Providing opportunities for mean-\\ningful work supports positive outcomes for workers (Allan',\n",
       "  'requires the coordinated exercise of varied and complex \\nskills to benefit others. Providing opportunities for mean-\\ningful work supports positive outcomes for workers (Allan \\net\\xa0al., 2019) and is ethically important as a basis for human \\nwellbeing and flourishing (Bailey et\\xa0al., 2019; Lysova et\\xa0al., \\n2019). However, despite becoming an increasingly prevalent \\nfeature of workplaces, there remains a poor understanding of \\nhow AI use will influence opportunities for meaningful work \\nand the ethical implications of such changes.\\nHistorically technological advancements have, since at \\nleast the first\\xa0industrial revolution, significantly changed \\nopportunities for meaningful work by altering what work-\\ners do, the nature of their skills, and their feelings of aliena-\\ntion from or integration with the production process (Vallor, \\n2015). AI use will likely extend such changes, but its unique \\nfeatures and uses also generate new and conflicting impli-',\n",
       "  'tion from or integration with the production process (Vallor, \\n2015). AI use will likely extend such changes, but its unique \\nfeatures and uses also generate new and conflicting impli-\\ncations for meaningful work. Optimistic accounts suggest \\nthat\\xa0AI will expand the range of meaningful higher-order \\nhuman work tasks (WEF, 2018), whereas more pessimis-\\ntic accounts suggest that\\xa0AI will degrade and even elimi-\\nnate human work (Frey & Osborne, 2017). These ongoing \\ntensions point to a lack of conceptual clarity regarding the \\n *\\t Sarah Bankins \\n\\t\\nsarah.bankins@mq.edu.au\\n\\t\\nPaul Formosa \\n\\t\\npaul.formosa@mq.edu.au\\n1\\t\\nDepartment of\\xa0Management, Macquarie Business School, \\nMacquarie University, North Ryde Campus, Sydney, \\nNSW\\xa02109, Australia\\n2\\t\\nDepartment of\\xa0Philosophy, Faculty of\\xa0Arts, Macquarie \\nUniversity, North Ryde Campus, Sydney, NSW\\xa02109, \\nAustralia',\n",
       "  '726\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nimpacts of AI on meaningful work, leading to calls for more \\nresearch in this area (Parker & Grote, 2022).\\nThis conceptual paper aims to help\\xa0address such gaps \\nby examining how workplace use of AI has the potential \\nto both enhance and diminish experiences of meaningful \\nwork, depending largely on the implementation choices of \\nemployers. This research is positioned at the intersection of \\nthe meaningful work and ethical AI literatures and makes \\ntwo key contributions. First, we contextualise the meaning-\\nful work literature for the era of AI by developing concep-\\ntual resources to examine how the implementation of such \\ntechnologies affects workers’ opportunities for meaningful \\nwork and connect this assessment to the ethical implications \\nof these changes. Second, we help remedy a neglected aspect \\nof the ethical AI literature by offering a detailed examination \\nof AI’s implications for meaningful work.',\n",
       "  'of these changes. Second, we help remedy a neglected aspect \\nof the ethical AI literature by offering a detailed examination \\nof AI’s implications for meaningful work.\\nWe begin by outlining the nature of meaningful work and \\nits ethical importance, integrating philosophical and busi-\\nness ethics accounts. We then examine the impacts of three \\npaths of AI deployment—replacing some simple and com-\\nplex tasks (replacement), ‘tending the machine’ (creating \\nnew forms of human work), and amplifying human skills \\n(augmenting/assisting workers)—across five dimensions \\nof meaningful work. These dimensions integrate both job-\\nspecific (through Hackman & Oldham’s, 1976 job charac-\\nteristics model) and more holistic (through Lips-Wiersma & \\nMorris’, 2009 model) drivers of meaningful work. We then \\ndevelop the ethical implications of our analysis by drawing \\non the AI4People ethical AI framework (Floridi et\\xa0al., 2018) \\nand its five principles of beneficence, non-maleficence,',\n",
       "  'develop the ethical implications of our analysis by drawing \\non the AI4People ethical AI framework (Floridi et\\xa0al., 2018) \\nand its five principles of beneficence, non-maleficence, \\nautonomy, justice, and explicability. We conclude with prac-\\ntical insights into how experiences of meaningful work will \\nchange as AI becomes more widespread and offer several \\ndirections for future research.\\nAI and\\xa0Work: Uses and\\xa0Unique Features\\nCurrent AIs constitute artificial narrow intelligence, or AIs \\nthat can undertake actions only within restricted domains, \\nsuch as classifying pictures of cats (Boden, 2016). The “holy \\ngrail” of AI research is artificial general intelligence (Boden, \\n2016), or AIs that can perform at least as well as humans \\nacross the full range of intelligent activities. We focus only \\non narrow AI as it is already used across many diverse sec-\\ntors, including in healthcare, judicial, educational, manu-\\nfacturing, and military contexts, among many others (see',\n",
       "  'on narrow AI as it is already used across many diverse sec-\\ntors, including in healthcare, judicial, educational, manu-\\nfacturing, and military contexts, among many others (see \\nBankins & Formosa, 2021; Bekey, 2012; Walsh et\\xa0al., 2019). \\nThe established use of narrow AI also allows us to draw on \\npractical examples to ground our assessment of its effects on \\nmeaningful work. While considering the possible implica-\\ntions of artificial general intelligence for meaningful work \\nis important, and we discuss this in our future research \\ndirections, there remain persistent disagreements about \\nwhen, if ever, it will be achieved (Boden, 2016). This makes \\nit critical to examine the impacts of current AI capabilities \\non opportunities for meaningful work that are occurring now \\nand in the near-term (Webster & Ivanov, 2020).\\nPast research demonstrates the dual positive and negative \\neffects of technology upon aspects of meaningful work. For',\n",
       "  'and in the near-term (Webster & Ivanov, 2020).\\nPast research demonstrates the dual positive and negative \\neffects of technology upon aspects of meaningful work. For \\nexample, technology use can upskill workers and enhance \\ntheir autonomy, but it can also deskill and serve to control \\nthem (Vallor, 2015; Mazmanian et\\xa0al., 2013), with meaning-\\nfulness generally elevated in the former case (Cheney et\\xa0al., \\n2008). Technology’s positive effects can also help individu-\\nals confirm pre-existing notions of meaningful work, but \\nits negative outcomes can require them to re-interpret and \\nadjust those meanings as the technology’s dual effects are \\nrealised, for example by providing on-demand connection to \\nwork but heightening distraction from other responsibilities \\n(Symon & Whiting, 2019). Such dual effects remain evident \\nin advancing forms of technology, such as workplace robot-\\nics that offer both benefits and threats to meaningful human \\nwork (see Smids et\\xa0al., 2020).',\n",
       "  'in advancing forms of technology, such as workplace robot-\\nics that offer both benefits and threats to meaningful human \\nwork (see Smids et\\xa0al., 2020).\\nThese findings are critical, but their focus is on broader \\ntypes of information and communication technologies, \\nwhereas we focus specifically upon AI and its implications \\nfor meaningful work. While AI use should also generate \\nthese types of dual effects, its unique features warrant spe-\\ncific attention. For example, compared to past technologies \\nAI can undertake more cognitive tasks, expanding beyond \\n‘blue collar’ work in manufacturing where technology’s role \\nin replacing human labour has a long history, and into more \\n‘white collar’ forms of work (Bankins & Formosa, 2020). \\nFurther, machine learning in AIs is often driven by large \\namounts of data, the acquisition of which raises serious con-\\ncerns about privacy, consent, and surveillance, with impli-\\ncations for worker autonomy (Bailey et\\xa0al., 2019). Potential',\n",
       "  'amounts of data, the acquisition of which raises serious con-\\ncerns about privacy, consent, and surveillance, with impli-\\ncations for worker autonomy (Bailey et\\xa0al., 2019). Potential \\nbiases in data collection, the use of AI models built from \\nbiased data, and the resultant replication of systemic injus-\\ntices (Walsh et\\xa0al., 2019), as already evidenced in some AI-\\ndriven recruitment practices (Dastin, 2018), raises further \\nconcerns about the potential for one’s AI-informed work \\nto harm others. The potential for such harms is then exac-\\nerbated given the scale at which AI can be deployed. The \\nway AIs expand opportunities to manipulate and control \\nhumans also raises important issues (Susser et\\xa0al., 2019), \\nparticularly through the way it can act as an information \\ngatekeeper for human workers (Kellogg et\\xa0al., 2020). Finally, \\nthe ‘blackbox’ nature of the neural networks many AIs use \\nmeans end-users and even AI developers cannot understand',\n",
       "  'gatekeeper for human workers (Kellogg et\\xa0al., 2020). Finally, \\nthe ‘blackbox’ nature of the neural networks many AIs use \\nmeans end-users and even AI developers cannot understand \\nhow an AI generates its outputs (Jarrahi, 2019). This can \\nmake it difficult to trust AIs, to feel competent in working \\nalongside them, and to build responsible systems for which \\nhuman workers can be held meaningfully accountable (Dahl, \\n2018). These features of AI have attendant consequences',\n",
       "  '727\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nfor meaningful work that we will explore. We first turn to \\nexplaining the components of meaningful work and its ethi-\\ncal importance.\\nWhat Constitutes Meaningful Work?\\nSeveral approaches outline what constitutes meaningful \\nwork. One dominant task-based framework is Hackman and \\nOldham’s (1976) job characteristics model (JCM), which \\nexamines how job and task design influences experiences \\nof meaningfulness in work (Pratt & Ashforth, 2003).1 Other \\nframeworks extend beyond a task focus to adopt a more \\n“humanistic” approach (Lips-Wiersma & Morris, 2009, p. \\n493). For example, Lips-Wiersma and Morris (2009) suggest \\nthat\\xa0meaningful work derives from finding balance between \\n“being (true to self)-doing (making a difference)” and a \\nfocus on “self (self-actualisation)-others (serving others)”. \\nThis creates the meaningful work dimensions of “developing',\n",
       "  '“being (true to self)-doing (making a difference)” and a \\nfocus on “self (self-actualisation)-others (serving others)”. \\nThis creates the meaningful work dimensions of “developing \\nand becoming self”, “serving others”, “unity with others”, \\nand “expressing one’s full potential” (Lips-Wiersma & Mor-\\nris, 2009, p. 501).\\nTo adopt a holistic approach for exploring the impacts of \\nAI on meaningful work we integrate aspects of meaningful \\njob design from the JCM (Hackman & Oldham, 1976) with \\ndimensions of work that facilitate the more wide-ranging \\nenhancement of oneself through development, contribu-\\ntion, and connection to others from Lips-Wiersma and \\nMorris’ (2009) framework. This harmonisation generates \\nfive meaningful work dimensions that we focus our analy-\\nsis upon.2 The first dimension we label task integrity. This \\nencompasses task identity from the JCM, or the range of \\ntasks an individual does and the opportunity to complete',\n",
       "  'sis upon.2 The first dimension we label task integrity. This \\nencompasses task identity from the JCM, or the range of \\ntasks an individual does and the opportunity to complete \\na whole piece of work. This ability to undertake integrated \\nrather than fragmented tasks then influences the extent to \\nwhich workers can fully develop themselves, their capaci-\\nties, and express their full potential as an integrated whole \\nperson (“developing and becoming self” and “expressing full \\npotential” from Lips-Wiersma & Morris, 2009). The second \\ndimension we label skill cultivation and use. This encom-\\npasses skill variety and use from the JCM, or the ability to \\nuse and develop a range of skills at work. Like the types of \\ntasks to which they are applied, prospects for skill utilisation \\nthen influence opportunities for growth through learning and \\nthe broader cultivation of the self and one’s potential via \\ndeveloping, testing, and exercising a varied range of com-',\n",
       "  'then influence opportunities for growth through learning and \\nthe broader cultivation of the self and one’s potential via \\ndeveloping, testing, and exercising a varied range of com-\\npetencies (“developing and becoming self” and “expressing \\nfull potential” from Lips-Wiersma & Morris, 2009).\\nThe third dimension is task significance (per the JCM) \\nwhich connects one’s work to the wider world. This dimen-\\nsion reflects the extent to which individuals can see how \\ntheir work benefits, and contributes to the betterment of, oth-\\ners (“serving others” from Lips-Wiersma & Morris, 2009). \\nThe fourth dimension is autonomy (per the JCM), which \\nreflects how freely individuals can determine their work \\napproaches and the extent of their freedom from intrusive \\nsurveillance and monitoring. The more autonomy workers \\nexperience the greater their capacity to engage in activi-\\nties like job crafting to enhance fit between individual needs \\nand job requirements, and to undertake work that fosters',\n",
       "  'experience the greater their capacity to engage in activi-\\nties like job crafting to enhance fit between individual needs \\nand job requirements, and to undertake work that fosters \\nself-development, moral cultivation, and that affords align-\\nment with one’s values (“developing and becoming self” and \\n“expressing full potential” from Lips-Wiersma & Morris, \\n2009). The final dimension is belongingness, reflecting the \\nways that work can help us feel connected to a wider group \\nto generate meaningfulness through a sense of unity with \\nothers (Bailey et\\xa0al., 2019; Lips-Wiersma & Morris, 2009; \\nMartela & Riekki, 2018). Now that we know what underpins \\nexperiences of meaning in work, we can turn to explaining \\nthe ethical dimensions of both meaningful work and AI.\\nThe Ethics of\\xa0Meaningful Work and\\xa0Ethical AI\\nRecent philosophical discussions of meaningfulness tend to \\nfocus on what makes life itself, or the activities and rela-\\ntionships that compose a well-lived life, meaningful (Wolf,',\n",
       "  'Recent philosophical discussions of meaningfulness tend to \\nfocus on what makes life itself, or the activities and rela-\\ntionships that compose a well-lived life, meaningful (Wolf, \\n2010). The paradigm of meaningless work is Sisyphus, who \\nis condemned as punishment to repeatedly roll a rock to the \\ntop of a mountain (Camus, 1955). Sisyphus’ work is bor-\\ning, repetitive, simple, does not benefit others, and is not \\nfreely chosen.3 By implication, meaningful work should be \\nengaging, varied, require the use of complex skills, benefit \\nothers, and be freely chosen. This emphasises two aspects \\nof meaningfulness that Wolf (2010) calls subjective (do you \\nexperience work as meaningful?) and objective (is the work \\nactually meaningful?) elements. As we take meaningful \\nwork to be “personally significant and worthwhile” (Lys-\\nova et\\xa0al., 2019, p. 375), our definition is inclusive of these \\n1\\u2002 Pratt and Ashforth (2003) also discuss meaningfulness at work, or',\n",
       "  'work to be “personally significant and worthwhile” (Lys-\\nova et\\xa0al., 2019, p. 375), our definition is inclusive of these \\n1\\u2002 Pratt and Ashforth (2003) also discuss meaningfulness at work, or \\nthe ways leaders craft and convey organisational values to build feel-\\nings of organisational membership. To maintain a manageable scope, \\nour analysis only examines meaning in work, which is largely driven \\nby job design.\\n2\\u2002 The job characteristics model also includes feedback. We draw on \\nthe model’s first three aspects as they are theorised to directly gen-\\nerate the psychological state of experienced meaningfulness at work, \\nand both autonomy and belongingness are viewed in the wider litera-\\nture as other critical components of meaningful work. See Parker and \\nGrote (2022) for an assessment of technology’s impact on feedback \\nat work.\\n3\\u2002 Of course, Sisyphus’ story is more complicated than this, with \\nCamus (1955) arguing that Sisyphus finds a form of happiness in his',\n",
       "  'at work.\\n3\\u2002 Of course, Sisyphus’ story is more complicated than this, with \\nCamus (1955) arguing that Sisyphus finds a form of happiness in his \\nscornful embrace of the absurdity of his condition.',\n",
       "  '728\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nsubjective (it is personally significant) and objective (it is \\nworthwhile) aspects.\\nThe Ethical Implications of\\xa0Meaningful Work: Why \\nis\\xa0it Ethically Important?\\nLiterature in business ethics and political philosophy explore \\nthe ethical significance of meaningful work (Michaelson \\net\\xa0al., 2014). Meaningful work can be viewed as ethically \\nsignificant either because it is intrinsically valuable (first \\nbasis), or because it is a constitutive element of a broader \\ngood (second basis), or because it is an instrumental good \\nthat leads to other valuable goods (third basis) (Michaelson \\net\\xa0al., 2014). From these three bases we can see that there \\nare good grounds for holding meaningful work to be ethi-\\ncally important across each of our three most used ethical \\ntheories: Kantian ethics, Virtue Theory, and Utilitarianism.\\nRegarding the first basis, Kantian ethical theories focus \\non treating people with dignity and respect as rational',\n",
       "  'theories: Kantian ethics, Virtue Theory, and Utilitarianism.\\nRegarding the first basis, Kantian ethical theories focus \\non treating people with dignity and respect as rational \\nagents who have normative authority over their lives, and \\nthis includes imperfect duties to promote and develop the \\nrational capacities and self-chosen ends of moral agents \\n(Formosa, 2017). Meaningful work is ethically significant \\nas it provides an important way to develop and exercise one’s \\nrational capacities and use them in ways that help others to \\nmeet their ends. Bowie (1998, p. 1083) identifies six features \\nof meaningful work that explain why Kantians should care \\nabout it, including that the work is “freely entered into”, \\n“not paternalistic”, ‘‘provides a wage sufficient for physi-\\ncal welfare”, allows workers to exercise their “autonomy \\nand independence”, “develop” their “rational capacities”, \\nand promotes their “moral development”. In terms of the',\n",
       "  'cal welfare”, allows workers to exercise their “autonomy \\nand independence”, “develop” their “rational capacities”, \\nand promotes their “moral development”. In terms of the \\nsecond basis, many virtue ethicists argue that meaningful \\nwork is an integral part of flourishing as a human being. \\nFor example, Nussbaum (2011) argues that “being able to \\nwork as a human being” is a central human capability. This \\nmeans being able to exercise our practical reason, use our \\nsenses, imagination and thought, have some control over our \\nwork environment, and being able to have “meaningful rela-\\ntions of mutual recognition with other workers” (Nussbaum, \\n2011, p. 34). The capability to pursue meaningful work is \\nthus an important right and component of human flourish-\\ning. In terms of the third basis, evidence shows the positive \\ninstrumental impacts that meaningful work has on wellbeing \\nand a range of other goods (Allan et\\xa0al., 2019). This gives us',\n",
       "  'ing. In terms of the third basis, evidence shows the positive \\ninstrumental impacts that meaningful work has on wellbeing \\nand a range of other goods (Allan et\\xa0al., 2019). This gives us \\ngood reasons to care about meaningful work for the sake of \\nother important goods it contributes to and promotes, such \\nas human wellbeing, that are valued on a range of ethical \\ntheories, including Utilitarianism.\\nOverall, according to all three of our most used moral \\ntheories there are good reasons to care about meaningful \\nwork given that it respects workers’ autonomy and their abil-\\nity to exercise complex skills in helping others, contributes \\nto their wellbeing, and allows them to flourish as complex \\nhuman beings. Given its ethically valuable nature, it fol-\\nlows that organisations have strong pro tanto reasons to \\npromote, support, and offer meaningful work (Michaelson \\net\\xa0al., 2014). Of course, pro tanto reasons are not indefeasi-\\nble reasons, and so other considerations may outweigh them,',\n",
       "  'promote, support, and offer meaningful work (Michaelson \\net\\xa0al., 2014). Of course, pro tanto reasons are not indefeasi-\\nble reasons, and so other considerations may outweigh them, \\nsuch as improved efficiency, which means changes that lead \\nto less meaningful work are not necessarily unethical. Fur-\\nther, some workers may be willing to trade off less mean-\\ningful work for other gains, such as more income or leisure \\ntime. Even so, meaningful work remains ethically important \\nand changes that impact the amount of meaningful work \\nfor humans\\xa0must be taken into ethical account, even if such \\nconsiderations are not always overriding.\\nThe Ethical Implications of\\xa0AI Use\\nGiven the ethical importance of meaningful work, more \\nscholarship is needed to explore the potential impacts of \\nAI upon it. The ethical significance of AI use is widely \\nrecognised and discussed (see\\xa0Floridi et\\xa0al., 2018; Hagen-\\ndorff, 2020; Jobin et\\xa0al., 2019), leading to various organi-',\n",
       "  'AI upon it. The ethical significance of AI use is widely \\nrecognised and discussed (see\\xa0Floridi et\\xa0al., 2018; Hagen-\\ndorff, 2020; Jobin et\\xa0al., 2019), leading to various organi-\\nsational, national, and international documents outlining \\nethical principles for AI deployment. However, AI’s effects \\non meaningful work are not a focus of any of these prin-\\nciples. For example, Jobin et\\xa0al.’s (2019) meta-analysis of \\nethical AI guidelines identifies 11 principles, but none men-\\ntion meaningful work directly. Hagendorff’s (2020) analysis \\nalso does not identify it, although related issues around the \\n“future of employment” are discussed. An analysis by Ryan \\nand Stahl (2020, p. 67) mentions the need to “retrain and \\nretool” human workers who are fully replaced by AI, but this \\nsidelines human-AI collaborations in workplaces and AI’s \\nbroader impacts on meaningful work. The AI4People frame-\\nwork also makes no direct\\xa0mention of meaningful work, but',\n",
       "  'sidelines human-AI collaborations in workplaces and AI’s \\nbroader impacts on meaningful work. The AI4People frame-\\nwork also makes no direct\\xa0mention of meaningful work, but \\nit does note the possibility of AI liberating people from the \\n“drudgery” of some work (Floridi et\\xa0al., 2018, p. 691).\\nWhile these frameworks do not mention meaningful work \\nexplicitly, we can nonetheless draw on them to identify ethi-\\ncal concerns that AI’s impacts on meaningful work raise. \\nTo do this we draw on the AI4People ethical AI framework \\n(Floridi et\\xa0al., 2018) and its five principles of beneficence, \\nnon-maleficence, autonomy, justice, and explicability. We \\nutilise this widely discussed framework as it emerged from \\na robust consensus-building program to formulate ethical \\nAI principles. The framework’s focus on the impacts of AI \\non “human dignity and flourishing” across its elements of \\n“autonomous self-realisation… human agency… individual',\n",
       "  'AI principles. The framework’s focus on the impacts of AI \\non “human dignity and flourishing” across its elements of \\n“autonomous self-realisation… human agency… individual \\nand societal capabilities... [and] societal cohesion” (Floridi \\net\\xa0al., 2018, p. 690) also fits our focus, given that the impacts \\nof AI on dignity, autonomous agency, social cohesion, skills \\nand capabilities, and human flourishing all relate to our \\ndimensions of meaningful work. The foundational principles',\n",
       "  '729\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nof this framework (minus explicability) have also been uti-\\nlised in related work on the ethical design and deployment of \\nbroader information technologies (see Wright, 2011), which \\nagain emphasises the framework’s usefulness in our context.\\nThe five principles of the AI4People framework allow us \\nto explore the wide-ranging impacts of AI on meaningful \\nwork. The first principle is beneficence, or the benefits AI \\ncan bring toward promoting human wellbeing and preserv-\\ning human dignity in an environmentally sustainable way. \\nNon-maleficence is about ensuring that AI does not harm \\nhumanity, and this includes not violating individuals’ pri-\\nvacy and maintaining the safety and security of AI systems. \\nAutonomy is about giving humans the power to decide what \\nAI does. A linking concern between the first two princi-\\nples is the use of AI, intentionally or not, to cause harm',\n",
       "  'Autonomy is about giving humans the power to decide what \\nAI does. A linking concern between the first two princi-\\nples is the use of AI, intentionally or not, to cause harm \\nby interfering with and disrespecting human autonomy by \\n“nudging… human behaviour in undesirable ways” (Floridi \\net\\xa0al., 2018, p. 697). Nudging involves setting up the “choice \\narchitecture”, or decision context, to intentionally attempt to \\npush (or “nudge”) people to make certain choices (Thaler & \\nSunstein, 2008). Justice is about fairly distributing the bene-\\nfits and burdens from AI use and not undermining solidarity \\nand social cohesion. Finally, explicability is about ensuring \\nthat AI operates in ways that are intelligible and accountable, \\nso that we can understand how it works and we can require \\nsomeone to be responsible for its actions. In the context of \\nmeaningful work, these principles lead us to focus on the \\nbenefits and harms that AI can bring to workers, includ-',\n",
       "  'someone to be responsible for its actions. In the context of \\nmeaningful work, these principles lead us to focus on the \\nbenefits and harms that AI can bring to workers, includ-\\ning on their tasks, skills and social relations, the way AI \\nmight control, nudge, and manipulate workers’ autonomy, \\nthe distribution of the benefits and harms AI brings, and the \\nextent of intelligibility and accountability in AI workplace \\ndeployments.\\nOur overall conceptual framework is presented in Fig.\\xa01 \\nand our analysis is structured as follows. We first outline the \\nimpacts of AI on the five dimensions of meaningful work by \\nanalysing its effects through three pathways (outlined below) \\nfor AI deployment: replacement; ‘tending the machine’; and \\namplifying. We then turn to the AI4People ethical frame-\\nwork to draw out the ethical implications of these impacts \\non meaningful work. This structure allows us to focus in \\na systematic way on each important set of analyses, first',\n",
       "  'work to draw out the ethical implications of these impacts \\non meaningful work. This structure allows us to focus in \\na systematic way on each important set of analyses, first \\nrelated to AI’s effects on meaningful work and then the ethi-\\ncal implications of this, while highlighting how these effects \\nare often contingent on the ways in which AI is deployed.\\nThe Effects of\\xa0Artificial Intelligence \\non\\xa0Meaningful Work\\nAI represents a range of technologies that can be used in \\nmany ways alongside human workers doing many different \\ntasks. This makes it important to examine not only what \\ntasks the AI does, but also how human workers’ tasks change \\nfollowing AI deployment and the comparative meaningful-\\nness of their new work. While we briefly discuss the impacts \\nof full human replacement by AI upon meaningful work, \\nwe focus our analysis on meaningful work outcomes when \\nhumans work alongside AI.4 This is because such work con-',\n",
       "  \"of full human replacement by AI upon meaningful work, \\nwe focus our analysis on meaningful work outcomes when \\nhumans work alongside AI.4 This is because such work con-\\nfigurations already, and will increasingly, characterise many \\nworkplaces (Jarrahi, 2018) and reflects our focus on clear \\ncurrent and near-term impacts of narrow AI.\\nTechnology’s Effects on\\xa0Work: Three Paths\\nOur analytical framework adapts and expands Langlois' \\n(2003) characterisation of how technology integrates into a \\nwork process. This structures our analysis around three key \\npaths through which AI will shape humans’ experiences of \\nmeaningful work.\\nIn the first path, AI assumes some tasks (either simple or \\ncomplex) while workers remain engaged elsewhere in the \\nFig.\\u202f1\\u2002 \\u2009Overview of conceptual \\nframework\\nFive Dimensions of \\nMeaningful Work\\n1. Task integrity\\n2. Skill cultivation and\\nuse\\n3. Task significance\\n4. Autonomy\\n5. Belongingness\\nFive Ethical AI \\nPrinciples\\n1.\\nBeneficence\\n2.\\nNon-maleficence\\n3.\\nAutonomy\\n4.\",\n",
       "  'Meaningful Work\\n1. Task integrity\\n2. Skill cultivation and\\nuse\\n3. Task significance\\n4. Autonomy\\n5. Belongingness\\nFive Ethical AI \\nPrinciples\\n1.\\nBeneficence\\n2.\\nNon-maleficence\\n3.\\nAutonomy\\n4.\\nJustice\\n5.\\nExplicability\\nThree AI \\nImplementation \\nPathways\\n1. Replacing\\n2. Tending the \\nmachine\\nManaging the \\nmachine\\nMinding the \\nmachine\\n3. Amplifying\\nThe implementation \\nof AI impacts \\nmeaningful work \\ndimensions in \\ndifferent ways\\nThe outcomes of AI \\nimplementation on \\nmeaningful work are \\nethically assessed\\n•\\n•\\n4\\u2002 We do not significantly detail the effects of AI fully replacing a \\nworker because, at least currently, AI is unlikely to predominantly \\nautomate entire jobs (Chui et\\xa0al., 2015). But where this does occur \\nthe impacts are clear, the unemployed worker has lost meaning-\\nful paid work until they can find another job (which may offer more \\nopportunities for meaningful work, see Cheney et\\xa0 al., 2008). This \\nalso raises broader issues, beyond our scope, around other sources of',\n",
       "  'opportunities for meaningful work, see Cheney et\\xa0 al., 2008). This \\nalso raises broader issues, beyond our scope, around other sources of \\nmeaningfulness if increasingly sophisticated AI makes paid meaning-\\nful work rarer (see Bruun & Duka, 2018).',\n",
       "  '730\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\n(roughly similar) work process. This is akin to AI replacing \\nhumans in some tasks. For example, if a personalised maths \\nlearning app is introduced in a classroom, the teacher may \\nre-focus upon other existing tasks (e.g., more time for les-\\nson planning) or undertake new work (e.g., individualised \\nmaths coaching), but the overall work process of ‘teach-\\ning’ remains similar (see such examples in\\xa0Acemoglu & \\nRestrepo, 2020). We also focus on the two ends of the skills \\nspectrum for illustrative purposes (i.e., simple and complex \\ntasks), and acknowledge that tasks will likely involve vari-\\nous skills. The key difference between this path and the next \\nis that here the replacement work undertaken by humans is \\nnot focused on managing the AI, but in the next path it is.\\nIn the second path, AI assumes a set of tasks resulting in \\nnew human work focused on “tending the machine” (Lan-\\nglois, 2003, p. 175). This is akin to creating new types of',\n",
       "  'In the second path, AI assumes a set of tasks resulting in \\nnew human work focused on “tending the machine” (Lan-\\nglois, 2003, p. 175). This is akin to creating new types of \\ntasks for workers.5 We further divide ‘tending the machine’ \\ninto two emerging forms of work associated with manag-\\ning AI: (1) what we term ‘managing the machine’, which \\ngenerates new, complex, and interesting forms of work for \\nhumans; and (2) what Langlois (2003, p. 175) terms “mind-\\ning the machine”, which generates more mundane, rote, and \\nlower-skilled work for humans. Again, we focus on two ends \\nof a spectrum for illustrative purposes, while acknowledging \\nthat human work may exist across both categories. ‘Man-\\naging the machine’ reflects integrated and complex work, \\nsuch as “coordination and buffering” roles (Langlois, 2003, \\np. 175), as well as trainer, explainer, and sustainer roles \\n(Daugherty & Wilson, 2018). Examples include: manag-\\ning the interactions between data, the wider organisation,',\n",
       "  'p. 175), as well as trainer, explainer, and sustainer roles \\n(Daugherty & Wilson, 2018). Examples include: manag-\\ning the interactions between data, the wider organisation, \\nand other stakeholders (coordination and buffering); train-\\ning the AI to complete tasks and training others in AI use \\n(training); explaining and interpreting the AI’s operation \\nand outputs to stakeholders (explaining); and ensuring the \\nsystem’s continued explainability, accountability, and fair-\\nness (sustaining) (Daugherty & Wilson, 2018). In contrast, \\n‘minding the machine’ work involves tasks such as “AI prep-\\naration” (sourcing, annotating, and labelling data) and “AI \\nverification” through validating AI output (such as checking \\nimage recognition accuracy) (Tubaro et\\xa0al., 2020, p. 1). This \\ntype of work tends to reflect fragmented and disconnected \\nmicro-work tasks that are often outsourced to low wage and \\nlow skill workers (Tubaro et\\xa0al., 2020), leading to charac-',\n",
       "  'type of work tends to reflect fragmented and disconnected \\nmicro-work tasks that are often outsourced to low wage and \\nlow skill workers (Tubaro et\\xa0al., 2020), leading to charac-\\nterisations of “janitor work” and new digitalised forms of \\nTaylorism (Jarrahi, 2019, p. 183).\\nIn the third path, AI ‘amplifies’ or ‘assists’ workers \\nby improving how human workers do their existing work \\n(Daugherty & Wilson, 2018). This is akin to AI assisting \\nworkers with their tasks and/or augmenting and enhanc-\\ning workers’ abilities. Here AI is neither assuming specific \\ntasks that a human previously did (as in the first path) nor \\ndoes managing the AI constitute a worker’s primary role \\n(as in the second path), but rather the technology assists the \\nworker to do her existing work better. For example, the AI \\nCorti provides real-time assistance to emergency operators \\nby analysing callers’ responses to questions, assessing the \\nseverity of their condition, and recommending actions to the',\n",
       "  'Corti provides real-time assistance to emergency operators \\nby analysing callers’ responses to questions, assessing the \\nseverity of their condition, and recommending actions to the \\noperator based on modelling of thousands of previous calls \\n(Formosa & Ryan, 2021). This amplifies, in a significant \\nnew way, the abilities of emergency operators to determine \\noptimal responses. The use of AI to amplify a human worker \\naccords with Zuboff’s (1988) “informating” powers of tech-\\nnology, whereby it improves humans’ access to integrated \\nand more meaningful forms of data, often cross-functionally, \\nto generate new insights (see Jarrahi, 2019).\\nWe now analyse how, through each of these three deploy-\\nment pathways, AI use will impact the five dimensions of \\nmeaningful work. While individual jobs could experience \\nelements of all three paths (e.g., some replacing, some ‘tend-\\ning the machine’ work, and some amplifying) and some \\noverlap may occur (e.g., AI replacing a rote human task also',\n",
       "  'elements of all three paths (e.g., some replacing, some ‘tend-\\ning the machine’ work, and some amplifying) and some \\noverlap may occur (e.g., AI replacing a rote human task also \\nassists the worker), we discuss each path as distinct for ana-\\nlytical purposes. The ethical implications of these impacts \\nare then assessed in the subsequent section.\\nTask Integrity and\\xa0Skill Cultivation and\\xa0Use\\nWorkers’ tasks can range from being highly fragmented to \\nbeing highly integrated, and the diversity of skills they can \\nactivate will also vary as a result. Both of\\xa0these aspects gen-\\nerate opportunities to achieve and develop one’s abilities \\nand potential through work (Lips-Wiersma & Morris, 2009). \\nAs the nature of what a worker does (i.e., tasks) strongly \\nimpacts what they need to do that work (i.e., skills), we dis-\\ncuss these two dimensions together.\\nFirst, we consider the path of AI taking over some tasks \\nwhile leaving workers engaged in other work. The tasks',\n",
       "  'cuss these two dimensions together.\\nFirst, we consider the path of AI taking over some tasks \\nwhile leaving workers engaged in other work. The tasks \\nthe AI assumes could be simple or complex (or anything in \\nbetween), but the predominance of narrow AI means it is \\nmainly deployed to replace humans in specific narrow tasks. \\nAn espoused benefit of AI is its ability to undertake simple \\ntasks that are often boring and unchallenging for humans, \\nsuch as collating information for meetings (Pulse\\u2009+\\u2009IT, \\n2020) or assessing fruit quality (Roberts, 2020). Deploying \\nAI in this way is unlikely to generate significant feelings \\nof marginalisation from a wider work process due to the \\nsimple nature of the tasks it is assuming, particularly when \\nthe human takes on other comparable or more interesting \\nwork. This should result in neutral or improved perceptions \\n5\\u2002 We acknowledge that other forms of new human work are also \\nlikely to emerge (see Acemoglu & Restrepo, 2020), but its nature',\n",
       "  'work. This should result in neutral or improved perceptions \\n5\\u2002 We acknowledge that other forms of new human work are also \\nlikely to emerge (see Acemoglu & Restrepo, 2020), but its nature \\nremains speculative. New work associated with AI management \\nalready exists or is emerging, aligning with our focus on near-term \\nwork implications of AI.',\n",
       "  '731\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nof task integrity and may free workers’ time to engage in \\nmore learning and development.\\nHowever, when AI assumes more complex and significant \\ntasks then its implications, both positive and negative, may \\nbe more profound. For example, in human resource man-\\nagement an AI can shortlist candidates to progress to inter-\\nviews based on natural language processing of applications \\n(Bankins, 2021; Leicht-Deobald et\\xa0al., 2019). Shortlisting \\napplicants can be a complex and significant component of \\nthe recruitment and selection process. Using AI for this task \\ncould then degrade workers’ experiences of task integrity as \\nthey no longer undertake a significant part of a work process, \\nassuming this work is not comparably replaced. Shifting \\nworkers to other more rote tasks, despite adding work that \\nmaintains their level of involvement in the work process, is',\n",
       "  'assuming this work is not comparably replaced. Shifting \\nworkers to other more rote tasks, despite adding work that \\nmaintains their level of involvement in the work process, is \\nalso likely to compound feelings of reduced task integrity, as \\nthe worker moves from undertaking more significant to less \\nsignificant work. This can also limit the scope for workers to \\ndevelop and express their full capabilities at work and reduce \\ntheir opportunities for growth.\\nIn contrast, if workers shift to new but similarly complex \\nor even more significant tasks elsewhere in the work process, \\nthen this should support task integrity as the worker contin-\\nues to contribute meaningfully to work outcomes. For exam-\\nple, the AI ‘AlphaFold’ developed by DeepMind is designed \\nto automate and accelerate the process of determining pro-\\ntein structures, an important step in developing new treat-\\nments for human diseases (Hassabis & Revell, 2021). While',\n",
       "  'to automate and accelerate the process of determining pro-\\ntein structures, an important step in developing new treat-\\nments for human diseases (Hassabis & Revell, 2021). While \\nAlphaFold can assume significant tasks previously done by \\nhuman scientists (i.e., determining protein structures) this \\nshould positively impact, or at least have a neutral effect, \\non task integrity if it allows scientists to re-focus their work \\nefforts on other important aspects of their broader goal of \\ncuring diseases. However, there remain risks to AI being \\nused in this way. Continuing with this example, if scientists \\nhave trained for many years to do the experimental work \\nthat AlphaFold can now do more quickly and accurately, \\nthis generates significant risks for their ability to exercise \\ntheir full capacities, demonstrate their mastery, and utilise \\nthe skills they have invested years in developing to reach \\ntheir full potential.\\nChanges in skill cultivation and use due to technology',\n",
       "  'the skills they have invested years in developing to reach \\ntheir full potential.\\nChanges in skill cultivation and use due to technology \\nreplacing either simple or complex tasks also raises deskill-\\ning concerns, whereby skilled human work is offloaded to \\nmachines resulting in skill loss (Vallor, 2015). Ethically, it \\nis critical to establish whether the human skills lost (i.e., \\noffloaded to machines) are important and whether they can \\nbe exercised and maintained through other forms of work or \\nin other life domains (Michaelson et\\xa0al., 2014; Wolf, 2010). \\nAs simple and rote work generally requires basic skills that \\ncan be cultivated elsewhere or are not significant, there is \\nlimited scope for significant deskilling in this case. How-\\never, complex tasks generally require complex skills, such \\nas judgement, intuition, context awareness, and ethical think-\\ning. From a deskilling perspective, these types of skills are \\nparticularly ethically problematic for workers to risk losing.',\n",
       "  'as judgement, intuition, context awareness, and ethical think-\\ning. From a deskilling perspective, these types of skills are \\nparticularly ethically problematic for workers to risk losing. \\nThis means when workers are left with fewer overall com-\\nplex and significant tasks following AI deployment, then \\ntheir ability to cultivate and use important skills will likely \\ndecrease, negatively impacting this dimension of meaning-\\nful work.\\nIt is worth noting that where replacement involves AI \\nassuming a worker’s whole job, for example where the job \\nis constituted entirely of simple and rote tasks that are most \\nsusceptible to full automation (Gibbs, 2017), this will likely \\nlead to unemployment (if redeployment is not possible). This \\neffectively removes, at least temporarily, paid meaningful \\nwork from that worker’s life and poses the greatest risk to \\nthe ability to experience meaningful work. This also pro-\\nvides the conditions for a wide range of skills to be lost or',\n",
       "  'work from that worker’s life and poses the greatest risk to \\nthe ability to experience meaningful work. This also pro-\\nvides the conditions for a wide range of skills to be lost or \\ndegraded, as well as having significant negative impacts on \\nimportant self-attitudes, such as feelings of self-respect and \\nself-worth\\xa0(see Selenko et\\xa0al., 2022 for work on AI use and \\nemployees’ sense of\\xa0identity). This case also raises broader \\npolitical questions about how society should deal with such \\na scenario should it become more widespread (Hughes, \\n2014). While these questions are beyond our focus here, \\nwe do highlight them in our discussion of future research \\ndirections.\\nSecond, we consider the path of workers ‘tending the \\nmachine’, whether in ‘managing’ or ‘minding’ forms. ‘Man-\\naging the machine’ work should enhance what Bourmault \\nand Anteby (2020, p. 1453) term “administrative responsi-\\nbility”, through offering a wider scope and variety of duties.',\n",
       "  'aging the machine’ work should enhance what Bourmault \\nand Anteby (2020, p. 1453) term “administrative responsi-\\nbility”, through offering a wider scope and variety of duties. \\nThis should enhance task integrity where the shift to coor-\\ndination and buffering work provides opportunities for inte-\\ngrated and challenging activities across training, explaining, \\nand sustaining roles through supervisory work, technology \\noversight, exceptions management, and cross-functional \\ncoordination of entire work processes. Such coordination \\nand buffering work will also require the development of \\nflexible and wide-ranging skill sets (Langlois, 2003), sup-\\nporting skill cultivation and use and more broadly widening \\nand deepening one’s ability to learn, achieve, and develop \\nat work.\\nIn contrast, rather than generating more complex and \\ninteresting human work, ‘minding the machine’ produces a \\n“more benignant role for humans” through more mundane',\n",
       "  'at work.\\nIn contrast, rather than generating more complex and \\ninteresting human work, ‘minding the machine’ produces a \\n“more benignant role for humans” through more mundane \\nand rote tasks (Langlois, 2003, p. 174). This would reduce \\ntask integrity as workers become more distanced from their \\nwork outcomes. The generally repetitive and fragmented \\nnature of ‘minding the machine’ work also suggests its asso-\\nciated skills are low and narrow, offering little opportunity \\nfor varied skill cultivation. Such AI “janitor work” (Jarrahi, \\n2019, p. 183) risks degrading workers’ abilities to meaning-\\nfully develop their capabilities and reach and express their',\n",
       "  '732\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nfull potential at work, leading to lower levels of meaningful-\\nness on this dimension.\\nThird, when AI amplifies workers’ abilities to do their \\ncurrent tasks, positive impacts on task integrity and skill \\ncultivation and use should ensue. For example, in the polic-\\ning domain, machine learning technologies\\xa0can collate previ-\\nously disparate data sources to analyse characteristics and \\nhistories of domestic violence victims and perpetrators to \\nbetter predict, compared to current human-driven systems, \\nrepeat attacks and better prioritise preventative actions \\n(Grogger et\\xa0al., 2020).6 In such cases, experiences of task \\nintegrity are likely to remain consistent or improve as AI \\nsupports workers to better complete their tasks and achieve \\nwork goals. Skill cultivation and use should remain neutral \\nor improve as it is likely that workers, while maintaining \\ntheir current skills, will need to develop new ones to inter-',\n",
       "  'work goals. Skill cultivation and use should remain neutral \\nor improve as it is likely that workers, while maintaining \\ntheir current skills, will need to develop new ones to inter-\\npret and integrate AI output into their decision making.\\nHowever, a feature of AI that may constrain skill use \\nacross all three paths is its ‘blackbox’ nature (Boden, 2016). \\nWhile AI designers are developing ways to improve lay per-\\nson interfaces, the use of ‘blackbox’ (or unexplainable) AI \\nin workplaces may degrade workers’ skill cultivation, use, \\nand feelings of competence. For example, where workers \\nare highly reliant on the decision making of an AI, they may \\nfeel lower levels of competence in their use of it due to little \\nunderstanding of its functioning. This effect will likely be \\nmore acutely felt where workers are expected to understand \\nand explain what the AI is doing. Poor explainability can \\nalso create opaque chains of accountability for decisions',\n",
       "  'more acutely felt where workers are expected to understand \\nand explain what the AI is doing. Poor explainability can \\nalso create opaque chains of accountability for decisions \\ninformed by AI (Dahl, 2018) and this risks making workers \\noverly dependent on an AI that they cannot comprehend.\\nTask Significance\\nTask significance means employees see their work as having \\npositive impacts (Grant, 2008) through their service to oth-\\ners (Lips-Wiersma & Morris, 2009), within or outside the \\norganisation (Hackman & Oldham, 1975). Task significance \\nis influenced by how employees assess their job impact on \\nand contact with beneficiaries (Grant, 2007). Job impacts \\non beneficiaries are shaped by the dimensions of magni-\\ntude, scope, frequency, and focus (i.e., preventing harms \\nor promoting benefits) (Grant, 2007). Contact with benefi-\\nciaries is shaped by the dimensions of frequency, duration, \\nphysical proximity (including virtual proximity), depth, and',\n",
       "  'or promoting benefits) (Grant, 2007). Contact with benefi-\\nciaries is shaped by the dimensions of frequency, duration, \\nphysical proximity (including virtual proximity), depth, and \\nbreadth of contact (Grant, 2007). Given the range of these \\ndimensions, workers’ assessments of task significance can \\nbe complex. Evidence suggests that\\xa0employees can derive \\ntask significance from even objectively rote, mundane, and \\nlow skill work (the\\xa0objective element\\xa0of meaningful work), \\nwhen that work is framed in the right way (the\\xa0subjective \\nelement\\xa0of meaningful work). For example, Carton (2018, \\np. 323) shows that when leaders at NASA carefully framed \\nthe space agency’s goals, workers could connect work such \\nas “mopping the floor” to “helping put a man on the moon”. \\nHowever, carrying out impactful tasks without opportuni-\\nties for “personal, emotional connections to the beneficiar-\\nies of those tasks” can impede overall experiences of task',\n",
       "  'However, carrying out impactful tasks without opportuni-\\nties for “personal, emotional connections to the beneficiar-\\nies of those tasks” can impede overall experiences of task \\nsignificance (Grant, 2007, p. 398; Bourmault & Anteby, \\n2020). This means that both job impact on beneficiaries and \\ncontact with them are important to assess. Given this, and \\nfollowing Grant (2007), we suggest that employees’ global \\nassessments of the impact of AI on their jobs, rather than on \\nspecific tasks, is most relevant when assessing perceptions \\nof task significance.\\nFirst, we consider the path of AI taking over simple or \\ncomplex tasks while workers remain engaged elsewhere. \\nGiven our focus here\\xa0at the job level, if only some sim-\\nple tasks are assumed by an AI this should have limited \\nimpact on task significance, assuming the remaining or \\nnew tasks provide opportunities for workers to positively \\nimpact and connect with beneficiaries. In contrast, when',\n",
       "  'impact on task significance, assuming the remaining or \\nnew tasks provide opportunities for workers to positively \\nimpact and connect with beneficiaries. In contrast, when \\nAI assumes more complex tasks, these are likely significant \\nto an individual’s overall assessments of task significance. \\nThis may lead to more extensive and complex sensemak-\\ning of this change. To see this, we draw on construal-level \\ntheory (CLT), which describes the way individuals cogni-\\ntively represent people or events at either higher or lower \\nlevels of abstraction (Trope & Liberman, 2003). Higher lev-\\nels of abstraction involve “mental representations that are \\nrelatively broad, inclusive, (and) general”, such as higher-\\nlevel goals or principles (Wiesenfeld et\\xa0al., 2017, p. 368). \\nLower levels of abstraction involve “applying relatively \\nspecific, detailed, and contextualised representations”, such \\nas focusing on lower-level actions to achieve higher-level',\n",
       "  'Lower levels of abstraction involve “applying relatively \\nspecific, detailed, and contextualised representations”, such \\nas focusing on lower-level actions to achieve higher-level \\ngoals (Wiesenfeld et\\xa0al., 2017, p. 368). Returning to the \\nearlier\\xa0AlphaFold example, at a higher level of construal \\nworkers may perceive improved task significance regard-\\ning job impact as the AI is significantly contributing to the \\nhigher-level goal of treating diseases. This could facilitate \\nhigher perceptions of magnitude, scope, and frequency of \\npositive impact. At a lower level of construal, the worker \\nmay then ask: “but what am I doing to help meet this goal?”. \\nIf workers can re-focus on other comparatively significant \\ntasks in the work process, they should experience higher \\ntask significance as the AI helps advance the field toward \\nreaching the overarching goal and the worker continues to \\nmeaningfully contribute toward that goal. However, where',\n",
       "  'task significance as the AI helps advance the field toward \\nreaching the overarching goal and the worker continues to \\nmeaningfully contribute toward that goal. However, where \\nthe remaining or new tasks fail, at lower levels of construal, \\nto deliver at least the same experiences of task significance \\n6\\u2002 Although in practice such predictive policing systems have been \\nshown to risk biased outcomes against minority groups, driven by \\nover-representation of those groups in policing statistics (Berk, 2021). \\nWe discuss these issues in a later section.',\n",
       "  '733\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nas before, then one’s perceived ability to ‘serve others’ is \\nlikely to degrade overall.\\nIn cases of both simple and complex task change, \\nwhere AI is used in ways that have sub-optimal, biased, \\nunjust, or harmful outcomes for end users, this could also \\ndecrease\\xa0workers’ perceptions of task significance. For \\nexample, where AI provides facial recognition and predic-\\ntive policing data for law enforcement agencies and the AI’s \\noutputs are biased against minority groups, then workers \\nmay see reduced task significance given their organisations’ \\nconnections to negative outcomes (via the negative magni-\\ntude, scope, and frequency dimensions of job impact). Impli-\\ncating workers in injustices and harms perpetrated by an \\nAI, through their involvement with or responsibility for the \\ntechnology, can particularly diminish the experience of serv-\\ning others and the autonomous ability to act in alignment',\n",
       "  'AI, through their involvement with or responsibility for the \\ntechnology, can particularly diminish the experience of serv-\\ning others and the autonomous ability to act in alignment \\nwith one’s values and morals (related to ‘developing and \\nbecoming self’), degrading overall work meaningfulness.\\nSecond, we consider the ‘tending the machine’ path. In \\nterms of ‘managing the machine’, the heightened adminis-\\ntrative responsibility associated with such work (Bourmault \\n& Anteby, 2020) should improve task significance through \\njob impact and more opportunities to benefit others, given \\nthe expansive duties this work entails (i.e., enhanced scope \\nof impact). However, such work can distance workers from \\nthose they serve, diminishing feelings of direct “personal \\nresponsibility” (Bourmault & Anteby, 2020, p. 1453) or feel-\\nings of having a direct and significant impact on the lives \\nof others, which can reduce task significance. For example,',\n",
       "  'responsibility” (Bourmault & Anteby, 2020, p. 1453) or feel-\\nings of having a direct and significant impact on the lives \\nof others, which can reduce task significance. For example, \\nwhen replaced by autonomously driven trains and moved to \\n‘managing the machine’ work, metro train drivers experi-\\nenced enhanced administrative responsibility but diminished \\npersonal responsibility, alongside lower task significance \\noverall, as they were no longer directly responsible for com-\\nmuters’ safety (Bourmault & Anteby, 2020).\\nIn terms of ‘minding the machine’ work we suggest that \\ntask significance will generally be reduced. This is because \\nsuch fragmented work means workers may have little idea of \\nthe point of their labour and its impacts, potentially limiting \\nall job impact dimensions. As they may also\\xa0be working in \\nisolation from others because of outsourcing\\xa0(Tubaro et\\xa0al., \\n2020), potentially limiting all contact with beneficiaries, this',\n",
       "  'all job impact dimensions. As they may also\\xa0be working in \\nisolation from others because of outsourcing\\xa0(Tubaro et\\xa0al., \\n2020), potentially limiting all contact with beneficiaries, this \\nfurther disconnects workers’ tasks from the end user benefits \\ngenerated, eroding task significance.\\nThird, when AI amplifies a worker’s abilities this should \\nhave significant and positive implications for task signifi-\\ncance, particularly through the magnitude and focus dimen-\\nsions of job impact and the duration and depth dimensions \\nof contact with beneficiaries. Here, the AI is not focused \\non substantially changing the range of tasks in a work pro-\\ncess, but rather on improving something that humans were \\nalready doing in that process, leading to better outcomes for \\nbeneficiaries. Drawing on earlier amplification examples, \\nwhere AI can support police officers by collating and ana-\\nlysing new data sources to help them better prevent inci-',\n",
       "  'beneficiaries. Drawing on earlier amplification examples, \\nwhere AI can support police officers by collating and ana-\\nlysing new data sources to help them better prevent inci-\\ndences of domestic violence (assuming it does not do so in \\nunfair or biased ways), then this should heighten percep-\\ntions of both being able to achieve higher-level goals (e.g., \\npreventing crime) and seeing the importance and connec-\\ntion of lower-level tasks to reaching that goal (e.g., through \\ninterpreting better predictive analytics). Use of AI in this \\nway can also help reduce human biases in decision making, \\nsuch as through building fairness principles into AI systems \\n(see Selbst et\\xa0al., 2019). In recruitment, for example, AI \\ncan limit the impact of unconscious human biases and vari-\\nous other human constraints on rational decision making by \\nassessing all candidates’ applications against standard cri-\\nteria and providing auditable, transparent, and explainable',\n",
       "  'ous other human constraints on rational decision making by \\nassessing all candidates’ applications against standard cri-\\nteria and providing auditable, transparent, and explainable \\ndecision trails (see Hagras, 2018; Bankins et\\xa0al., 2022). This \\npath demonstrates that a significant potential benefit of AI \\nis that it can elevate humans’ abilities to address complex \\nproblems, enhance the impact of their work, and thus better \\nserve others, through its analysis of large datasets to identify \\nnovel insights.\\nAutonomy\\nAutonomy means self-rule. Individually, that means being \\nable to do what you really want to do. In addition to the free-\\ndom from interference needed to rule yourself, autonomy is \\nalso commonly taken to include competency (i.e., you have \\nthe skills and capacities needed to rule yourself) and authen-\\nticity (i.e., your ends are authentically your own and not the \\nresult of oppression, manipulation, or coercion) conditions',\n",
       "  'the skills and capacities needed to rule yourself) and authen-\\nticity (i.e., your ends are authentically your own and not the \\nresult of oppression, manipulation, or coercion) conditions \\n(Formosa, 2021). In the workplace, autonomy refers to “the \\ndegree to which the job provides substantial freedom, inde-\\npendence, and discretion to the individual in scheduling the \\nwork and in determining the procedures to be used in carry-\\ning it out” (Hackman & Oldham, 1976, p. 258). AI’s impact \\non individuals’ autonomy is a key issue for the ethical AI \\nliterature. A particular concern is that ceding authority to AI \\ndiminishes human autonomy (Floridi et\\xa0al., 2018). However, \\npotential benefits for human autonomy can also accrue from \\nincreasing AI’s autonomy. We assess these different impacts \\nof AI at work as either promoting or diminishing autonomy \\nacross competency and authenticity conditions.\\nIn terms of promoting human autonomy, this depends on',\n",
       "  'of AI at work as either promoting or diminishing autonomy \\nacross competency and authenticity conditions.\\nIn terms of promoting human autonomy, this depends on \\nwhat work the AI assumes but also, and more importantly, \\non what work takes its place and what control and input \\nworkers have over AI deployment. When AI assumes simple \\nor complex tasks that workers find boring or repetitive, then \\nthis potentially promotes autonomy by freeing up time for \\nworkers to build their autonomy competencies through doing \\nother more challenging or authentic work. For example, if an \\nAI prioritises a worker’s emails so that she only sees those \\nrequiring a response, this may free her to work on other',\n",
       "  '734\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nmore valuable tasks. In terms of ‘managing the machine’, \\nthis path could promote autonomy if new work is more skil-\\nful and engaging than the work it replaces, and if workers \\nhave a degree of control over how that work is done. Where \\nAI amplifies workers by giving them more power and use-\\nful information, then this can improve worker autonomy by \\nhelping them to better achieve their self-given ends.\\nIn terms of diminishing human autonomy, these impacts \\nare partly the converse of the above. In our first path, if \\ncomplex, interesting, and creative tasks that workers want \\nto do are assumed by AIs, this potentially diminishes auton-\\nomy. There may also be good reasons why humans should \\nremain engaged in certain complex tasks and decisions, such \\nas due to their moral complexity. This means that where AI \\nassumes these tasks it can diminish human achievement of \\nvaluable ends, degrade important human skills, and limit',\n",
       "  'as due to their moral complexity. This means that where AI \\nassumes these tasks it can diminish human achievement of \\nvaluable ends, degrade important human skills, and limit \\nopportunities for moral development (Lips-Wiersma & Mor-\\nris, 2009). For example, when we delegate to AI decisions \\nregarding ethically sensitive aspects of human resource man-\\nagement, the skills associated with that work can degrade \\nand thereby diminish important autonomy competencies. AI \\ncan also make our autonomy more vulnerable by making us \\ndependent on it, which means our autonomy can diminish \\nif access to the technology is removed. Across the ‘tending \\nthe machine’ path, through ‘managing the machine’ work AI \\ncan diminish worker autonomy by filtering and potentially \\nrestricting the information that is made available for humans \\nto view and use (Kellogg et\\xa0al., 2020). Such constraints can \\nlimit the ability for workers to authentically develop them-',\n",
       "  'restricting the information that is made available for humans \\nto view and use (Kellogg et\\xa0al., 2020). Such constraints can \\nlimit the ability for workers to authentically develop them-\\nselves and their capabilities at work. Broader autonomy con-\\ncerns also exist with ‘minding the machine’ work, which \\nis itself mundane and boring, making workers feel like a \\n‘slave to the machine’ (Engel, 2019) and thereby experienc-\\ning\\xa0diminished autonomy at work.\\nAcross all paths, a more pernicious threat to autonomy \\nmay exist through surveillance and manipulation by AI. \\nThis reflects what Foucault calls the rise of a “surveillance \\nsociety”, which seeks to control bodies through making \\npeople feel permanently monitored (Abrams, 2004). When \\npeople are surveilled they tend to feel constrained and act \\nin less authentic and autonomous ways (Molitorisz, 2020). \\nThe use of AI to surveil workers will likely have similar \\nimpacts and can be a way for employers to use their power',\n",
       "  'in less authentic and autonomous ways (Molitorisz, 2020). \\nThe use of AI to surveil workers will likely have similar \\nimpacts and can be a way for employers to use their power \\nto exert control over employees. For example, the use of \\nAI-powered cameras to surveil Amazon delivery drivers \\ncould make them more self-conscious in their trucks, which \\ncould lead them to feel more constrained and unable to act \\nautonomously (Asher-Schapiro, 2021). A similar example \\nis when AI is implemented to monitor online meetings and \\nmeasure whether workers are engaged and contributing to \\nthe discussion (see Pardes, 2020), which could lead to stress \\nand inauthentic behaviour. Such monitoring could also result \\nin workers engaging in intentional “deviance” to challenge \\nthe control of surveillance (Abrams, 2004), by trying to \\n“game” the AI by matching or openly flouting what the \\nAI is expecting in terms of eye contact and body language \\n(Pardes, 2020), or finding other ways to operate outside the',\n",
       "  '“game” the AI by matching or openly flouting what the \\nAI is expecting in terms of eye contact and body language \\n(Pardes, 2020), or finding other ways to operate outside the \\ngaze of the surveillance system.\\nBelongingness\\nBelongingness refers to “the meaningfulness of work-\\ning together with other human beings” (Lips-Wiersma & \\nWright, 2012, p. 673). Across all our paths, we argue that \\nAI may impact workers’ belongingness in two main ways: \\nthrough generating the conditions for more or less meaning-\\nful connections and a sense of unity with others; and through \\nits implementation creating differences across workers that \\nundermines solidarity.\\nIn terms of the first way, where AI assumes tasks that \\nmay otherwise have required in-person and face-to-face \\ninteraction with other workers or customers, this can create \\nless human contact in the workplace. For example, where \\nan AI chatbot allows workers to access information previ-',\n",
       "  'interaction with other workers or customers, this can create \\nless human contact in the workplace. For example, where \\nan AI chatbot allows workers to access information previ-\\nously provided by a human worker, this lessens that worker’s \\ninteractions with other humans and reduces opportunities \\nfor forming connections with others that are the bedrock for \\ngenerating a sense of belonging (Seppala et\\xa0al., 2013). In \\ncontrast, AI use may increase opportunities for human inter-\\naction, for example through ‘managing the machine’ work \\nwhere workers are responsible for supervising AI deploy-\\nment that requires extensive human-to-human training.\\nIn terms of the second way, a key concern in the ethical \\nAI literature is how AI use may disproportionately and nega-\\ntively affect lower-skilled and lower-paid workers, while its \\nbenefits may disproportionately accrue to those with higher \\nskills and wages (Ernst et\\xa0al., 2018), effectively creating new',\n",
       "  'tively affect lower-skilled and lower-paid workers, while its \\nbenefits may disproportionately accrue to those with higher \\nskills and wages (Ernst et\\xa0al., 2018), effectively creating new \\ntypes of workplace in-groups and out-groups. For example, \\nmany of the negative impacts of AI at work, such as surveil-\\nlance and simplistic ‘minding the machine’ work, will tend \\nto fall on less skilled ‘blue collar’ workers, whereas more of \\nthe amplifying and autonomy-enhancing benefits associated \\nwith taking on even more interesting and engaging work \\nwill tend to fall to already privileged workers. This creates \\njustice concerns around how the benefits and burdens of AI \\nin workplaces are being distributed, potentially undermining \\nsolidarity between those who benefit from AI’s introduction \\nand those who do not. For example, in a call centre context \\nan AI may be used to monitor and evaluate the calls of every \\ncall centre operator. Such heightened surveillance may be',\n",
       "  'and those who do not. For example, in a call centre context \\nan AI may be used to monitor and evaluate the calls of every \\ncall centre operator. Such heightened surveillance may be \\nperceived by operators as intrusive and diminishing their \\nautonomy. However, using AI in this way may amplify the \\nwork of quality assurance staff in the same organisation, \\nproviding them with more information and assisting them \\nin better training and managing operators. This shows how \\nAI may generate distinct groups experiencing very different',\n",
       "  '735\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nimpacts, such as being viewed as unnecessary surveillance \\nby some but as an amplifying source of information by oth-\\ners. Such outcomes particularly threaten the ability to create \\na sense of belongingness and shared values (Lips-Wiersma \\n& Morris, 2009), which underpins the ‘unity with others’ \\ndimension of meaningful work.\\nEthical Implications: AI and\\xa0Meaningful \\nWork\\nWe have analysed how the three paths of AI deployment \\nmay enhance or diminish opportunities for meaningful work \\nacross five dimensions. We now surface the ethical implica-\\ntions of this analysis via the five principles of the AI4Peo-\\nple ethical AI framework (Floridi et\\xa0al, 2018): beneficence; \\nnon-maleficence; autonomy; justice; and explicability. As \\nwith any principlist framework there are potential conflicts \\nand tensions between principles (Formosa et\\xa0al., 2021). For \\nexample, there may be benefits for some from AI deploy-',\n",
       "  'with any principlist framework there are potential conflicts \\nand tensions between principles (Formosa et\\xa0al., 2021). For \\nexample, there may be benefits for some from AI deploy-\\nment (beneficence) while others suffer harm (non-malefi-\\ncence) or interference with their autonomy. As identified \\nearlier, the\\xa0provision of meaningful work is not always the \\nonly or most important ethical value at stake, and so less \\nmeaningful work may not be ethically worse overall if there \\nare other ethical benefits, such as improved wellbeing for \\nothers through higher productivity.\\nTo assess ethical implications, we synthesise and summa-\\nrise how the three paths (replacing, ‘tending the machine’, \\nand amplifying) support or limit experiences of meaningful \\nwork and so contribute to, or diminish, meeting the AI4Peo-\\nple principles. We summarise these impacts in Table\\xa01 across \\nthe five ethical principles (beneficence and non-maleficence \\nare combined in the Table as the latter reflects the converse',\n",
       "  'ple principles. We summarise these impacts in Table\\xa01 across \\nthe five ethical principles (beneficence and non-maleficence \\nare combined in the Table as the latter reflects the converse \\nof the former), while noting the main deployment pathways \\nthrough which these impacts occur.\\nIn terms of the beneficence principle, there can be sig-\\nnificant benefits for employees when AI use supports the \\nvarious dimensions of meaningful work. When AI amplifies \\na worker’s skills it can support them to complete their tasks, \\nundertake more complex tasks, and utilise higher-order \\nthinking and analysis skills (task integrity and skill cultiva-\\ntion and use). It can also afford workers the opportunity to \\nachieve better outcomes and enhance the positive impact of \\ntheir work on beneficiaries (task significance), give them \\nmore control over their work through improved access to \\ninformation (autonomy), and potentially generate new con-\\nnections with other workers and stakeholders (belonging-',\n",
       "  'more control over their work through improved access to \\ninformation (autonomy), and potentially generate new con-\\nnections with other workers and stakeholders (belonging-\\nness). Similarly, when AI assumes some simple or complex \\ntasks and the human worker can re-focus on other impor-\\ntant and challenging tasks in the work process, then posi-\\ntive experiences across all dimensions of meaningful work \\nshould be maintained or improved. ‘Managing the machine’ \\nwork can also improve meaningfulness through a wider \\nscope of enriched work (task integrity and skill cultivation \\nand use) and a wider positive job impact within and outside \\nthe organisation (task significance), as well as greater inter-\\naction with a range of stakeholders through coordination and \\nsupervisory work (belongingness).\\nIn terms of the non-maleficence principle, we also show \\nthe harms that AI can create when it is deployed in ways that \\nlead to less (or no) meaningful work, or other related harms.',\n",
       "  'In terms of the non-maleficence principle, we also show \\nthe harms that AI can create when it is deployed in ways that \\nlead to less (or no) meaningful work, or other related harms. \\nTwo paths generate greatest risk of harms through signifi-\\ncantly reducing experiences of meaningful work. First, when \\nAI replaces some tasks, the risk of degraded task integ-\\nrity, deskilling, reduced task significance, and constrained \\nautonomy is greatest when it assumes more complex tasks \\nand the worker is not afforded any new comparable or more \\ninteresting work. This is because complex tasks generally \\nconstitute a large and significant part of the work process \\nand undertaking them exercises a range of important skills. \\nBeing removed from such work can also distance workers \\nfrom the output of their labour and lower perceptions of \\nbeneficiary impact. In the worst case, it could involve the \\ncomplete loss of paid meaningful work where AI replaces',\n",
       "  'from the output of their labour and lower perceptions of \\nbeneficiary impact. In the worst case, it could involve the \\ncomplete loss of paid meaningful work where AI replaces \\nwhole jobs, which removes workers from important social \\nrelationships and denies them the opportunity to skilfully \\nutilise their talents to help others. Second, ‘minding the \\nmachine’ work, as we have characterised its fragmented, \\npiecemeal, and micro-work nature, threatens these same \\naspects of meaningful work and feelings of belongingness \\nwhen work is outsourced to disconnected workers. Other \\npaths can also generate harms, but arguably at lower lev-\\nels. For example, we identified that while ‘managing the \\nmachine’ work may increase meaningful work experiences \\noverall through heightened administrative responsibility, it \\ncan lessen feelings of task significance by increasing dis-\\ntance between workers and their beneficiaries and reducing \\nfeelings of personal responsibility.',\n",
       "  'can lessen feelings of task significance by increasing dis-\\ntance between workers and their beneficiaries and reducing \\nfeelings of personal responsibility.\\nIn terms of the autonomy principle, across each path we \\nshow how autonomy is supported when AI is used to free \\nup humans to focus their time on other more valued tasks, \\nallows them to develop new or enhanced autonomy com-\\npetencies, and gives them more control over their work. In \\nparticular, the task replacement, ‘managing the machine’, \\nand amplifying paths that afford employees access to bet-\\nter data and information, the opportunity to engage in more \\ninteresting work, and exercise more control over how their \\nwork is done, can all promote autonomy as a dimension of \\nmeaningful work. However, many of these positive impacts \\nalso depend on whether workers have input into how AI \\nis deployed in their organisations. A particular risk to \\nautonomy is the use of AI to surveil and monitor, which can',\n",
       "  'also depend on whether workers have input into how AI \\nis deployed in their organisations. A particular risk to \\nautonomy is the use of AI to surveil and monitor, which can \\nundermine authenticity and encourage workers to align their \\nbehaviours with the AI’s implicit expectations or seek ways \\nto subvert or avoid its control.',\n",
       "  '736\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nThe justice principle centres on ensuring fair, just, and \\nnon-discriminatory outcomes from AI and requires a focus \\non how the benefits and burdens of AI use are distributed. \\nFor example, the amplifying path generally achieves strongly \\npositive outcomes for meaningful work, but there is evidence \\nthat such benefits are disproportionately allocated to already \\nprivileged workforces (i.e., higher-skilled and higher-paid \\nworkers). In contrast, the ‘minding the machine’ path gen-\\nerally achieves strongly negative outcomes for meaningful \\nwork, but such burdens tend to disproportionately impact \\nless privileged workforces (i.e., lower-paid and lower-skilled \\nworkers). Lower-skilled workers are also more likely to \\nhave their entire jobs replaced by AI (Gibbs, 2017). This \\nuneven distribution raises important justice concerns and \\ncan undermine solidarity and feelings of belongingness \\nwithin and across work groups. However, AI can also be',\n",
       "  'uneven distribution raises important justice concerns and \\ncan undermine solidarity and feelings of belongingness \\nwithin and across work groups. However, AI can also be \\ndeployed to promote justice, which can positively impact \\ntask significance. For example, when AI is used to mini-\\nmise bias and maximise evidence-based decision making \\nthrough giving workers access to new data-driven insights \\n(such as through amplification, ‘managing the machine’, or \\nreplacing complex tasks paths), this promotes fair outcomes \\nwhile also enhancing task significance through a greater \\npositive impact on beneficiaries. But the converse also holds \\nwhen the justice principle is threatened by an AI trained on \\nbiased datasets and deployed in workplaces where it gener-\\nates unjust outcomes that can decrease task significance and \\nimplicate workers in injustices.\\nFinally, the explicability principle relates to the explain-\\nability, transparency, and accountability of AI. In paths',\n",
       "  'implicate workers in injustices.\\nFinally, the explicability principle relates to the explain-\\nability, transparency, and accountability of AI. In paths \\nwhere AI plays a significant role alongside human workers, \\nsuch as the amplifying, ‘managing the machine’, and the \\nreplacement of complex tasks paths, an inability of work-\\ners to understand an AI’s operation, particularly where they \\nTable\\u202f1\\u2002 \\u2009Ethical impacts of AI for enhancing or diminishing meaningful work\\nEthical aspects of meaningful work (AI4 \\nPeople principles)\\nHow AI could enhance meaningful work (and \\nmain pathways)\\nHow AI could diminish meaningful work (and \\nmain pathways)\\nBeneficence & Non-maleficence\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Less boring and repetitive human work\\n• Increased opportunities for new and/or more \\nchallenging human work\\n• Enhanced human learning, skills, and \\ndevelopment\\n• Augmenting and assisting human workers\\n• Facilitating higher positive impacts on \\nothers',\n",
       "  'challenging human work\\n• Enhanced human learning, skills, and \\ndevelopment\\n• Augmenting and assisting human workers\\n• Facilitating higher positive impacts on \\nothers\\nMain pathways: Replacing & minding the \\nmachine\\n• Loss of work for humans\\n• Reduced human role in the work process\\n• Human deskilling\\n• More boring and fragmented ‘minding the \\nmachine’ work\\n• Less belonging and less human interaction\\nAutonomy\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Improved autonomy competencies (e.g., \\nthrough more time for valuable work)\\n• More control and power for workers through \\ngreater access to information\\nMain pathways: All pathways, especially mind-\\ning the machine\\n• Reduced autonomy competencies (e.g., \\nthrough deskilling)\\n• Surveillance\\n• Manipulation & nudging (e.g., controlling \\nhuman behaviour)\\n• Employers exerting more power and control \\nover workers\\n• Worker vulnerability & dependence on AI\\n• Inauthentic behaviours (e.g., acting more self-\\nconsciously)',\n",
       "  'human behaviour)\\n• Employers exerting more power and control \\nover workers\\n• Worker vulnerability & dependence on AI\\n• Inauthentic behaviours (e.g., acting more self-\\nconsciously)\\n• Resistance to AI and greater deviance (e.g., \\n‘gaming’ the AI)\\nJustice\\nMain pathways: Replacing & amplifying\\n• More data-driven choices\\n• Less human bias in decision making\\n• Fairer decision making\\nMain pathways: All pathways\\n• Unfair distribution of benefits and burdens \\n(e.g., lower-skilled workers suffer more bur-\\ndens and receive less benefits)\\n• Undermining solidarity\\n• Implicating workers in injustices resulting \\nfrom AI bias\\nExplicability (explainability & accountability) Main pathways: Managing the machine & \\namplifying\\n• Informating (i.e., improved access to infor-\\nmation)\\n• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability',\n",
       "  '• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making',\n",
       "  '737\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nare highly reliant upon it and are\\xa0accountable for it, can \\nconstrain skill use and feelings of competence. This could \\npotentially undermine the benefits AI may otherwise bring. \\nThis suggests that training workers not only in what AI does \\nbut also how it does it, and making chains of accountability \\nclear, will be important for supporting experiences of mean-\\ningfulness at work.\\nPractical Implications\\nOrganisational use of AI can reap many benefits through \\nimproved service range and quality, efficiency, and profit-\\nability. However, the ethical deployment of AI requires \\nweighing up its many costs and benefits. We help articulate \\nsome of those costs and benefits for workers in terms of \\nAI’s impacts on meaningful work. Practically, this is impor-\\ntant because some authors suggest an emerging trend is for \\norganisations to use AI for full automation (Acemoglu &',\n",
       "  'AI’s impacts on meaningful work. Practically, this is impor-\\ntant because some authors suggest an emerging trend is for \\norganisations to use AI for full automation (Acemoglu & \\nRestrepo, 2020), without also considering opportunities to \\nuse it for enhancing human work, and then poorly preparing \\ntheir workforces for the changes that\\xa0AI use entails (Hal-\\nloran & Andrews, 2018). For organisations we highlight \\nthose pathways, such as ‘minding the machine’ work, that \\nare likely to significantly limit opportunities for meaningful \\nwork, which implies that other considerations such as effi-\\nciency benefits must strongly outweigh the harms to workers \\nthat AI used in this way can generate, in order to justify its \\nuse. We also highlight that, when considering meaningful-\\nness, it is insufficient to focus only on the AI itself, as the \\nimplications of its deployment are strongly driven by what \\nwork remains for humans, which is something that organi-',\n",
       "  'ness, it is insufficient to focus only on the AI itself, as the \\nimplications of its deployment are strongly driven by what \\nwork remains for humans, which is something that organi-\\nsations can directly influence and decide. Overall, we offer \\nguidance on how organisations can maintain or build oppor-\\ntunities for meaningful work when they implement AI and \\npoint leaders toward specific areas for intervention to sup-\\nport meaningful work experiences. For example, task sig-\\nnificance is critical for meaningful work (Grant, 2007), yet \\nthe ways AI can distance workers from beneficiaries threat-\\nens these experiences. However, there are ways in which \\norganisations can remedy this, such as by sharing end users’ \\npositive stories with workers (Grant, 2008).\\nFuture Research Directions\\nAlthough we did not frame explicit propositions from our \\nconceptual work, there are several relationships we sug-\\ngest warrant empirical examination. For example, assess-',\n",
       "  'Future Research Directions\\nAlthough we did not frame explicit propositions from our \\nconceptual work, there are several relationships we sug-\\ngest warrant empirical examination. For example, assess-\\ning\\xa0whether AI performing simple tasks enhances task \\nintegrity, but AI performing complex tasks degrades this \\ndimension and perceptions of task significance, and\\xa0exam-\\nining whether the ‘managing the machine’ and amplifying \\npaths enhance task integrity and skill cultivation and use \\noverall, but ‘minding the machine’ work diminishes these \\naspects. We also suggest several contingencies will affect \\nthese relationships, such as what other or new work employ-\\nees do following AI implementation (task-related factors) \\nand how aspects of the technology, such as its explainabil-\\nity or potential for bias, shape workers’ experiences of it \\n(technology-related factors).\\nWhile we adapted Langlois’ (2003) work to develop \\nour three pathways, these may manifest in different ways',\n",
       "  'ity or potential for bias, shape workers’ experiences of it \\n(technology-related factors).\\nWhile we adapted Langlois’ (2003) work to develop \\nour three pathways, these may manifest in different ways \\nand will likely overlap. Future research could explore how \\neach path operates in workplaces, how they may differ from \\nour conceptualisation, and whether there are other path \\nconfigurations to AI deployment that our framework does \\nnot capture. There may also be nuances within pathways \\nthat warrant investigation. For example, Jarrahi (2018, p. \\n3) suggests that advances in AI could create new forms of \\n“human–machine symbiosis” that result in “both parties \\n(becoming) smarter over time”. This could generate new \\nforms of human skills, tasks, and perhaps whole jobs that \\nhave not yet been imagined, with implications for meaning-\\nful work.\\nAnother area for future work is examining how lead-\\ners construct and influence subjective perceptions of the',\n",
       "  'have not yet been imagined, with implications for meaning-\\nful work.\\nAnother area for future work is examining how lead-\\ners construct and influence subjective perceptions of the \\nmeaningfulness of work, particularly through the values, \\nstrategies, and vision that underpin how they implement AI \\n(Pratt & Ashforth, 2003). For example, if an organisation is \\nfocused on full automation and replacing human workers, \\nit will likely deploy AI toward this end and degrade oppor-\\ntunities for meaningful work. But if leaders adopt multi-\\nstakeholder governance approaches that support ethical AI \\ndeployment (Wright & Schultz, 2018), such participatory \\npractices may enhance perceptions of meaningful work fol-\\nlowing AI deployment.\\nFinally, while we centred our analysis on the meaning-\\nful work implications of narrow AI, future work could uti-\\nlise conceptual tools such as thought experiments (Bankins \\n& Formosa, 2020) and work on posthumanism (Gladden,',\n",
       "  'ful work implications of narrow AI, future work could uti-\\nlise conceptual tools such as thought experiments (Bankins \\n& Formosa, 2020) and work on posthumanism (Gladden, \\n2016) to prospectively analyse the impacts of potential \\nfuture forms and deployments of more advanced AI. For \\nexample, developments in virtual and augmented reality are \\ncreating movements toward a metaverse, or a persistent form \\nof virtual world that is accessible through various devices \\nand that people combine with their existence in the physi-\\ncal world (Ravenscraft, 2021). Such technologies have the \\npotential to transform the nature of social interactions and \\nthus impact the belongingness dimension of meaningful \\nwork. Likewise, advances in natural language processing \\nand speech interfaces could result in workers having multi-\\nple “digital assistants” (Zhou et\\xa0al., 2021, p. 258), which will',\n",
       "  '738\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nimpact the nature of workers’ tasks and relationships and the \\nskills they will require in the future.\\nExtending even further, artificial general intelligence \\n(AGI) would constitute “a new general-purpose technol-\\nogy” (Naudé & Dimitri, 2020) that has been predicted to \\npose existential threats such as eradicating large swathes of \\nhuman work (Bruun & Duka, 2018) and even risking human-\\nity’s annihilation (Torres, 2019). The reality of such tech-\\nnologies would inevitably lead to more extreme conclusions \\nfor the future of meaningful work than we have generated \\nhere through our focus on narrow AI, as they would likely \\nrender all but our replacing path largely obsolete. The pos-\\nsibilities of such technologies may therefore lead us back to \\nsubstantive discussions on the value of work generally, and \\nwhat forms of human work we believe must be preserved or \\nnewly created no matter what technologies are developed.',\n",
       "  'substantive discussions on the value of work generally, and \\nwhat forms of human work we believe must be preserved or \\nnewly created no matter what technologies are developed. \\nThis also raises questions of what broader social changes, \\nsuch as increased volunteering, provision of other forms of \\nmeaningful activity, or a Universal Basic Income (Hughes, \\n2014), will be required to cushion negative impacts should \\nAGI deployment ever become a reality. It also augurs the \\npotential for heavier regulation of the development and use \\nof AI (and potentially AGI) to maintain meaningful forms of \\nhuman employment, and to place limits on where, how, and \\nwhy AI is used. However, at this point the discussion relies \\non largely technical questions about whether AGI is indeed \\npossible (Boden, 2016). In the meantime, the impacts of \\nnarrow AI on meaningful work are ones we need to address \\nhere and now.\\nConclusion\\nThis paper focused on a neglected aspect of the ethical',\n",
       "  'possible (Boden, 2016). In the meantime, the impacts of \\nnarrow AI on meaningful work are ones we need to address \\nhere and now.\\nConclusion\\nThis paper focused on a neglected aspect of the ethical \\nimplications of AI deployment, namely the impacts of AI \\non meaningful work. This is an important contribution as \\nthe ethical AI literature, while focused on the impacts of \\nunemployment resulting from AI, needs to also\\xa0attend\\xa0to \\nthe impacts of AI on meaningful work for the remaining \\nworkforce. Given the ethical importance of meaningful \\nwork and its considerable impacts on human wellbeing, \\nautonomy, and flourishing, this is a significant omission \\nthat we help to remedy. We have done so by examining the \\nimpacts of three paths of AI deployment (replacing tasks, \\n‘tending the machine’, and amplifying) across five dimen-\\nsions of meaningful work (task integrity, skill cultivation \\nand use, task significance, autonomy, and belongingness).',\n",
       "  '‘tending the machine’, and amplifying) across five dimen-\\nsions of meaningful work (task integrity, skill cultivation \\nand use, task significance, autonomy, and belongingness). \\nUsing this approach, we identify specific ways in which AI \\ncan both promote and diminish experiences of meaningful \\nwork across these dimensions and draw out the ethical impli-\\ncations of this by utilising five key ethical AI principles. \\nFinally, we offer practical guidance for organisations by \\narticulating the ways that AI can be implemented to support \\nmeaningful work and suggest opportunities for future \\nresearch. Overall, we show that AI has the potential to make \\nwork more meaningful for some workers by undertaking less \\nmeaningful tasks for them and amplifying their capabilities, \\nbut that it can also make work less meaningful for others by \\ncreating new boring tasks, restricting worker autonomy, and \\nunfairly distributing the benefits of AI away from less-skilled',\n",
       "  'but that it can also make work less meaningful for others by \\ncreating new boring tasks, restricting worker autonomy, and \\nunfairly distributing the benefits of AI away from less-skilled \\nworkers. This suggests that AI’s future impacts on meaning-\\nful work will be both significant and mixed.\\nAcknowledgements\\u2002 The authors would like to sincerely thank the \\nSpecial Issue Guest Editors, their Action Editor Associate Professor \\nLuke Fletcher, and the anonymous reviewers for their insightful and \\nconstructive feedback during the review process.\\nFunding\\u2002 Open Access funding enabled and organized by CAUL and \\nits Member Institutions.\\nDeclarations\\u2002\\nConflict of interest\\u2002 The authors have no conflicts of interest to declare \\nthat are relevant to the content of this article.\\nOpen Access\\u2002 This article is licensed under a Creative Commons Attri-\\nbution 4.0 International License, which permits use, sharing, adapta-\\ntion, distribution and reproduction in any medium or format, as long',\n",
       "  \"bution 4.0 International License, which permits use, sharing, adapta-\\ntion, distribution and reproduction in any medium or format, as long \\nas you give appropriate credit to the original author(s) and the source, \\nprovide a link to the Creative Commons licence, and indicate if changes \\nwere made. The images or other third party material in this article are \\nincluded in the article's Creative Commons licence, unless indicated \\notherwise in a credit line to the material. If material is not included in \\nthe article's Creative Commons licence and your intended use is not \\npermitted by statutory regulation or exceeds the permitted use, you will \\nneed to obtain permission directly from the copyright holder. To view a \\ncopy of this licence, visit http://\\u200bcreat\\u200biveco\\u200bmmons.\\u200borg/\\u200blicen\\u200bses/\\u200bby/4.\\u200b0/.\\nReferences\\nAbrams, J. J. (2004). Pragmatism, artificial intelligence, and posthuman \\nbioethics: Shusterman, Rorty, Foucault. Human Studies, 27(3), \\n241–258.\",\n",
       "  'References\\nAbrams, J. J. (2004). Pragmatism, artificial intelligence, and posthuman \\nbioethics: Shusterman, Rorty, Foucault. Human Studies, 27(3), \\n241–258.\\nAcemoglu, D., & Restrepo, P. (2020). The wrong kind of AI? Artificial \\nintelligence and the future of labour demand.\\xa0Cambridge Journal \\nof Regions, Economy and Society, 13, 25–35.\\nAllan, B. A., Batz-Barbarich, C., Sterling, H. M., & Tay, L. (2019). \\nOutcomes of meaningful work: A meta-analysis. Journal of Man-\\nagement Studies, 56(3), 500–528.\\nAsher-Schapiro, A. (2021). Amazon AI van cameras spark surveil-\\nlance concerns. News.Trust.Org. https://\\u200bnews.\\u200btrust.\\u200borg/\\u200bitem/\\u200b\\n20210\\u200b20513\\u200b2207-\\u200bc0mz7/\\nBailey, C., Yeoman, R., Madden, A., Thompson, M., & Kerridge, G. \\n(2019). A review of the empirical literature on meaningful work: \\nProgress and research agenda. Human Resource Development \\nReview, 18(1), 83–113.\\nBankins, S. (2021). The ethical use of artificial intelligence in human',\n",
       "  'Progress and research agenda. Human Resource Development \\nReview, 18(1), 83–113.\\nBankins, S. (2021). The ethical use of artificial intelligence in human \\nresource management: A decision-making framework.\\xa0Ethics and \\nInformation Technology, 23, 841–854.\\nBankins, S., & Formosa, P. (2020). When AI meets PC: Exploring \\nthe implications of workplace social robots and a human-robot',\n",
       "  \"739\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\npsychological contract. European Journal of Work and Organi-\\nzational Psychology, 29(2), 215–229.\\nBankins, S., & Formosa, P. (2021). Ethical AI at work: The social \\ncontract for artificial intelligence and its implications for the work-\\nplace psychological contract. In: M. Coetzee & A. Deas (Eds.), \\nRedefining the Psychological Contract in the Digital Era: Issues \\nfor Research and Practice (pp. 55–72). Springer: Switzerland.\\nBankins, S., Formosa, P., Griep, Y., & Richards, D. (2022). AI decision \\nmaking with dignity? Contrasting workers' justice perceptions of \\nhuman and AI decision making in a human resource management \\ncontext.\\xa0Information Systems Frontiers, 24(3), 857–875.\\nBekey, G. A. (2012). Current trends in robotics. In P. Lin, K. Abney, & \\nG. A. Bekey (Eds.), Robot ethics (pp. 17–34). MIT Press:\\xa0Cam-\\nbridge, Mass.\\nBerk, R. A. (2021). Artificial intelligence, predictive policing, and risk\",\n",
       "  'G. A. Bekey (Eds.), Robot ethics (pp. 17–34). MIT Press:\\xa0Cam-\\nbridge, Mass.\\nBerk, R. A. (2021). Artificial intelligence, predictive policing, and risk \\nassessment for law enforcement. Annual Review of Criminology, \\n4(1), 209–237.\\nBoden, M. A. (2016). AI. Oxford University Press: UK.\\nBourmault, N., & Anteby, M. (2020). Unpacking the managerial blues: \\nHow expectations formed in the past carry into new jobs. Organi-\\nzation Science, 31(6), 1452–1474.\\nBowie, N. E. (1998). A Kantian theory of meaningful work. Journal \\nof Business Ethics, 17, 1083–1092.\\nBruun, E., & Duka, A. (2018). Artificial intelligence, jobs and the \\nfuture of work. Basic Income Studies, 13(2), 1–15.\\nCamus, A. (1955). The myth of Sisyphus and other essays. Hamish \\nHamilton.\\nCarton, A. M. (2018). I’m not mopping the floors, I’m putting a man \\non the moon: How NASA leaders enhanced the meaningfulness \\nof work by changing the meaning of work. Administrative Science \\nQuarterly, 63(2), 323–369.',\n",
       "  'on the moon: How NASA leaders enhanced the meaningfulness \\nof work by changing the meaning of work. Administrative Science \\nQuarterly, 63(2), 323–369.\\nCheney, G., Zorn Jr, T. E., Planalp, S., & Lair, D. J. (2008). Meaningful \\nwork and personal/social well-being organizational communica-\\ntion engages the meanings of work. Annals of the International \\nCommunication Association, 32(1), 137–185.\\nChui, M., Manyika, J., & Miremadi, M. (2015). The four fundamentals \\nof workplace automation. McKinsey. http://\\u200bwww.\\u200bmckin\\u200bsey.\\u200bcom/\\u200b\\nbusin\\u200bess-\\u200bfunct\\u200bions/\\u200bdigit\\u200bal-\\u200bmckin\\u200bsey/\\u200bour-\\u200binsig\\u200bhts/\\u200bfour-\\u200bfunda\\u200bmenta\\u200b\\nls-\\u200bof-\\u200bworkp\\u200blace-\\u200bautom\\u200bation\\nDahl, E. S. (2018). Appraising black-boxed technology: The positive \\nprospects. Philosophy & Technology, 31, 571–591.\\nDastin, J. (2018, October 11). Amazon scraps secret AI recruiting tool \\nthat showed bias against women. Reuters. https://\\u200bwww.\\u200breute\\u200brs.\\u200b\\ncom/\\u200bartic\\u200ble/\\u200bus-\\u200bamazon-\\u200bcom-\\u200bjobs-\\u200bautom\\u200bation-\\u200binsig\\u200bht-\\u200bidUSK\\u200b\\nCN1MK\\u200b08G',\n",
       "  'that showed bias against women. Reuters. https://\\u200bwww.\\u200breute\\u200brs.\\u200b\\ncom/\\u200bartic\\u200ble/\\u200bus-\\u200bamazon-\\u200bcom-\\u200bjobs-\\u200bautom\\u200bation-\\u200binsig\\u200bht-\\u200bidUSK\\u200b\\nCN1MK\\u200b08G\\nDaugherty, P. R., & Wilson, H. J. (2018). Human + Machine: Reim-\\nagining work in the age of AI.\\xa0Harvard Business Review Press.\\nEngel, S. (2019). Minding machines: A note on alienation. Fast Capi-\\ntalism, 16(2), 129–139.\\nErnst, E., Merola, R., & Samaan, D. (2018). The economics of artificial \\nintelligence. International Labour Organization. https://\\u200bwww.\\u200bilo.\\u200b\\norg/\\u200bwcmsp5/\\u200bgroups/\\u200bpublic/\\u200bdgrep\\u200borts/\\u200bcabin\\u200bet/\\u200bdocum\\u200bents/\\u200bpubli\\u200b\\ncation/\\u200bwcms_\\u200b647306.\\u200bpdf\\nFloridi, L., et\\xa0al. (2018). AI4People - An ethical framework for a good \\nAI society. Minds and Machines, 28(4), 689–707.\\nFormosa, P. (2017). Kantian ethics, dignity and perfection. Cambridge \\nUniversity Press: Cambridge.\\nFormosa, P. (2021). Robot autonomy vs human autonomy: Social \\nrobots, artificial intelligence (AI), and the nature of autonomy. \\nMinds and Machines, 31, 595–616.',\n",
       "  'University Press: Cambridge.\\nFormosa, P. (2021). Robot autonomy vs human autonomy: Social \\nrobots, artificial intelligence (AI), and the nature of autonomy. \\nMinds and Machines, 31, 595–616.\\nFormosa, P., & Ryan, M. (2021). Making moral machines: Why we \\nneed artificial moral agents. AI & Society, 36, 839–851.\\nFormosa, P., Wilson, M., & Richards, D. (2021). A principlist frame-\\nwork for cybersecurity ethics. Computers & Security, 109, \\n102382.\\nFrey, C. B., & Osborne, M. A. (2017). The future of employment: How \\nsusceptible are jobs to computerisation?\\xa0Technological Forecast-\\ning and Social Change, 114, 254–280.\\nGibbs, M. J. (2017). How is new technology changing job design? IZA \\nWorld of Labor. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b15185/\\u200bizawol.\\u200b344\\nGladden, M. E. (2016). Posthuman management: Creating effective \\norganizations in an age of social robotics, ubiquitous AI, human \\naugmentation, and virtual worlds. Defragmenter Media: USA.',\n",
       "  'Gladden, M. E. (2016). Posthuman management: Creating effective \\norganizations in an age of social robotics, ubiquitous AI, human \\naugmentation, and virtual worlds. Defragmenter Media: USA.\\nGrant, A. M. (2007). Relational job design and the motivation to make \\na prosocial difference. Academy of Management Review, 32(2), \\n393–417.\\nGrant, A. M. (2008). The significance of task significance: Job perfor-\\nmance effects, relational mechanisms, and boundary conditions. \\nJournal of Applied Psychology, 93(1), 108–124.\\nGrogger, J., Ivandic, R., & Kirchmaier, T. (2020). Comparing con-\\nventional and machine-learning approaches to risk assessment in \\ndomestic abuse cases. Journal of Empirical Legal Studies, 18(1), \\n90–130.\\nHackman, J. R., & Oldham, G. R. (1975). Development of the job diag-\\nnostic survey. Journal of Applied Psychology, 60(2), 159–170.\\nHackman, J. R., & Oldham, G. R. (1976). Motivation through the \\ndesign of work: Test of a theory. Organizational Behavior and',\n",
       "  'nostic survey. Journal of Applied Psychology, 60(2), 159–170.\\nHackman, J. R., & Oldham, G. R. (1976). Motivation through the \\ndesign of work: Test of a theory. Organizational Behavior and \\nHuman Performance, 16(2), 250–279.\\nHagendorff, T. (2020). The ethics of AI ethics: An evaluation of guide-\\nlines. Minds and Machines, 30, 99–120.\\nHagras, H. (2018). Toward human-understandable, explainable AI. \\nComputer, 51(9), 28–36.\\nHalloran, L. & Andrews, J. (2018). Will you wait for the future to \\nhappen? Ernst and Young. https://\\u200bwww.\\u200bey.\\u200bcom/\\u200ben_\\u200bau/\\u200bworkf\\u200borce/\\u200b\\nwill-\\u200byou-\\u200bshape-\\u200bthe-\\u200bfuture-\\u200bof-\\u200bwork-\\u200bor-\\u200bwill-\\u200bit-\\u200bshape-\\u200byou\\nHassabis, D., & Revell, T. (2021). With AI, you might unlock some of \\nthe secrets about how life works. New Scientist, 249(3315), 44–49.\\nHughes, J. (2014). A strategic opening for a basic income guarantee in \\nthe global crisis being created by AI, robots, desktop manufactur-\\ning and biomedicine. Journal of Ethics and Emerging Technolo-\\ngies, 24(1), 45–61.',\n",
       "  \"the global crisis being created by AI, robots, desktop manufactur-\\ning and biomedicine. Journal of Ethics and Emerging Technolo-\\ngies, 24(1), 45–61.\\nJarrahi, M. H. (2018). Artificial intelligence and the future of work: \\nHuman-AI symbiosis in organizational decision making. Business \\nHorizons, 61(4), 577–586.\\nJarrahi, M. H. (2019). In the age of the smart artificial intelligence: AI's \\ndual capacities for automating and informating work. Business \\nInformation Review, 36(4), 178–187.\\nJobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI \\nethics guidelines. Nature Machine Intelligence, 1(9), 389–399.\\nKellogg, K. C., Valentine, M. A., & Christin, A. (2020). Algorithms at \\nwork: The new contested terrain of control. Academy of Manage-\\nment Annals, 14(1), 366–410.\\nLanglois, R. N. (2003). Cognitive comparative advantage and the \\norganization of work: Lessons from Herbert Simon's vision of \\nthe future. Journal of Economic Psychology, 24(2), 167–187.\",\n",
       "  \"Langlois, R. N. (2003). Cognitive comparative advantage and the \\norganization of work: Lessons from Herbert Simon's vision of \\nthe future. Journal of Economic Psychology, 24(2), 167–187.\\nLeicht-Deobald, U., et\\xa0al. (2019). The challenges of algorithm-based \\nHR decision-making for personal integrity. Journal of Business \\nEthics, 160, 377–392.\\nLips-Wiersma, M., & Morris, L. (2009). Discriminating between \\n‘meaningful work’ and the ‘management of meaning.’ Journal of \\nBusiness Ethics, 88(3), 491–511.\\nLips-Wiersma, M., & Wright, S. (2012). Measuring the meaning of \\nmeaningful work: Development and validation of the comprehen-\\nsive meaningful work scale. Group & Organization Management, \\n37(5), 655–685.\\nLysova, E. I., Allan, B. A., Dik, B. J., Duffy, R. D., & Steger, M. \\nF. (2019). Fostering meaningful work in organizations: A multi-\\nlevel review and integration. Journal of Vocational Behavior, 110, \\n374–389.\",\n",
       "  '740\\n\\t\\nS.\\xa0Bankins, P.\\xa0Formosa \\n1 3\\nMartela, F., & Riekki, T. J. J. (2018). Autonomy, competence, relat-\\nedness, and beneficence: A multicultural comparison of the four \\npathways to meaningful work. Frontiers in Psychology, 9, 1157.\\nMazmanian, M., Orlikowski, W. J., & Yates, J. (2013). The autonomy \\nparadox: The implications of mobile email devices for knowledge \\nprofessionals. Organization Science, 24(5), 1337–1357.\\nMichaelson, C., Pratt, M. G., Grant, A. M., & Dunn, C. P. (2014). \\nMeaningful work: Connecting business ethics and organization \\nstudies. Journal of Business Ethics, 121, 77–90.\\nMolitorisz, S. (2020). Net privacy: How we can be free in an age of \\nsurveillance. McGill-Queen’s University Press: Canada.\\nNaudé, W., & Dimitri, N. (2020). The race for an artificial general \\nintelligence: Implications for public policy. AI & Society, 35, \\n367–379.\\nNussbaum, M. C. (2011). Creating capabilities: The human develop-\\nment approach. Harvard University Press: USA.',\n",
       "  'intelligence: Implications for public policy. AI & Society, 35, \\n367–379.\\nNussbaum, M. C. (2011). Creating capabilities: The human develop-\\nment approach. Harvard University Press: USA.\\nPardes, A. (2020, November). AI can run your work meetings now. \\nWired. https://\\u200bwww.\\u200bwired.\\u200bcom/\\u200bstory/\\u200bai-\\u200bcan-\\u200brun-\\u200bwork-\\u200bmeeti\\u200bngs-\\u200b\\nnow-\\u200bheadr\\u200boom-\\u200bclock\\u200bwise/\\nParker, S. K., & Grote, G. (2022). Automation, algorithms, and \\nbeyond: Why work design matters more than ever in a digital \\nworld. Applied Psychology, 71(4), 1171–1204.\\nPratt, M. G., & Ashforth, B. E. (2003). Fostering meaningfulness in \\nworking and at work. In K. Cameron, J. E. Dutton, & R. E. Quinn \\n(Eds.), Positive organizational scholarship: Foundations of a new \\ndiscipline (pp. 308–327). Berrett-Koehler: San Francisco.\\nPulse+IT. (2020). The San using AI to automate multidisciplinary \\nteam meetings. Pulse+IT. https://\\u200bwww.\\u200bpulse\\u200bitmag\\u200bazine.\\u200bcom.\\u200bau:\\u200b\\n443/\\u200baustr\\u200balian-\\u200beheal\\u200bth/\\u200b5558-\\u200bthe-\\u200bsan-\\u200busing-\\u200bai-\\u200bto-\\u200bautom\\u200bate-\\u200bmulti\\u200b',\n",
       "  'team meetings. Pulse+IT. https://\\u200bwww.\\u200bpulse\\u200bitmag\\u200bazine.\\u200bcom.\\u200bau:\\u200b\\n443/\\u200baustr\\u200balian-\\u200beheal\\u200bth/\\u200b5558-\\u200bthe-\\u200bsan-\\u200busing-\\u200bai-\\u200bto-\\u200bautom\\u200bate-\\u200bmulti\\u200b\\ndisci\\u200bplina\\u200bry-\\u200bteam-\\u200bmeeti\\u200bngs\\nRavenscraft, E. (25 November, 2021). What is the metaverse, exactly? \\nWired. Retrieved from: https://\\u200bwww.\\u200bwired.\\u200bcom/\\u200bstory/\\u200bwhat-\\u200bis-\\u200bthe-\\u200b\\nmetav\\u200berse/\\nRoberts, P. (2020). Working smarter with data. Australian Manufactur-\\ning Forum. https://\\u200bwww.\\u200bauman\\u200bufact\\u200buring.\\u200bcom.\\u200bau/\\u200bworki\\u200bng-\\u200bsmart\\u200b\\ner-\\u200bwith-\\u200bdata-\\u200bai-\\u200bgives-\\u200bagric\\u200bulture-\\u200bthe-\\u200bcompe\\u200btitive-\\u200bedge\\nRyan, M., & Stahl, B. C. (2020). Artificial intelligence ethics guide-\\nlines for developers and users: Clarifying their content and nor-\\nmative implications. Journal of Information, Communication and \\nEthics in Society, 19(1), 61–86.\\nSelbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & \\nVertesi, J. (2019). Fairness and abstraction in sociotechnical sys-\\ntems. In Proceedings of the Conference on Fairness, Account-',\n",
       "  'Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & \\nVertesi, J. (2019). Fairness and abstraction in sociotechnical sys-\\ntems. In Proceedings of the Conference on Fairness, Account-\\nability, and Transparency (pp. 59–68).\\nSelenko, E., Bankins, S., Shoss, M., Warburton, J., & Restubog, S. \\nL. D. (2022). Artificial intelligence and the future of work: A \\nfunctional-identity perspective. Current Directions in Psychologi-\\ncal Science, 31(3), 272–279.\\nSeppala, E., Rossomando, T., & Doty, J. R. (2013). Social connection \\nand compassion: Important predictors of health and well-being. \\nSocial Research, 80(2), 411–430.\\nSmids, J., Nyholm, S., & Berkers, H. (2020). Robots in the workplace: \\nA threat to - or opportunity for - meaningful work?\\xa0Philosophy & \\nTechnology, 33, 503–522.\\nSusser, D., Roessler, B., & Nissenbaum, H. (2019). Technology, auton-\\nomy, and manipulation. Internet Policy Review. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n14763/\\u200b2019.2.\\u200b1410',\n",
       "  'Technology, 33, 503–522.\\nSusser, D., Roessler, B., & Nissenbaum, H. (2019). Technology, auton-\\nomy, and manipulation. Internet Policy Review. https://\\u200bdoi.\\u200borg/\\u200b10.\\u200b\\n14763/\\u200b2019.2.\\u200b1410\\nSymon, G., & Whiting, R. (2019). The sociomaterial negotiation of \\nsocial entrepreneurs’ meaningful work. Journal of Management \\nStudies, 56(3), 655–684.\\nThaler, R., & Sunstein, C. (2008). Nudge:\\xa0 Improving decisions about \\nhealth, wealth, and happiness. Yale University Press:\\xa0New Haven, \\nCT.\\nTorres, P. (2019). The possibility and risks of artificial general intel-\\nligence. Bulletin of the Atomic Scientists, 75(3), 105–108.\\nTrope, Y., & Liberman, N. (2003). Temporal construal. Psychological \\nReview, 110(3), 403–421.\\nTubaro, P., Casilli, A. A., & Coville, M. (2020). The trainer, the veri-\\nfier, the imitator: Three ways in which human platform workers \\nsupport artificial intelligence. Big Data & Society, 7(1). https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b1177/\\u200b20539\\u200b51720\\u200b919776',\n",
       "  'fier, the imitator: Three ways in which human platform workers \\nsupport artificial intelligence. Big Data & Society, 7(1). https://\\u200b\\ndoi.\\u200borg/\\u200b10.\\u200b1177/\\u200b20539\\u200b51720\\u200b919776\\nVallor, S. (2015). Moral deskilling and upskilling in a new machine \\nage: Reflections on the ambiguous future of character. Philosophy \\n& Technology, 28(1), 107–124.\\nWalsh, T., Levy, N., Bell, G., Elliott, A., Maclaurin, J., Mareels, I., & \\nWood, Fiona. (2019). The effective and ethical development of \\nartificial intelligence. ACOLA. https://\\u200bacola.\\u200borg/\\u200bwp-\\u200bconte\\u200bnt/\\u200buploa\\u200b\\nds/\\u200b2019/\\u200b07/\\u200bhs4_\\u200bartif\\u200bicial-\\u200bintel\\u200bligen\\u200bce-\\u200breport.\\u200bpdf\\nWang, P. (2019). On defining artificial intelligence. Journal of Artificial \\nGeneral Intelligence, 10(2), 1–37.\\nWebster, C., & Ivanov, S. (2020). Robotics, artificial intelligence, and \\nthe evolving nature of work. In: B. George & J. Paul (Eds.), Digi-\\ntal Transformation in Business and Society. Palgrave Macmillan, \\nCham.',\n",
       "  'the evolving nature of work. In: B. George & J. Paul (Eds.), Digi-\\ntal Transformation in Business and Society. Palgrave Macmillan, \\nCham.\\nWiesenfeld, B. M., Reyt, J.-N., Brockner, J., & Trope, Y. (2017). Con-\\nstrual level theory in organizational research. Annual Review of \\nOrganizational Psychology and Organizational Behavior, 4(1), \\n367–400.\\nWolf, S. (2010). Meaning in life and why it matters. Princeton Univer-\\nsity Press: New Jersey.\\nWorld Economic Forum. (2018). The future of jobs report. Centre for \\nthe New Economy and Society: Geneva, Switzerland.\\nWright, D. (2011). A framework for the ethical impact assessment of \\ninformation technology. Ethics and Information Technology, 13, \\n199–226.\\nWright, S. A., & Schultz, A. E. (2018). The rising tide of artificial intel-\\nligence and business automation: Developing an ethical frame-\\nwork. Business Horizons, 61(6), 823–832.\\nZhou, L., Paul, S., Demirkan, H., Yuan, L., Spohrer, J., Zhou, M.,',\n",
       "  \"ligence and business automation: Developing an ethical frame-\\nwork. Business Horizons, 61(6), 823–832.\\nZhou, L., Paul, S., Demirkan, H., Yuan, L., Spohrer, J., Zhou, M., \\n& Basu, J. (2021). Intelligence augmentation: Towards build-\\ning human-machine symbiotic relationship. AIS Transactions on \\nHuman-Computer Interaction, 13(2), 243–264.\\nZuboff, S. (1988). In the age of the smart machine: The future of work \\nand power. Basic Books: New York.\\nPublisher's Note\\u2002 Springer Nature remains neutral with regard to \\njurisdictional claims in published maps and institutional affiliations.\"],\n",
       " 'uris': None,\n",
       " 'included': ['embeddings', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': None}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive_vector.get(include=['embeddings','documents'])\n",
    "# You can acess the metadata also  as recursive_vector.get(include=['embeddings','documents','metadatas'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4820c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['c521c5ef-900f-4a1c-bbec-bba1aa844bb3'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['at work.\\nIn contrast, rather than generating more complex and \\ninteresting human work, ‘minding the machine’ produces a \\n“more benignant role for humans” through more mundane \\nand rote tasks (Langlois, 2003, p. 174). This would reduce \\ntask integrity as workers become more distanced from their \\nwork outcomes. The generally repetitive and fragmented \\nnature of ‘minding the machine’ work also suggests its asso-\\nciated skills are low and narrow, offering little opportunity \\nfor varied skill cultivation. Such AI “janitor work” (Jarrahi, \\n2019, p. 183) risks degrading workers’ abilities to meaning-\\nfully develop their capabilities and reach and express their'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'Books\\\\Ethical Paper.pdf',\n",
       "   'format': 'PDF 1.4',\n",
       "   'moddate': '2023-07-03T17:42:49+05:30',\n",
       "   'creator': 'Springer',\n",
       "   'modDate': \"D:20230703174249+05'30'\",\n",
       "   'trapped': '',\n",
       "   'creationdate': '2023-02-24T14:25:09+05:30',\n",
       "   'producer': 'Acrobat Distiller 10.1.8 (Windows)',\n",
       "   'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work',\n",
       "   'total_pages': 16,\n",
       "   'file_path': 'Books\\\\Ethical Paper.pdf',\n",
       "   'creationDate': \"D:20230224142509+05'30'\",\n",
       "   'author': 'Sarah Bankins',\n",
       "   'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work',\n",
       "   'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7',\n",
       "   'page': 6}]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = recursive_vector._client.get_collection(\"recursive_chunks_sample\")\n",
    "doc_data = collection.get(ids=['c521c5ef-900f-4a1c-bbec-bba1aa844bb3'])\n",
    "\n",
    "doc_data\n",
    "# Checking single Document does it stored correctly in the vector or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "965bc49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['89053abe-c0cc-4a71-909a-3e4c6d0e64cd',\n",
       "  '8f40dfe8-404d-4a9b-943a-7cbd63a656f0',\n",
       "  '462e82b1-da8a-496e-95fa-4a055ea30684',\n",
       "  '4c90290a-b7ca-4845-91f2-4538784587ed',\n",
       "  'b04d5dd7-d1cf-4543-a110-178e408ab430',\n",
       "  'fb3e86af-cb8c-441c-9336-e87381c43ddd',\n",
       "  '2034030b-04b2-4f64-a775-5d96903131ba',\n",
       "  '32feb875-bec2-44a7-add3-ff6585ddc9e5',\n",
       "  'ded5e040-8f18-4689-bbad-d25374a6b151',\n",
       "  '618a9cb1-b4d7-42fc-8507-e5098a6b559d',\n",
       "  'a478700d-3991-489a-be0e-a51ceab03aec',\n",
       "  '67991f68-e519-45b0-aa42-bd935dc715a6',\n",
       "  'c8d39016-8fa9-4f1f-b3da-f52dafb6b024',\n",
       "  '9ecd5482-da24-429e-ad18-fe2deb3f2035',\n",
       "  '81e46cec-44db-4e6d-8edf-b742b477451a',\n",
       "  '4eb661dd-d942-4fe9-9d1f-15ceb27f10e0',\n",
       "  'eae3642e-9bb6-4242-9b4c-5b3d653f3b74',\n",
       "  'd3d01c5c-e9b4-4f7c-a40a-a772f19c5e45',\n",
       "  '8e2c7ebd-1da3-46d7-a2f8-323804bf1d55',\n",
       "  '718c5cba-605a-404f-b7ef-79b39e57248e',\n",
       "  '401eb9c4-4dca-4832-973d-bf0df4807379',\n",
       "  '28d291b6-1b78-425d-baf9-d3baa6bb19e4',\n",
       "  '8f73fd9c-42dd-4b1e-9604-8d4cf6b400a8',\n",
       "  'a018dd1c-9a75-473b-9df9-b80b78065d50',\n",
       "  '6bbf724e-453f-4b8e-8cd9-5744b78e9ca1',\n",
       "  'bfd801e7-60ff-4b49-8e0e-0c8131f425ad',\n",
       "  'd887384b-3c24-4f52-a078-bfd1d528ee0a',\n",
       "  '12558183-31e7-435d-b9cb-3fabb8d64e19',\n",
       "  '1401e664-78a9-42f9-9a40-ae1dacd73a50',\n",
       "  'cf5a8195-9ff5-404e-bc68-0ecdda949933',\n",
       "  '2f897b02-14e1-455e-bef6-3a8373a96f9e',\n",
       "  'd73dfd75-4dde-4d7f-8ce3-365513abb8ec',\n",
       "  'e802da64-5f7c-435b-b914-46a5f856370d',\n",
       "  '199c4e0d-0327-43cc-857b-4e8394325448',\n",
       "  '51493e7e-0cfb-4ebe-985d-1b2d15991622',\n",
       "  '5a58c919-99a9-4c88-9ef9-e312d3014bb4',\n",
       "  '78203c98-1c64-46f5-b1b2-24a817e9d480',\n",
       "  'd76ec5b8-ad6c-4e1f-9389-1b0b37029e5a',\n",
       "  '32bdd7b0-825a-4ddb-ae62-356c65c2de11',\n",
       "  'e08ca1ef-9723-4f78-8c8c-1438932ae9aa',\n",
       "  'bd920175-8e52-4f82-ab4f-47bdbefffaa9',\n",
       "  '20774eb6-4ac8-498b-abb2-a0bb361aaf30',\n",
       "  '39bf1382-7d24-4823-8ab2-612850b98a29',\n",
       "  '6c2e61fb-0114-4d5b-b643-96a63c1b8563',\n",
       "  '2ef6d3a4-11ac-4621-b7d3-dccb2c052556',\n",
       "  '62ffce15-bbd4-4203-bfbc-7b7e5e354c2b',\n",
       "  '8869fb47-350c-49d4-bd0a-947e06b8b1f3',\n",
       "  '90b1dff2-3da7-48d0-ab40-5044d02f7611',\n",
       "  '425ac639-653e-4468-8706-376324d2bee0',\n",
       "  '2d90e500-46cc-4433-b59a-fff6f1fed13b',\n",
       "  '1e61d346-dac1-42be-8eee-a7f42b83c454',\n",
       "  'c360bec9-6f8e-41fc-ab13-9b9be543b6f6',\n",
       "  '3c9102bc-e878-4e2a-baa0-7d643137ea51',\n",
       "  'd32675be-8be8-48bc-8726-55b90aadb560',\n",
       "  'ae34f2fd-3c2b-4ced-a59e-d747a344107d',\n",
       "  '88776159-2470-4709-8bdd-4013632f716c',\n",
       "  '49f60568-b508-48ab-a139-1b2d4ed37054',\n",
       "  'f1e66633-966a-4cb3-ad5a-8c482368d49e',\n",
       "  'e0f200c9-ec8c-4eaa-bf81-1c53dc6b1cda',\n",
       "  '403781f5-aa64-4bad-80ad-5eef10fc738e',\n",
       "  '960b8096-6d7f-4029-bf8e-0537896ded1f',\n",
       "  'cffa339f-0ebe-481a-8339-84c6f7c8b75f',\n",
       "  'b8d7572a-cf6f-49a3-86eb-131b11745536',\n",
       "  '3e8ffa46-c5c0-401d-a65d-c94040045c0e',\n",
       "  '3a771dfa-4bb0-4a2d-bc7e-d5987ce29b27',\n",
       "  '79f1dffd-1359-4456-84a1-f574fab07e47',\n",
       "  '5c790f2c-47df-4bcc-a213-f9d610f8069d',\n",
       "  '43fcc231-2fbe-472b-9142-adddc7e36555',\n",
       "  'fb3ad690-c67a-4d14-8aa8-11fe4762e906',\n",
       "  '24d1147a-1a6c-4258-91be-156c3bf24c31',\n",
       "  'c7d58c2c-2e6e-430b-9e81-2262c6cb8a25',\n",
       "  '18331e99-2570-4f29-9bfe-c02715743502',\n",
       "  'bc45e5e1-ab4e-415b-ba31-56f001e7432c',\n",
       "  'a984a2fa-04ca-4660-b55e-ef005b8a41ef',\n",
       "  '93f303cc-4e9e-4e5f-b431-bcc70056e6a2',\n",
       "  '32396ce3-15cb-4829-a621-e2516b309412',\n",
       "  '2374987b-b646-4ee7-bdae-dbb820ec761a',\n",
       "  'b5641b17-e276-42b3-a088-d581c49bf665',\n",
       "  'a98aed80-b339-4fe9-bd8c-cddd21e3f374',\n",
       "  '68878689-c639-4d80-b626-34085c79fc97',\n",
       "  'baff2ac9-b385-4826-9541-fb4c5db4de10',\n",
       "  '98eeeeb8-7c3c-4f66-87e9-dce931f4f22c',\n",
       "  '09dbea00-c87a-46f5-bc53-2b621c2548cb',\n",
       "  '52dec50b-2f7c-4f9c-a28e-32b2873fb7e3',\n",
       "  '35b4544c-c840-4c86-90bc-61c7a74b9e76',\n",
       "  '7750e330-be9b-4905-95e6-7b67900d842b',\n",
       "  '7d7d04c0-e5d5-410a-a3b8-1ff8d8001dcf',\n",
       "  'dbe90820-25fb-49a8-b029-443b766531d3',\n",
       "  '162aeeac-86b8-44e9-865d-1f5346563606',\n",
       "  '8c31c92c-3e27-41e8-b098-d066c1c78fa4',\n",
       "  '03ad35d6-fb28-48fd-825f-95ce19bf6ebc',\n",
       "  'be20da8b-0cbd-4af0-bf7a-d0d2c7063c89',\n",
       "  'c549f65a-414d-4515-b6f0-f44eb5166942',\n",
       "  'a60a4bcb-3c96-4ac0-9c35-ad480d3e67db',\n",
       "  'f5679086-89e2-4a0a-97ab-f5a724e807a7',\n",
       "  '02b93be5-cc48-48a2-8d08-c9fce3a1b914',\n",
       "  '36aea6ba-c590-4742-8906-bbdc9ee43731',\n",
       "  '5f5e4a4a-a70f-4705-a3a5-1f890d2dee8a',\n",
       "  'b1b9cb42-b0bb-4cc4-83e4-63c7bfe91dc6',\n",
       "  '11f98b4b-14f0-48de-a83c-39ec758fa92e',\n",
       "  'ac27c535-67ab-4bc7-88fd-9f34e717c452',\n",
       "  '0b9145c8-e5d2-4c03-87c6-90407fd3a88b',\n",
       "  'd1066933-3b5f-4031-9b37-73ed700a95ba',\n",
       "  'accb1220-09ec-4bb0-837a-3b3982b98fd3',\n",
       "  '5ceb4641-b475-445f-8043-6695c5a1f2a6',\n",
       "  'c61c3554-4f02-4f72-ab45-417bb54e3b15',\n",
       "  '37dd1cfd-460a-4840-ac88-ff573dfd9b64',\n",
       "  'f79b9c37-0637-4903-8459-bc0160c573d7',\n",
       "  '54b4fc52-9d34-4a45-9fe9-49bedf417836',\n",
       "  '37a2f871-b7a9-4af7-a947-9ced90446620',\n",
       "  '4cdc2e23-a0fb-488e-af77-8f60584b9599',\n",
       "  '6d7fc3d6-989e-48d1-9825-a9a5a81282d6',\n",
       "  'ef39ead3-414b-4d83-aa23-512a7db95710',\n",
       "  'db687935-f92a-4132-b1d6-b6203edda3ab',\n",
       "  '60296250-0456-488f-9121-840125d6ab10',\n",
       "  'cc6edacc-1bd5-43a1-beef-84b1542a0262',\n",
       "  '8ff21e5f-4b32-41c9-8fa3-771678fd7693',\n",
       "  '5794e758-e2a7-4b4c-8080-99daf72b9d6f',\n",
       "  '4ec07271-47bc-409a-9822-5902d2a78b82',\n",
       "  'f270957f-b47f-4833-86ed-e8cef0125608',\n",
       "  'cdc1e8b2-e9f6-401b-aca4-a8c33e76f1a5',\n",
       "  '32652972-1071-4c99-87e7-4f11629d0778',\n",
       "  '73a5b64f-b891-480f-88c7-2e6688b5a5d1',\n",
       "  '76333bd2-9535-4743-920a-36c05f8a2d06',\n",
       "  'c8af2da5-7c84-4ed1-89e2-1f633ebe9911',\n",
       "  'c5aad5b4-8e55-4348-ba79-794888a0933d',\n",
       "  'a45449bb-17fb-4955-ad3e-8c05f1606169',\n",
       "  '4c51eb72-78c6-42da-a799-9165da345b2e',\n",
       "  '040bdf6c-7baf-4b1b-b131-fc037c574449',\n",
       "  'ed26fa29-08d7-457c-8500-f14335baffdc',\n",
       "  '79f1e21e-20df-42f3-bfbf-90a9a1c7f3d8',\n",
       "  'f61fdf78-4cc2-48af-9d8a-25fff5f503ad',\n",
       "  'c0456c37-06f7-4af4-b7d2-91591c7ffb67',\n",
       "  '9577f9c4-e939-448f-9151-d723cb2747e9',\n",
       "  '2fbe4dfe-83e6-4e77-8386-e6fd9d80a221',\n",
       "  'd67e6d8c-15dc-4174-8954-8d6ca5e26192',\n",
       "  '36945e40-4dbe-4c2a-9b79-acc590cb6196',\n",
       "  '77bed6c3-6251-4bcf-bbe9-bffa1001fc46',\n",
       "  '78c12d96-9037-4c68-b1f2-11c6f0beb15a',\n",
       "  'bbd30223-5caa-45e8-af8b-73ded734f746',\n",
       "  '6d06380a-951d-4cb5-92e2-8b757203015f',\n",
       "  '5e317030-c54d-484f-a61b-d9386f85d2b2',\n",
       "  'fc66db94-2042-408a-b9f0-172172c436eb',\n",
       "  '40033ea3-2b0a-44fb-a568-b1031b60d520',\n",
       "  'eb389c68-bfab-4d9e-9cd2-280f3a52a848',\n",
       "  'da69d282-9ca4-4f79-b7dc-6c4d4d205b56',\n",
       "  '681d9549-01a2-4f18-81e8-8ac71f467bfe',\n",
       "  '470ed0cc-e8c3-415f-b92e-a6c0eb7046de',\n",
       "  'da2460ec-5e35-4d15-a5a8-8a20d1961152',\n",
       "  '749952be-8fb1-4fa3-a015-99c25b03f500',\n",
       "  '719118e5-0556-45fe-8075-a1171901a37b',\n",
       "  'd95cd0a0-55e2-4517-9839-6fef58018663',\n",
       "  '0c6b77e2-76f9-49b7-8f1f-c159ed6a4e7f',\n",
       "  '6c603825-58e1-43f9-9fc8-1f4debd721d7',\n",
       "  '38f2cbf7-ee5a-47a4-84b8-d0255146705a',\n",
       "  'eeacc5bc-4ccd-4d75-8715-d9366ddbe834',\n",
       "  '52773740-63b7-4879-8475-bbc43fafef81',\n",
       "  '8eda8f28-0739-4634-933d-b26a1a1eae93',\n",
       "  '5da64f2d-f7fc-404b-a465-58ea2a3482aa',\n",
       "  '864735cc-b32d-4323-a7d5-eda1ee05b63a',\n",
       "  '4152e31e-2d74-448f-b802-ec155276774f',\n",
       "  'a2d803f6-2e0f-468e-9e7b-1d7257307f03',\n",
       "  '596964d9-8eb9-49cf-b97b-a9dc44e3490c',\n",
       "  '98f3749e-9b36-404d-b96f-f503e99ff0f0',\n",
       "  '19711b10-35de-4810-a63c-c4b6e59a721d',\n",
       "  'ff43b943-f4db-4835-8775-c64f79d94e7d',\n",
       "  '8afa45e3-906b-423a-8fab-ab223d8dcd7e',\n",
       "  '91d17650-176e-4256-95e4-6974134837e7',\n",
       "  '16374933-c902-4a37-9673-b8dca23a68ea',\n",
       "  '0c5c2fd1-3f56-4312-ba9d-51c8b146eec9',\n",
       "  'ea505e1c-2305-4e6c-8c66-ec709a1ae3b0',\n",
       "  'fa432911-b57e-47b4-bd9a-f48cc7c7b903',\n",
       "  '948177ae-e8d7-4e86-83fd-fa4ed08a6204',\n",
       "  '4d55186d-6653-49b7-bb7b-754b80c77ca3',\n",
       "  'c85203f2-375b-423d-8cd6-34350baa6335',\n",
       "  '0a434165-86f0-46ad-8333-a691f5f19dd7',\n",
       "  'e9002fc9-66ac-4805-8fbd-f7bbf9d55464',\n",
       "  'a3ef2866-85ae-47bd-869f-1d06a792eeb6',\n",
       "  '5c1e115e-da8d-43e2-9f27-f3311daf2f1d',\n",
       "  '8ab429e4-2337-4256-880d-b4bed72bfaf6',\n",
       "  '5c3f701f-2466-453c-a12f-0991804fbea3',\n",
       "  '08f7d2ab-dc9b-48ca-a6ac-3957faeed79f',\n",
       "  '54413fc8-2ad6-4318-a161-464ea2ea9e2c',\n",
       "  '0de58fc7-f06c-424a-858a-e5f4dcc9768a',\n",
       "  '9ee84bed-8545-49eb-999e-584d5c3ec785',\n",
       "  'dbf76602-0572-41fe-9f62-a6abd7775ac4',\n",
       "  '952be6d5-ab19-45c4-9c2e-6acafeeb62c0',\n",
       "  '1b0f9222-feba-42a2-bdbf-59aecb170dc1',\n",
       "  '1a225835-312b-471b-aab8-ff517da8305c',\n",
       "  'a41001fe-a8c5-4a69-82c4-43e9029bf43b',\n",
       "  'f5c6d882-783a-45a8-90a8-f0c59bd2c88a',\n",
       "  '990873c6-961d-48e2-9bd1-288ab2bc8a29',\n",
       "  'c133dc7b-7dc8-4399-80aa-95c8608fef96',\n",
       "  '9dd64694-188e-4688-88ca-4c88c323c2f3',\n",
       "  '45f46dcb-a32f-4866-aa62-7569c1497747',\n",
       "  'b8272790-c0cb-476a-b106-c2628906c3da',\n",
       "  '0e42d819-59b5-4ad1-a6ab-b46ef74164fa',\n",
       "  '5c7334ab-6f96-4280-8ee7-ccd7784d9b6d',\n",
       "  'd8c57d3f-0fbd-42f0-80f2-4ad58d3d7307',\n",
       "  '55cdf8fc-e78f-42ea-bf9e-7854b66d40ab',\n",
       "  '2b371a30-7739-4249-aead-5bdff3cbca68',\n",
       "  '84a59de7-4a47-496b-8be1-389a66d589cd',\n",
       "  'a00afc88-7e8a-4da4-8621-b2d7ef70feae',\n",
       "  '6825e10b-f77c-45ee-b49b-902ccdd3c903',\n",
       "  '559f7df6-65ec-404f-af8c-323dec759e89',\n",
       "  '58e6c206-2edf-4afc-b622-a762e5ecd29b',\n",
       "  'ec62c611-5dcd-46e2-b375-dcf1b4842a16',\n",
       "  '4305be06-c4a2-483a-860b-ee80c8cc0772',\n",
       "  '0b93e5c3-d652-4a59-a3db-d4f90ae408fb',\n",
       "  'cfafd9d4-19bc-4fb2-a3a4-2266c6e9ba9f',\n",
       "  '937559f6-f65c-4f45-acbf-0bf28aaffeb2',\n",
       "  'f2da00bf-7d97-489b-ad5c-d57cab23b993',\n",
       "  'e9b84114-eae0-4258-8636-84b5ce93cdfd',\n",
       "  'e3ac0fc1-46aa-406d-84b1-7425d39c9a9a',\n",
       "  '4ab5e197-d550-42bd-a62a-4196d781774c',\n",
       "  'fe00f238-633f-4219-a78a-421cee484367',\n",
       "  'c1a7648a-a4ff-4a80-bc11-c50e4c11feaa',\n",
       "  '247fda03-8ca6-4528-b112-b60f89a66058',\n",
       "  '7ff1d986-bbf4-4579-a8b4-3df79a164b5b',\n",
       "  '660997fd-875f-45a1-8903-5364949e1fcb',\n",
       "  '308ca6e4-3528-4844-828e-9281f71eae12',\n",
       "  '3a3e7e25-484e-47bb-9fa3-fa43a47e7f5d',\n",
       "  'aced897d-fdc3-4aa8-b8c2-ead5f26de56e',\n",
       "  'd72f196e-392f-4ed8-878d-2f70cdd87ecc',\n",
       "  '3f103570-0606-496c-a444-4a3e4b08ce13',\n",
       "  'd996a139-1270-461a-96c7-a2388bd533c2',\n",
       "  'a8e44fd3-8e8d-43e5-b7fd-3b425b9ae0ca',\n",
       "  'd9dc8f45-c6aa-4cea-bed0-ecf032c68fe3',\n",
       "  'fbd2f4a5-0ba9-4859-a78c-9383adb511a2',\n",
       "  '1913f57e-4959-4568-9f7a-f33efc8733a5',\n",
       "  'a90bc250-80d3-4518-8487-1ea6b49f221a',\n",
       "  '4a94e586-d169-41ce-9156-328bf25fd3d4',\n",
       "  'bcdd5fd8-9213-4cd9-8b48-9747fc9be5da',\n",
       "  '5ea2263c-58b5-4a2b-baa7-05c5e12c520f',\n",
       "  '529f1d08-0a86-47d0-9c01-14c87eed6cea',\n",
       "  'ab760730-3674-471d-bbe0-0365b7b8d6d8',\n",
       "  '5f39d07a-33bd-4e38-a1a8-da9b6f0efa78',\n",
       "  'f6bdcdff-71b5-42f0-a5b3-4ea575bf967b',\n",
       "  'd86274e2-119c-4e37-a5ca-b9dfd93fcb9b',\n",
       "  '6e9cb9cc-6689-4ef0-abf4-f617e63dc932',\n",
       "  '9c788f95-6cc0-47fe-a912-7f0611fb00be',\n",
       "  '6141c29b-b434-4f9c-8d1f-4b064fd408e7',\n",
       "  '6d512418-eab7-4548-a989-6e800a23819e',\n",
       "  '4b0584f7-ca0f-47d7-a271-1af018b07513',\n",
       "  '8011d704-78f9-4799-af74-55beeb85b5b1',\n",
       "  'd3d328fa-c396-481d-94eb-caed1c24f3ba',\n",
       "  'c2f642ed-6fd6-4916-bce3-24fcaaca7ea6',\n",
       "  'b6885f18-9c7c-49b4-adf4-4dbd883dbe54',\n",
       "  'dad4544c-11c2-41e8-bf12-40b5c8661ce9',\n",
       "  '8af2912a-3cc2-428b-8c39-4ab7a010b438',\n",
       "  '959c8bea-25ee-4e55-8ec5-778a82ca25c7',\n",
       "  'a7596b2e-4816-4b81-8deb-f47884f41e53',\n",
       "  'e76ff976-d1f9-42c2-b5cb-802c4fc934e0',\n",
       "  'ef5b0841-831c-4007-ab14-ef378bde9a62',\n",
       "  '755edb1a-ce23-4a2c-98e3-a3a280d8ba71',\n",
       "  '8505f801-41b0-40c7-8f1f-2748769daa84',\n",
       "  'a3fe342a-f1f7-441c-a81f-5d847d9441e0',\n",
       "  '08bd3933-9ef9-488f-881a-169ee401652b',\n",
       "  '5f4ce1f9-d1cb-498a-b692-dd621ee35f49',\n",
       "  '9077dfaf-fb30-4cdb-906d-66356b3811ef',\n",
       "  '357c5734-3add-443a-9f61-789913255102',\n",
       "  '636a8133-757a-429a-a38c-c76ed6670901',\n",
       "  'aa5948b9-675a-46f6-a9e9-13d12a38be2e',\n",
       "  'c1971398-92e5-424f-ad91-41210dc364fb',\n",
       "  'b35bb7e8-3dd1-4adc-95b4-9eec0210f4d1',\n",
       "  '42f2a268-85cb-49b9-a3ca-6b5398a89eea',\n",
       "  'ba437fe6-1fd4-40c7-b929-88b772ed18c4',\n",
       "  '0accf740-28cb-498b-bff4-c750739ea484',\n",
       "  'fc671f7a-48c3-45ba-82db-ee9810924e17',\n",
       "  '1eb1aa23-925b-4afc-b7df-ac6112dc7c3f',\n",
       "  'c70de9c3-b7a2-4722-92ec-0bb9d061e000',\n",
       "  'd5d783af-0170-4021-850b-885614fce35f',\n",
       "  '69226cb5-bf06-4483-b8a3-913157cf3d58',\n",
       "  '389b32eb-7999-4715-90bd-c537a53d4cc2',\n",
       "  '3381c085-b3f7-4a68-b3b3-364bac09dec6',\n",
       "  'cfc5ed92-f5b3-470f-bdf7-78bee7465e23',\n",
       "  '4a8bbc16-3676-4e88-8844-d889acb38e37',\n",
       "  '7c158c91-8264-421b-aaaf-778fb9f33a3c',\n",
       "  'b41a58ec-621f-4cc4-9419-e889969084ef',\n",
       "  'fb9118ad-f2bc-4abc-890e-0cc7f3566df0',\n",
       "  '6610b421-71c8-476d-8de5-9ab7873c5c69',\n",
       "  'b61f4f32-3645-4654-a8d8-c29e067f4497',\n",
       "  'd5736117-ccc2-4ce6-bc17-7d4f21ea58fd',\n",
       "  '79db6719-24f2-4c22-b475-9db10c3dc754',\n",
       "  'be5172f0-5170-4c8a-9000-fb6d647bda4e',\n",
       "  '67703f27-3abd-43a7-ac01-17b6fe690852',\n",
       "  '87ba58a8-79db-41f7-8665-7ac787b71400',\n",
       "  '7e969e8b-10a9-4b63-b924-66957d206e76',\n",
       "  '37ac529e-beb4-417b-80be-e62665c64852',\n",
       "  '6576bc65-e4bd-4e6a-b259-39dc57e67617',\n",
       "  '96a2c528-ea0a-4259-9516-51e60754199c',\n",
       "  '0b3361c9-a216-4047-87e5-119925e08aa6',\n",
       "  'a4fc86f0-9b3e-4c15-9a1b-b75dfe11e693',\n",
       "  '83061d89-8c90-446a-9c6e-dd510e3d72a4',\n",
       "  '622d9987-ccbf-47ab-8de0-e4133b40f255',\n",
       "  '09c7fd57-78c4-4be0-85ae-2422e4f7c098',\n",
       "  'ff78b328-b707-427d-b7c7-bdff7138884d',\n",
       "  '147cef02-235d-476c-bd9f-4728487b8eb1',\n",
       "  '19ebf178-cdbe-4c92-94e1-1be5393aa039',\n",
       "  'c0262120-a7de-440b-95be-1af3aaf55ac0',\n",
       "  '641fc7f1-b3ff-4db2-a59e-7aefc8fa651f',\n",
       "  'b73681a7-c7e0-471a-8407-4358278d4de6',\n",
       "  '6830be03-fc03-4eea-af35-1a1dc42d086d',\n",
       "  '814495f3-479a-4684-9f59-12511bf6f169',\n",
       "  '6d347475-5891-4ba6-9ec3-71fcaa462ec8',\n",
       "  'cc234a02-194c-4d35-9e9f-2ddd0df86d48',\n",
       "  '13663682-a6ae-4073-9482-82fb304ecdb8',\n",
       "  'c053969f-6270-4a26-bb88-e20315a176f7',\n",
       "  'bbae35d1-e06a-4bcf-aa3e-752523ecaed0',\n",
       "  '8e2c9f5a-559d-4bb9-a76f-d6e3acd192c5',\n",
       "  'c4fefdf8-a683-4d32-9ef1-7a9f3edf66bc',\n",
       "  'e776109d-4628-4cf3-acb3-26c62e9ae449',\n",
       "  '0f9221c7-5503-4729-ae91-10a68f216c4e',\n",
       "  'b15b80ef-fbde-4a69-b22b-eca1117ae125',\n",
       "  '5d9c200e-fb88-4cac-a7d7-e6e886e1dc33',\n",
       "  '2c72e4db-5575-4cac-a17e-44734d222e13',\n",
       "  '41a67523-e525-41f6-b352-598e3ee7e0ab',\n",
       "  'bc4b71dc-5eb5-45d5-a5f4-e5e42eb19236',\n",
       "  'eb6aed4b-ff9d-41d0-848f-d6b0db46b28e',\n",
       "  '78829c37-b182-4fcd-98fe-3a2032798707',\n",
       "  'de58ac11-dbc8-4182-90ca-1c1dad206115',\n",
       "  'f7b8d7f5-d27d-4c25-a75f-3aca2abaa58b',\n",
       "  'a970dcf8-e222-4985-8dc3-ea7e97b17588',\n",
       "  'f303a788-d881-4035-a3de-96a8d1a803eb',\n",
       "  '75e21f9e-aa36-4b34-b635-76c857c2abcd',\n",
       "  '9ca2902b-dc17-405e-9c8b-8869b1dcfcf9',\n",
       "  'b05f37fb-b178-4af6-8fcf-9d468ebad909',\n",
       "  'f6778726-c693-4a58-895a-cb8b5d41b52e',\n",
       "  '5d22e083-4b0d-4d2e-90b2-28c95b964755',\n",
       "  '787a4417-0f17-4936-a785-8632ffc1073b',\n",
       "  '9e7e70d7-6ba7-4b21-96c2-0d06f3f845d9',\n",
       "  'aca66dc7-896c-43f1-ad6d-f8a470ee9e51',\n",
       "  '3dc717cf-ceb9-407b-a061-ebe880ab2a03',\n",
       "  '4d7eaaff-253e-4357-b934-897956f3c54f',\n",
       "  '817d9cc9-ca49-40a1-822d-5c9e48dad53f',\n",
       "  'f7eeef95-ae90-480a-b7d5-b81e9db3eda5',\n",
       "  '5e56d685-4f25-47a2-89fa-88761f4ce3e2',\n",
       "  'ae87577b-2255-497d-9bda-85008acfbdf0',\n",
       "  'c3364a6f-7e6c-4e88-97bf-69235d4d5fd0',\n",
       "  '508a4ae6-213f-4e2b-9d93-fad8df7c0f37',\n",
       "  '385ddcbc-c976-48e3-bebe-91cb3f975196',\n",
       "  '92913c69-a690-4f2f-9efe-510645c31ed0',\n",
       "  '2bc0538c-6909-491a-98d9-8680a503376e',\n",
       "  'ce7a9b82-2859-45a3-8660-554841beb37d',\n",
       "  '5d78eaae-357e-4bc9-8b41-e793ee1f75da',\n",
       "  '993bbb12-1c0f-4517-a702-16e6e9771e81',\n",
       "  '571e8e61-5d6f-4fa0-ae81-5503c8473acf',\n",
       "  'ce522bfd-626e-48b0-98a8-3583ea5f86d1',\n",
       "  '8738fbd1-484f-4cf6-96de-b427dec3b6f6',\n",
       "  'e83210df-b468-4e87-8adc-d13634ce6224',\n",
       "  '90f999d4-0356-4671-9d0e-8092b7ec1acd',\n",
       "  'a28390dd-8f5c-4330-8a13-164b9a4fc7cf',\n",
       "  '52cb6097-7157-488f-b9e9-5e7f3fcdefc9',\n",
       "  '1527a55f-03d6-475c-a0a8-97b3dc244122',\n",
       "  '525e7ed1-8fa9-41f9-87ef-3298f30b4f0a',\n",
       "  '37fc104f-e8ac-4495-b72e-ae5eacd65e60',\n",
       "  '5ad8bd99-357c-4b2e-958c-c76bef6e1e7b',\n",
       "  'e6b0223f-1500-4744-a7de-91d327c14835',\n",
       "  '66ac4732-adfe-4f34-ad73-b55ffd2150ca',\n",
       "  '3ccd8b64-fc3f-42e1-96c4-0e290228da14',\n",
       "  'c143978f-a128-40b5-a09e-7325807491c8',\n",
       "  '7e78fbea-443f-4b89-a786-5d141d6ae2ac',\n",
       "  'cbce9509-2ccb-429d-b3e5-dd8a3074286e',\n",
       "  '6967897b-1c4a-4c7c-88de-e82d802a6499',\n",
       "  '84c863fd-0ee4-4251-83de-3965e7d3e79f',\n",
       "  'c2c8442f-8785-4199-8cb2-88bd0865b7a3',\n",
       "  '0a005759-2e69-476d-8768-c8bdb9746503',\n",
       "  '061e256f-21ef-4ea3-9f81-09e73e622e68',\n",
       "  'cc5cc739-3eb2-4428-a7c3-8823b5ab6102',\n",
       "  'a826fef5-bd80-4437-90ef-862a56e7eeae',\n",
       "  '68cf4f98-0dc3-4ca5-86c5-d3bc276083ae',\n",
       "  '04ff259f-873a-4810-9d8c-e2a39528653e',\n",
       "  '4e88005b-1c98-4200-93b8-5e8daf89976b',\n",
       "  '7c7db06f-03fc-4fe9-89b7-f80588f0d7c9',\n",
       "  'ddbdb531-1614-484c-883a-ebdaea672fe4',\n",
       "  'cb79050a-2695-48ab-a763-9fb67e2547f4',\n",
       "  'd0e4c64e-7fb1-4745-b255-b1440b3e87df',\n",
       "  'cc2542a9-d4b7-4346-8ed0-ef10fb9cdcfd',\n",
       "  '4528fe06-368f-41ae-aa33-9471df6135c2',\n",
       "  'cc81456b-e5af-4fe5-9afa-59ccc652a5e8',\n",
       "  'ddd81b5f-8afd-4f99-8bc9-20e5dd9537b8',\n",
       "  '636eea48-ec93-443f-a6d1-56c2cada8c66',\n",
       "  '695eb458-0466-4b6d-961d-ccca2e14270c',\n",
       "  'e9926f32-c3e6-459d-abb8-d318e0115145',\n",
       "  '0c4f0b30-20d4-450d-960f-214d2891c512',\n",
       "  'be5b8b22-35bc-485c-8fca-8bb383b146f3',\n",
       "  '11c04f56-46bf-4e00-b23a-3c7638b7e87f',\n",
       "  '1b22e6d6-63e0-4628-94f3-53e241b09577',\n",
       "  '483cca74-8b60-4d6f-858e-9882c7bbf465',\n",
       "  '92019c66-3808-49ca-93fe-29048c56d5ca',\n",
       "  '47505c31-3559-430f-8cbe-55ec5d26dc92',\n",
       "  'd1b3efa2-e702-4801-a31c-24d5bb77b754',\n",
       "  'c47c280a-d893-4e01-84c6-99be844448cd',\n",
       "  '447b5a8d-df9b-47cf-a1aa-63da855386d5',\n",
       "  '4cc34040-86f7-41a2-bc8c-fd68e9e65c61',\n",
       "  'd36808e4-464d-4c1d-b6ed-b0c300677ff9',\n",
       "  '3cf50ea3-aa40-45c6-906f-1637ef5f843e',\n",
       "  'bfd6a941-cdf1-4628-9cb5-b075375ae70f',\n",
       "  '00b5e891-d599-4ac1-8d80-80f292a7d7a5',\n",
       "  '95361f56-b5b0-4163-ba1b-a5666c2bc90a',\n",
       "  'bd971f0e-edb4-4acc-89ff-9610950461da',\n",
       "  'c1cc7166-553d-40f5-87e9-4c1f84982a15',\n",
       "  'fbedb118-fff4-4ad2-abb2-78c4328524fb',\n",
       "  '0bdda8d3-c948-4a0c-8153-83208ac2b94f',\n",
       "  '61339033-a44b-4371-a9c9-228149a5bd45',\n",
       "  '2c66069d-3476-4d57-bf88-7d58e163c48b',\n",
       "  '92fea8cb-9703-49cf-bd55-2109a2ef513a',\n",
       "  'c74b18c9-0b51-4dfa-8965-4464ebe5f190',\n",
       "  '132f7f3e-b655-4008-9a69-400bab5c27f3',\n",
       "  'f3e82bf3-3ed5-42a9-ba33-6023179ab759',\n",
       "  '95369763-fb96-4aa5-8c87-4a8e0c3515c8',\n",
       "  'f3aed06c-4803-4b85-802a-4c1cf1a0a109',\n",
       "  '1bfdc9b8-50de-4964-8021-c42bacda55ca',\n",
       "  '507aaa72-e822-4b32-8799-29e82bfd041d',\n",
       "  'edb19891-01f1-4427-960a-e836b4ea7ad5',\n",
       "  '412e4a0b-b840-4619-a300-59e9b9af86fb',\n",
       "  '0e907808-2138-42b3-9721-34897b771ee1',\n",
       "  '7e65c0d1-4f27-4c50-b596-6a9722ffec7f',\n",
       "  '3af6442f-a79d-47f0-a43b-b560d0a286ab',\n",
       "  '287395c1-360c-4910-b8f2-f58c9ee444a3',\n",
       "  'a7d8b157-1358-45ee-bd91-4b6d207a4012',\n",
       "  'affbd81a-5005-4c61-996d-d501a9bc45c9',\n",
       "  'b7d0323d-0601-413d-a301-5d3a1f19f6a0',\n",
       "  '484614a3-36a4-43aa-b686-d8196738d174',\n",
       "  'a29f6d1f-df61-472c-9aac-83111a3d9e89',\n",
       "  'af55f546-0a03-4b9b-b39e-594353b4f154',\n",
       "  'c4fcb6cd-4f11-40c6-903d-e5a395b059e3',\n",
       "  '55db1240-29f6-4dda-891f-73e4209da752',\n",
       "  '76c4858e-99a4-4cab-892a-caf4610a004e',\n",
       "  'e62ae4ac-e073-4747-adf6-086405db763a',\n",
       "  '29982a47-29e0-4479-befe-3d47e9a8169b',\n",
       "  '27eac2bb-04de-487a-8983-c8ce4a6adf37',\n",
       "  'bb9b7f34-7ef4-4462-817d-2ff49df9a8fe',\n",
       "  'a82f10f3-60dc-447d-912d-f89076c26b8a',\n",
       "  '92303764-d469-409c-8127-14382a3b6990',\n",
       "  'b4c06c26-07cb-446a-a995-dbf0884563a0',\n",
       "  '5ce1bff7-b4e3-4db7-9ae6-b59a9ab45d96',\n",
       "  '1d2edd1a-5119-4d36-af14-cd80086e619f',\n",
       "  'b9e750db-cf9c-4889-9fd4-d924048bb366',\n",
       "  '567f120f-1864-467c-9c82-2413f41d6154',\n",
       "  'bcb149ee-7e5b-4e3e-9f13-939ab0530539',\n",
       "  '62f9f889-3a08-4d65-9a7c-034458ebbcf5',\n",
       "  'd1d56eaa-9365-4eaa-9822-2dc75e311ceb',\n",
       "  'bc124bf8-94f9-4ae1-9de7-a7fd87c5cc9c',\n",
       "  '2dfc0ad9-a7e1-4048-b313-e497ee3c3e5b',\n",
       "  '01a8b397-5e12-463e-881d-9d7e5c6be561',\n",
       "  'd5d6b7ee-403b-46f0-acfc-590f7e0a9dfc',\n",
       "  'fe9e7381-dbce-47ce-a849-9cf0180867f1',\n",
       "  '27663f95-2837-480d-ad60-2ed503d947bc',\n",
       "  'c2f892b1-1734-4fa0-8867-bc2637585fd6',\n",
       "  '5d30e9f7-340e-40b0-985f-855e9d589ce3',\n",
       "  '2d9628ea-ba03-45ab-b246-606a2c82c138',\n",
       "  'af5529eb-5dcd-49bf-a0b4-f05b703ec80b',\n",
       "  '2ca87cef-48cd-43a6-aa50-8bae67878efc',\n",
       "  'cd97a01e-7449-4fac-9b9d-20e809cb4b9b',\n",
       "  '2146ab16-1afd-4a03-adca-377d30d0f4f1',\n",
       "  'cd1a91f4-6377-47f1-85e6-e58c45fc02f6',\n",
       "  'b04ad946-55ae-4b51-ab20-a0441a3955b1',\n",
       "  'cd079269-1d6b-46d8-9e63-7cf89e0177a1',\n",
       "  '463bba30-321a-40ce-bfa0-36503f5ebbba',\n",
       "  'aa39a773-eb88-4a50-9193-61a4dffcf379',\n",
       "  'f93469fe-0cfc-431b-a97e-bb89c615549c',\n",
       "  '0a31d203-8a21-40e6-9616-947792bd2498',\n",
       "  'fa10c334-7d1d-48e3-8dbd-81c64f823583',\n",
       "  '14a6e84c-e6e8-40d1-920f-4d766fab683e',\n",
       "  'b674d63a-36ec-4fa1-a420-86667ed9cfe6',\n",
       "  '2876aacc-5fa1-4f74-864c-a58ca09c7a83',\n",
       "  '52b69c6c-5d80-4bdf-9cb5-8fdb4aecfc2a',\n",
       "  'a281fc7d-283b-4fba-b5d5-85cd00df82b4',\n",
       "  'a5369336-8321-4e8f-ad8a-cc658f7abf38',\n",
       "  '3a93ce0a-fba8-47c0-82e6-b3d14ac15eb4',\n",
       "  '2c93132d-bfee-4ab7-ae8c-4eed4c1170fe',\n",
       "  'c630e72d-fec8-41ae-95c1-e2869e3a35bb',\n",
       "  '8f964b94-5ab2-4c8d-b01f-bc4a936eb184',\n",
       "  '6e9af820-f8a7-470e-ab44-8334295e5da7',\n",
       "  'd39bd33b-6e17-4786-bdff-6fb0df4b5c1f',\n",
       "  '1a156f06-675e-4238-a6d2-78d36d12ff80',\n",
       "  '376e9c7c-1607-4efe-984f-e9e5c5e0e343',\n",
       "  'ea10eb39-aa5b-40e0-a457-2c40e8be9b6d',\n",
       "  '3c69f5bc-7d98-47ad-9cfe-c96e37bd3b19',\n",
       "  '1d6ca309-55e4-459f-a04f-a0069f68a4e2',\n",
       "  'fe4b8241-3299-4384-be24-808246aee3a7',\n",
       "  '87c89383-30e1-4a4d-a03d-657ae1758911',\n",
       "  'c7e6e486-54be-4bb1-89f5-b64bb9bacb79',\n",
       "  '7f49b7b7-e9e9-4c5f-bd45-5fcdbd639c9e',\n",
       "  'a9ce13ab-eed5-4a3a-b84a-1adfe316c4b8',\n",
       "  '8c9d9831-b230-4dab-86b8-0be6b6689188',\n",
       "  'a24a5eaf-bbe2-4494-a236-e5bf847e7191',\n",
       "  '274b1460-cd23-447d-8b03-d2a51298ace6',\n",
       "  '1556877d-49af-4e77-9878-cd988730d53c',\n",
       "  '85710d2c-a9a5-4034-bb6a-d138b325e8e4',\n",
       "  '7dcbe3cb-0a0f-4976-8be1-0685b323100e',\n",
       "  '22d665e7-29e9-4809-89a5-fe40e394215e',\n",
       "  '60139eb1-3b06-45a1-b06a-dfd5f205ba01',\n",
       "  '60193a2f-3198-4c11-8ad8-d3ad09a67ba9',\n",
       "  '35e6af66-f5c2-4ec3-bbaa-9a8fde9c888e',\n",
       "  '2c12de73-a093-49b9-b20f-2854ff83a644',\n",
       "  'fca86ff2-cecf-4636-95ae-9fc280dfb1c8',\n",
       "  '36607098-520e-4a29-90df-d7106b0b58cc',\n",
       "  'd1686706-d96d-46a3-8335-8a2dbf72e4e8',\n",
       "  '63f86b66-44ff-4ce6-be8a-3eaf6f0fd93e',\n",
       "  '8c5cd6c2-e2b3-476e-abeb-a6e90b9a3270',\n",
       "  'e821a0e2-87a1-4679-89ae-f5607b97d3b1',\n",
       "  '97d2b680-2fea-49de-8daf-306d473cdc70',\n",
       "  'ea71df09-c6a2-4306-9018-a4c16fa454d5',\n",
       "  '98d30c44-5942-4ac2-b781-4105e03d9a44',\n",
       "  'd5af1c44-11d3-4626-b0d9-2a23f4e96c01',\n",
       "  'acf7a92c-6dbd-4c7b-b24a-4542e40bdd3b',\n",
       "  'ffc2dba1-4f6e-4946-936a-c444a9f721b5',\n",
       "  'ff614636-aab0-46f0-9b17-8d5b9b31ad74',\n",
       "  'e4255501-1e83-46f3-8655-9d5bca780b26',\n",
       "  '242069a0-a27d-4144-b815-f63903aa101d',\n",
       "  '05c32c1b-7398-42e7-9722-414fb11498c6',\n",
       "  '5ad99d88-3b4d-4fea-aa02-2c91bd431404',\n",
       "  '2051dc28-655e-429d-b94f-02839b2085fc',\n",
       "  'd016583c-f83b-4dbe-a01d-4c07b4bb45a5',\n",
       "  '86920404-b885-4b11-a616-b4f4f68958a0',\n",
       "  'cd360fce-4e67-4766-ab61-9602467e5cbc',\n",
       "  '4c328f9b-8f91-4a5e-a1f9-dbf43794aa6e',\n",
       "  'c8d5baca-c334-4bbf-a6c4-ba5a4a08b5d0',\n",
       "  'ff0a6ce9-056b-401d-90ee-f98dc313c652',\n",
       "  '926ded88-444e-4bd4-8d9a-8168d83154ef',\n",
       "  'acc054ff-cfed-4be0-9ad1-ad1f91ae212a',\n",
       "  '5767e9e1-42e8-428b-9453-5548ea7f918d',\n",
       "  '220bd07c-6473-41f4-a061-8ddf48012826',\n",
       "  'e4c4193c-b5d2-47f9-a6ce-429f6095be85',\n",
       "  'd76e435d-0332-4eae-a574-7c08c6c115bc',\n",
       "  'c8dfc040-910b-4b0e-ba3a-5e1700b4a92a',\n",
       "  'e6d35399-0bed-4e99-a3b1-2ea017cdb85e',\n",
       "  '083c3486-c013-4b0f-b0ac-61e68c8b7ec4',\n",
       "  'c82ba999-7716-4751-8da9-9cd859311b73',\n",
       "  '0ef82e72-af37-4c53-a2e3-f609c6724e52',\n",
       "  '9866c6d7-bfca-4723-9e6e-a5561da884b1',\n",
       "  '8c95e67c-0312-428b-b69f-85a7fd8d784d',\n",
       "  '2f239218-7213-478c-9b94-1c08db8b36ef',\n",
       "  'a8060ddc-fe28-4ee0-a218-c71c82fbd776',\n",
       "  '388b0c96-8b4f-4f3f-bc2b-31df157579b5',\n",
       "  '86fcd8d7-d620-4c31-902a-20bc14b97219',\n",
       "  '5e085ebf-e26e-4771-b1eb-266afc58c88e',\n",
       "  'acf105c7-0cc5-471b-93ef-8834d172e95a',\n",
       "  '581a5b38-f477-4403-9134-b06fe61c8392',\n",
       "  'fa86fdad-3ed3-4e9d-83c1-d15a6ddf1560',\n",
       "  'c119ac40-23c6-49ad-bc08-fec4df30b085',\n",
       "  'de8da563-7e0f-4307-8899-41f256b35f5e',\n",
       "  '7921f1fd-7771-4d2a-a8e9-872c837ec72d',\n",
       "  'a9b22b66-2ee1-4e6d-9d8e-f40b1da0e05c',\n",
       "  'c69cf9cc-9f26-4304-a2c9-fdc3c88d4e90',\n",
       "  'f3b02617-1614-4651-a4c2-003a2ae38b01',\n",
       "  'a1be953c-9694-415d-b614-ea7d4632ce23',\n",
       "  'db0e6bfe-aac0-4bc1-a921-5096510ab40c',\n",
       "  '29bc55c3-25af-4a3e-a574-bd4ad3eef067',\n",
       "  'e7719041-aced-4cc9-a21b-45f1dc0bc786',\n",
       "  'c7d1de11-05bb-491c-b29b-e2be876ad34b',\n",
       "  'f8a0fbc1-10ad-482e-915e-911b06d7ecb6',\n",
       "  'a5169401-0505-4915-a4b9-103ccc39224e',\n",
       "  '22515f04-bb8f-4576-af5b-a2eb911bce2a',\n",
       "  '3f9393ab-7ac9-4217-9061-5cb9747f2b6c',\n",
       "  'e51d1044-3edb-4ad4-8abf-17358ae8589f',\n",
       "  '2a2d1be3-ff4e-4f70-af98-ae18ae4c0665',\n",
       "  '9c9628e5-1e84-45ae-ad62-d3db944a9614',\n",
       "  'c66431d8-fc53-474f-bf99-504a17b77328',\n",
       "  '82d46973-e6fb-4f6a-b077-56ad99088601',\n",
       "  '2087e224-bc0e-4c1c-8fb2-3fafebfad2da',\n",
       "  '6b31fbbb-39aa-4ff5-9caa-ccf1551a47dc',\n",
       "  '28e7b033-9d9b-4a41-b472-13afac8cb1ca',\n",
       "  'cf5bec5f-d4b9-4a53-b062-e167229505ac',\n",
       "  'e965ea5d-cc48-4ed9-9c82-8e00825af477',\n",
       "  'd2ccbb58-bf41-4109-af5b-7fa6043afce5',\n",
       "  '6bca62f4-61e5-4472-a14c-fe0e1a264da4',\n",
       "  '0da90107-416a-4350-9503-781a77fe2df8',\n",
       "  '59c68fca-4881-4395-940b-ab649a25b98c',\n",
       "  'fb14b1b1-1c3a-41a5-bb88-c82f75cf9503',\n",
       "  '18f484b2-a739-4af4-9ae0-0ab1b06f5ea5',\n",
       "  'c23652b4-08b1-4865-8c1c-2fb33246ab2d',\n",
       "  '21a0435e-81ce-4517-b769-d69c34bdb797',\n",
       "  '8a0d8b57-aed4-475f-8291-7fbc914ce78f',\n",
       "  '4dfdc2fa-8b28-4aa5-8b14-b15d13c58458',\n",
       "  '7a424685-867d-474a-a420-ceb628426aff',\n",
       "  'db634de4-13fb-4667-88bb-c91ba60133b0',\n",
       "  '316cd894-b918-4587-9116-7673de95742e',\n",
       "  '7a726c3a-c0b8-4152-9c4d-e288741b4aef',\n",
       "  '48304baa-4ae4-41c6-a678-3f9a608fb986',\n",
       "  '457056e6-f0b3-43ba-be26-40b61d6dfa92',\n",
       "  'd01f8b82-f513-4610-a233-f223de4a74c3',\n",
       "  'ccf15a9b-ae5d-4ed7-9ddb-a456197f4eae',\n",
       "  'dcab51da-9717-4325-93b9-dc8ec9bb1c6b',\n",
       "  '6ea40396-aeac-4421-a027-80d0a292c29b',\n",
       "  'd1a51928-7183-4471-a4b9-9c1dc6f8f2b5',\n",
       "  'e902a83b-dd4b-4f04-a0f1-3d62f4183f9f',\n",
       "  '1f1630d5-fa09-4f77-ac3f-5f73c022c736',\n",
       "  'f2d08b21-975f-464c-821d-5cb2d3d05a05',\n",
       "  '75cb00c8-a0d0-407f-a3d2-cb275abba010',\n",
       "  'a8d373cb-d3b0-4a96-9594-30a51cac6c22',\n",
       "  '553b17db-9fa9-4347-aa85-2252489e17aa',\n",
       "  '18f49268-2f28-447e-8105-5c0ab434137c',\n",
       "  '036f8cb4-53ee-4994-ad5a-75c0b3b1688d',\n",
       "  'fb0ce21e-db7d-47b8-90e2-5094714846c8',\n",
       "  '739b9a29-a2e1-4914-b7f4-0a4754c7c6a5',\n",
       "  '055af9cc-efe4-4759-be51-276d4810b2ab',\n",
       "  'bf8aacb0-f3d2-4a39-a5b3-139e47ac25df',\n",
       "  'ec3876f5-d4ce-4f4a-85d8-2f9fbc075879',\n",
       "  'dd96bb61-8695-4c2e-a3e3-fe118ec2d62e',\n",
       "  'bdffd1df-f0c4-4638-a5bb-ef6469341392',\n",
       "  '8b8c5445-2684-411f-b2a0-8cb279ee1f46',\n",
       "  '98507922-d684-4049-8dc3-90b68191ac4d',\n",
       "  'b115c24e-714b-49a5-befe-7b622c467a99',\n",
       "  '61b0bb8b-7756-4bd9-af9b-1fb9072eb6de',\n",
       "  '54072f9e-a059-46e5-88a4-ae9a863a851a',\n",
       "  'bcc74581-7830-4e47-9686-ef86cc89a4d4',\n",
       "  '8af2d1e1-f2a5-435f-be39-65ca808035fe',\n",
       "  '9b9d9ac9-ef8b-4922-b522-802a15d87760',\n",
       "  '24452d6e-2cdd-4e03-a27c-5793789d3b95',\n",
       "  '0319b71e-d859-494b-ab30-9610e4c5f371',\n",
       "  'b9864cb7-8694-47e8-96d8-811ad5eddfe0',\n",
       "  'fc7359da-a2a7-4541-a479-29338c77c86e',\n",
       "  'e273ffbc-5a8e-4d63-8cb8-a6fbeefdd4e4',\n",
       "  'c4762722-80b8-4fd7-b362-94d188c1cc58',\n",
       "  '973df190-f239-42e1-a736-fa6e522752c7',\n",
       "  '078af245-5213-42bd-be99-46a0fd2891e6',\n",
       "  '214522fd-99d3-4668-8121-47f4fff6ade5',\n",
       "  'c9e93df9-7016-413a-b9cb-2e1a1f075854',\n",
       "  '5de7bee7-0c5d-4b05-8874-ab804c22f299',\n",
       "  '217c4b75-ee0c-479d-9b0d-00114fb0d32c',\n",
       "  'af1526e8-64ed-4656-955e-c64bcd2ab16a',\n",
       "  '242e6949-d8ff-4feb-a689-f2b73e9be3a4',\n",
       "  'efcc2e3c-ddae-4f4f-bc36-4c8b0a5ceb11',\n",
       "  '97d05c8f-f1d6-4dbe-b55b-36ef7b516d2d',\n",
       "  'ca8ca23e-e4ca-48f4-85de-e31c203c2b39',\n",
       "  '3680f631-7104-451c-af51-9ca4b744e5a6',\n",
       "  '365cba96-c907-4077-b680-9de2761a659c',\n",
       "  '9e15ad23-d9c5-4b86-95e2-f0b6276f9aaf',\n",
       "  '0e89e05f-fb31-4614-b3cf-249c28c6c6f1',\n",
       "  '8562a0ce-bbc6-477f-a711-3ddb1323f021',\n",
       "  'f3fe905a-d09e-42a8-98fd-6c9cc0d928c0',\n",
       "  'd992b522-3a91-4c23-8a86-c2ff6fa16219',\n",
       "  '0218ec7c-0569-4b11-8071-4f3049f12074',\n",
       "  '31e43754-3f4b-4f36-856b-fea37a644138',\n",
       "  'f04f9bc4-6cc5-42b3-b78e-37ad80d283bc',\n",
       "  '0ef3d03b-bf1e-406d-b641-5f928016ab41',\n",
       "  '3bd1e570-ad98-4dca-80aa-7b1877c6d3ab',\n",
       "  'a169d016-9b48-4b41-808c-1bed7784b0d2',\n",
       "  '951a1912-2a81-4b4c-8152-9f910689529f',\n",
       "  '5e7076f8-b201-4b1d-856a-f64353573a4d',\n",
       "  '6191674f-ae92-4a37-9943-5bb058faea25',\n",
       "  'a760098f-d64c-4b0c-a814-cce1fca1c0ec',\n",
       "  'ba90939b-0ac1-49e9-ba92-c70a63017323',\n",
       "  '787e8c3e-c591-46ec-b4c8-b73265b3d64f',\n",
       "  '90e1075a-229c-4600-8b3b-a0bba15871fc',\n",
       "  '5599745f-e546-44ac-b851-810ae8d1978b',\n",
       "  'cacb880c-011a-4ef6-8a91-041f9e160a2f',\n",
       "  '273f6293-4db1-4bcf-9c6d-12cb519d6627',\n",
       "  '00f2dab6-96ac-4891-87bd-d040d33bec13',\n",
       "  '64f78588-bc0c-43aa-ae9b-d08504c48c6c',\n",
       "  '8265335d-88c8-4cc1-9b03-ee3adcf95f47',\n",
       "  '569ff84e-4858-4e7f-a079-1ba5d4166d0d',\n",
       "  'd8564774-74a1-49a1-8fe9-1e6406ec2b59',\n",
       "  'e1d60416-5d7e-4df5-8e22-99e46640815e',\n",
       "  'd8a89132-3d20-45a3-a43d-09d1a7c0a9bb',\n",
       "  '505709fc-a284-440c-854d-63285b6b6909',\n",
       "  'b4b30d52-1fe2-4469-a4c0-b709240e09bf',\n",
       "  '4a4292c6-c386-4ec1-a344-bcecc3078374',\n",
       "  '1194df17-7bcd-44d3-a5d5-e9bc9b4ca48f',\n",
       "  'e859e6ce-845a-4303-bd96-6b648b969845',\n",
       "  '3b27a3db-c13d-4dbe-bbec-48f41b221579',\n",
       "  'ee441ab8-413c-4c38-ae84-251438a2bc16',\n",
       "  'f4fb3e72-27a5-4f47-b035-e5ef9edcc4ef',\n",
       "  '5f392410-bae2-4cdb-86aa-be1d9abe33e8',\n",
       "  '3ce45097-2c88-43ae-b203-b1398d4569c4',\n",
       "  'd2f86d8b-ec3d-4685-8b17-8b8082a747ec',\n",
       "  '5e376b4d-221a-4c51-b5af-e70eea871a58',\n",
       "  'd9dadc1e-d0e3-4f5d-9af3-f32cec51bcb5',\n",
       "  '11af6461-9e10-4606-9520-7165c24ae1d7',\n",
       "  '753fc3b2-458f-40da-9429-5fc98aea0885',\n",
       "  'bc72ff97-b1e2-49a6-805b-8146d7c633ab',\n",
       "  '768a7d46-4593-4b8e-9550-9ced3471a265',\n",
       "  'dcce883a-c796-4c53-8147-191ad407106e',\n",
       "  '2b79b5b2-0405-4d26-9c24-134de685dc66',\n",
       "  'c694ce24-792f-4894-88c4-7e3866a95169',\n",
       "  '77c20121-0a98-4f5a-967e-c94fbb1b86c3',\n",
       "  'c5221f27-c283-499f-b565-aa52059825c0',\n",
       "  '20eb11ed-ee8c-48c6-84b2-ff70f19dd17d',\n",
       "  '6277f0f6-5351-439b-afdd-4b21d1b599dc',\n",
       "  '5c370645-42b4-431a-a574-2d97eedd06b6',\n",
       "  'afefdaf6-335b-4bca-9a8d-d5040c61e04a',\n",
       "  'bd8c311c-3d0d-4290-97d0-ce1147216b75',\n",
       "  'c195676b-1611-47d8-8b75-39a6456db475',\n",
       "  'ef5b1a33-18ad-4f2e-9c4e-ac47137a6c60',\n",
       "  '24d1f032-4d82-41a1-a902-86472da5ad03',\n",
       "  '0fb22aab-6b87-4476-b446-48e95f8a4bc4',\n",
       "  'eaac8f0f-1ff2-401a-905e-2a7c30946205',\n",
       "  'c27fb0ce-dc04-4345-8114-6c30650ad624',\n",
       "  '1bca174e-e56c-4287-b51d-5e9d03b4a1c7',\n",
       "  'd4841f13-876b-4be2-adbb-7c95926b5d5d',\n",
       "  'ce485c39-b2e4-4e96-b8bb-4a85dfd0bd59',\n",
       "  '1779d39d-7a58-418f-b4b4-311deef4b968',\n",
       "  '1ccc8528-8eb5-4ff3-adc7-fcccff9a73f6',\n",
       "  '6f2ea05c-8763-4d3f-a7d0-72a8986a1c99',\n",
       "  '72bffe39-b367-47b6-89d0-001b7fd3e437',\n",
       "  '0a8e1fec-0565-4a9d-856e-c751d5ed9442',\n",
       "  '6b04bc00-b95a-4bcd-8951-7e269f29d99e',\n",
       "  '2f28be0d-aba0-4e5b-8812-20324422a0cd',\n",
       "  'e777ebce-e6a1-448f-a798-247d2f5180d6',\n",
       "  'be778c3f-07f9-4e76-adb3-03aedfbae22d',\n",
       "  'b56a4ea2-9d34-4f3f-bbe0-c06359d71ed8',\n",
       "  'e28987b9-a8a7-4a88-9f8e-574b8815b199',\n",
       "  '11d94d99-4e54-486f-b683-963808f2879c',\n",
       "  'bde02d81-50f7-4bd9-83c5-c9b6f151bd6e',\n",
       "  '91e27381-9aa9-4fa2-b34c-79fd9f2537cc',\n",
       "  '2d45ac80-b32a-46e5-98ad-fe1e426490df',\n",
       "  '51f3bac0-6d6c-4d0d-bc97-4782a9d47c24',\n",
       "  'eb87dcef-0d4f-4850-972c-a09efb2b7d94',\n",
       "  'a67671bf-dfc3-460c-9317-7af51a65276d',\n",
       "  'ff577ce2-6fc7-4170-a029-6105cd0cb8da',\n",
       "  'a8b63836-b377-4f03-b6a6-a4d5cebd630b',\n",
       "  '7ad51e9f-2cc2-4bf6-b578-085ad0247a76',\n",
       "  '960f95e5-544a-476f-89d9-67de1ef65b34',\n",
       "  'd68206d0-7c81-412b-88a2-b5850f005337',\n",
       "  '87966ba3-e64d-4bf0-b87f-3b82a3150fcc',\n",
       "  'd94d4529-c710-4926-a8f7-afa5d46bf1b9',\n",
       "  '6df4d187-c571-4391-ac85-8026ea874b74',\n",
       "  '53d5725f-0a09-4709-9df4-170dfde8c6e9',\n",
       "  'aaa82fa2-fb10-4a39-990c-29792f54f879',\n",
       "  '2892ada1-3018-4978-8704-9bb907cf3901',\n",
       "  'b43491dd-cd4f-4910-bd0c-4a2a3dadebcd',\n",
       "  'aefcdaa1-6a41-4d22-b149-540597960803',\n",
       "  '913ad669-03f2-44bd-af01-63129bafbb9e',\n",
       "  'a519a908-3a6d-4ec2-a336-5459fe7cd6f5',\n",
       "  '4437c7be-6a43-4039-b315-e6067daff65e',\n",
       "  'a5be1afb-fded-474b-acde-29539a07f82e',\n",
       "  '068b3328-b150-4240-acc7-c1f7e1dbfbe7',\n",
       "  '8f8d8412-ab7d-4347-a4f9-5d8516289fe3',\n",
       "  '3743ac56-ea5e-48b3-a0f7-836bd3c26b5a',\n",
       "  '43b048ed-e346-4ef8-9ede-dae13aaf9083',\n",
       "  'e3c3a6f8-e0a2-4a84-aebd-61c8509671ec',\n",
       "  '2ff6bf86-89c8-4dc5-bc9b-f3bc69c45b4a',\n",
       "  'cdcf85fd-4b27-4b29-b63e-8d8d10fa4449',\n",
       "  'ee9411d9-fc35-49c9-ba7f-cef59a4e5fc1',\n",
       "  '0eb58db1-b8db-4d94-9a51-a8ff85c1a895',\n",
       "  '85106f5b-b5d5-40d8-86ff-ce61f89b6d73',\n",
       "  '832f1803-7375-4ea8-be34-a99be013a97e',\n",
       "  '9ffa95f1-7ba1-47cf-93c0-41bc06fbe583',\n",
       "  'ad259e75-5a0a-485a-b22e-592e0db958f5',\n",
       "  'c4682c47-129d-47a3-b462-6d44d84424ab',\n",
       "  '6bdf2c9f-fae3-4e6e-be65-4f0c8671e821',\n",
       "  'a38fd94a-8605-48cd-bad2-9da74972978f',\n",
       "  'db264d9c-1f6b-4690-ad59-0c3ea5aa95d2',\n",
       "  '23a6e53b-f8fe-4cad-be24-f083bac5404a',\n",
       "  'bed622e2-c2cd-4b23-af39-f583e519431b',\n",
       "  '8d5ad9ff-eab5-492d-af81-d263e1e4ab73',\n",
       "  'dae38463-6c9d-4d2c-afd8-a401b1e3d9ba',\n",
       "  '06f9f758-d643-40b8-98cf-fc0ec701ae03',\n",
       "  'a60ede13-1118-4d37-9df7-23df4acac372',\n",
       "  '4a128397-8379-4297-86cd-b36192537d32',\n",
       "  '906096d3-017d-4e85-bd04-9259d0cfcca6',\n",
       "  'c32b5819-0205-4a59-916c-dec5964f4e53',\n",
       "  '47b8a933-1abb-490b-85af-c0a70f76b0e2',\n",
       "  '4d80bc80-b952-49aa-92fb-db26bde3a957',\n",
       "  '319eefc3-ba2d-4c67-9c6e-d1e92244005d',\n",
       "  '28ce02d7-6dd1-41a0-84c2-97f730f010bb',\n",
       "  '646ee50f-6d19-41f4-b971-9e33bc8eb574',\n",
       "  '169e0e0a-e702-4e57-9628-c197f4191581',\n",
       "  '985d5d73-6808-43d6-9dd9-314e404d16b8',\n",
       "  'a2076943-76de-41a9-910c-382a999cee28',\n",
       "  '5c7d0f4d-6310-4c47-b027-37a4a153f9ae',\n",
       "  'e7eae2ce-fe7c-4fa0-af2d-be7399577851',\n",
       "  '7b110f91-4e46-48f7-932f-0244c991fb1a',\n",
       "  'b99ede63-e6a3-4d7a-af8d-2cdadfc8ac3b',\n",
       "  '175ed919-e041-4fa5-847b-8ad6ff6ae0ac',\n",
       "  '74ec3871-a1ef-4790-910a-785da3272451',\n",
       "  'ef53faf8-19fa-425f-9620-679ac5214b57',\n",
       "  'da20162d-0fbf-4120-94a5-d7e5035c6054',\n",
       "  'b090157f-93e5-4fab-be17-8d8ade5bdf6f',\n",
       "  '378f028f-784d-4c38-94ea-1795bdb1608e',\n",
       "  'bc92a091-710d-45d3-82ac-d1f32c1dcb01',\n",
       "  'afa203c4-3cf8-4d74-bcb8-d9415ff73d90',\n",
       "  'd1a1b622-483d-42d9-867c-7b4a9e112b13',\n",
       "  '8a5520d7-0b84-4797-8aa0-76356d404c2f',\n",
       "  '7a288e6f-a5a9-4c57-a1a3-5e6763cfb226',\n",
       "  '8d1813ec-f46d-4793-a200-ff3cc565c279',\n",
       "  '49c70b8a-b8bd-4d1d-ba77-0a7e4d13c29e',\n",
       "  '11a3b9bd-feca-4649-930c-80e142221896',\n",
       "  '387b715c-bc59-4453-a927-5929f0cea914',\n",
       "  '5e1e69e9-6f56-4517-9bac-e48650ac4394',\n",
       "  'c169c2d5-e2a0-4a9f-a246-13bd8eaec7d6',\n",
       "  'a6aa377b-f6e4-41e9-b609-1d062d972aa1',\n",
       "  'b75a1048-b549-4ba0-a0fa-f2a4bb1f7c72',\n",
       "  '30530102-2ac6-48ba-b5aa-9013ea7e564f',\n",
       "  'f47a44e5-7401-408b-976f-281139a2aa77',\n",
       "  '4e0d79f8-002f-411d-a927-6133cd854bae',\n",
       "  '6b680f37-d75e-4144-a658-a02f8d3b5b66',\n",
       "  'd9098f2f-9d46-4d85-b973-dcdd2bddcd4a',\n",
       "  'eba48526-8c2c-4219-a762-9c855273cc41',\n",
       "  '69257f43-acb8-4c71-a8e5-25bf58956b84',\n",
       "  'e3f5da90-e45f-4273-bfcd-92d6a4e721d4',\n",
       "  '689ef45b-5a13-4df1-9be3-a8335c15ec36',\n",
       "  'e9bf177c-99e6-447b-b67a-eee8a4a14574',\n",
       "  '29101ac1-643b-457f-a958-545fcde78eb1',\n",
       "  '07fcfe30-2ea3-487a-b03e-b399744f04fd',\n",
       "  '9043f3ef-f0af-4c4c-b45a-84c9be341d58',\n",
       "  'd4faef1d-e96a-4512-8922-e1c24379ebdc',\n",
       "  '326c5c18-c7a3-4ca9-a1db-d86e783f2326',\n",
       "  '5e62e4a4-d7b6-4499-a2f2-6048acde65a4',\n",
       "  '3e59d6c1-4b57-4f96-bf07-f0ce6a917c0c',\n",
       "  '0ee71867-5fd5-4e04-9d69-06f2ea181b2b',\n",
       "  '44283b18-06e5-4bb0-bedc-99a0afcef957',\n",
       "  'd2c8d8c5-8047-4560-bf2e-d6ae77fb9ab6',\n",
       "  '2d88e4f1-7509-4849-a7d0-46287ea8b767',\n",
       "  '4863273e-e34a-4719-ad46-782552f156eb',\n",
       "  'e55bc50b-a525-40fa-85e0-eb53666fcabc',\n",
       "  'ca68285d-46d6-473e-abc0-d6d2bacddffc',\n",
       "  '74184f54-7a2e-403c-8409-98069cdd58bb',\n",
       "  '66773b27-4c34-4280-ae8b-28e5e32d356c',\n",
       "  '7aa90629-2d60-46f5-9b3e-5bbc6191d944',\n",
       "  '815edb4a-dba3-4e74-a484-b5abac811c2f',\n",
       "  'a928a7a2-0b17-4a01-9942-db0c267ae85b',\n",
       "  'ea4a71c4-c3ae-434e-a533-08293b3d62ac',\n",
       "  '57a6765d-04fb-4079-998a-60f3bf77d9b2',\n",
       "  'f79d5f51-7877-44b6-a757-7be5ecb438b1',\n",
       "  'b32281a7-162f-4043-98a3-349bcd726b77',\n",
       "  'bf2384a8-003d-4aab-ac1c-b231a63f3567',\n",
       "  '1bee329e-69db-4ba0-bf8e-a92a45d0980a',\n",
       "  'f86b1ea4-6ca2-444b-8049-613a5b4003a9',\n",
       "  '24b612d2-3ae7-4a56-903f-2aa60a3cce43',\n",
       "  '79f6d52c-2746-43ac-a36d-9e92be7c558a',\n",
       "  '483a8c37-0ec1-449d-8f77-96435fbb2aa4',\n",
       "  '5ab52105-396e-4f12-8050-264cfc013fb7',\n",
       "  '3f293d7d-31ce-4dd7-9e2b-ffac01e1db91',\n",
       "  '785a2b47-dfac-40f9-9857-3b38da129386',\n",
       "  'cbaf3097-3b5d-4fe2-bd79-8fb2b11bc864',\n",
       "  '04434cc1-0609-4960-b80a-d9a152da6fa3',\n",
       "  '9abf9ef8-3a11-46c0-8dd3-89341bd340d1',\n",
       "  '227278da-f93a-44da-906a-f5af82eea28e',\n",
       "  '70b7bb65-a4e1-47dc-a44f-e57048e422e6',\n",
       "  '551b19af-019f-4d47-9b2f-2b193e541c10',\n",
       "  '9befaae3-20f3-467a-8123-16ec5e8d876f',\n",
       "  '999963b8-5829-4dee-b105-fd44862e8b54',\n",
       "  '6733dd75-3d8e-478f-b0e6-acabc9aeec0c',\n",
       "  '803b027d-7785-46ab-87e6-b0ed15aa5ea0',\n",
       "  '8db74884-3271-48cb-be45-2e144d61fcee',\n",
       "  'd696859c-6807-487b-ab55-b308f09be981',\n",
       "  'e4134dbd-f205-434e-baf6-8ff64bddd682',\n",
       "  'c8cc02ba-73dd-4c01-b593-8ff0883f13e7',\n",
       "  '9bfb4814-6f97-4ff4-bb16-5c494c1c995d',\n",
       "  '45d1d135-a888-44f9-9ea5-ddcfddb1944a',\n",
       "  'b984e20e-91e0-4e43-a676-ba2f76f2c74e',\n",
       "  'f0ef4537-ab52-4bd3-b533-9f7b29455f6a',\n",
       "  '7589a916-aca5-48f4-b8cc-d62eef0c4f7e',\n",
       "  'e4636073-063f-4264-b1a3-d15ed99b93d0',\n",
       "  'a223573c-9e92-4284-a163-a50859bee468',\n",
       "  'f21b2886-0a85-4124-8099-f88a226c644a',\n",
       "  '3ae1a790-a5d4-493e-8900-6cd83bc47f6e',\n",
       "  '9e96fa1a-3eca-4a00-b716-bf0fc05ddaa9',\n",
       "  '7f944ee3-5368-4e70-972b-d57dd68cd08a',\n",
       "  '3bb9816d-c3fb-4c03-82f8-9f1271ac496a',\n",
       "  'd57f3faf-be74-43cc-ad69-531706fc25de',\n",
       "  '0344ae92-8088-44a1-a5b2-9002da0f2221',\n",
       "  'c21e57bb-1665-4bf1-8d19-51ea12d3d161',\n",
       "  '4ef5c828-d394-4fbb-8e0b-20455995abe8',\n",
       "  '8cb0a946-f64f-43f2-93df-8c77616e9f92',\n",
       "  '09c87c81-1caf-4c3c-8ae1-460c87ad87ce',\n",
       "  'c11306f3-ea84-4521-bfad-583c9940e0f6',\n",
       "  '87c07d6f-a0bd-42f8-9cab-145ab108d538',\n",
       "  'ba90e708-c981-4d73-89e6-d795e7b76930',\n",
       "  '3e064326-442a-4977-be99-8a416b885d6a',\n",
       "  '1eb0dc83-2a92-4a39-8873-34ef4bb106ff',\n",
       "  '15675cd8-f716-478a-bac0-c01a92dee1d1',\n",
       "  '46e1c0a0-ad53-4330-b695-58b90be2ba49',\n",
       "  '0dc0d73a-ce37-4969-b3d2-1278d3a6dc05',\n",
       "  'ae581192-24cd-4915-ae8a-eb08fac44fd2',\n",
       "  'e9c90873-732b-4be5-ba52-d6dff83364d2',\n",
       "  '4c4e18d1-b114-43a2-a85f-a35370bf09e0',\n",
       "  '92c4f894-c075-42d5-b507-9ce7d13f34dc',\n",
       "  '9e9f9099-f060-4f19-8698-052bcc5ebc5c',\n",
       "  '788ddc5e-8f3d-4dc8-91a7-b9b42ffe1350',\n",
       "  '41237cd1-2801-4dea-aeb8-9c75b5c651b2',\n",
       "  '7248078c-7ca9-444e-bbaa-87fe3afc2767',\n",
       "  '21063e94-2665-43a9-ae28-7c769f88841b',\n",
       "  '6007b461-3c1a-465d-93da-d7202f230311',\n",
       "  '89830b7d-9b8a-4da5-9c6f-7d3df7e84cbb',\n",
       "  '2f61429d-efa8-45fe-9645-8f406c40853b',\n",
       "  '8be02866-4623-43a7-8f01-c01cb97c0eae',\n",
       "  'a4ac5a21-9d9b-436e-9a57-256401af21ac',\n",
       "  '0282c4da-d787-458b-9b37-853b05c5fbbe',\n",
       "  '67c7debe-231e-4b6e-a380-309da4184c14',\n",
       "  '9369de57-c9db-4700-846f-ce8f1aa5bbbb',\n",
       "  '06b4980b-c5da-460c-9f40-acd3c7ce9f25',\n",
       "  '3765ab74-2a57-4d11-83dd-c3a17168759d',\n",
       "  'fedbaf1c-1cba-4e09-84d2-efd0e16440cd',\n",
       "  '27c0e158-2bd7-4ddb-86fd-e3c01b78ff2f',\n",
       "  'e3086962-c6dd-4341-b21b-c9cbd099008f',\n",
       "  'd017ce76-3438-4e0b-b195-9f14373eb2ee',\n",
       "  '4e6b35c4-128c-412f-a3cc-67e255e8ac00',\n",
       "  '0e15de6e-102f-41f4-8be2-9bb3b22567f1',\n",
       "  '8e65beb0-9894-488d-910b-20187909d287',\n",
       "  'e80949fc-4ed3-4b87-8156-81c974bc5493',\n",
       "  'd2878e77-90d2-4369-9705-1d1e0b6b42ad',\n",
       "  '47f54da1-dbe0-4a22-9cef-e10f7668decd',\n",
       "  'c9a9cfb1-0855-4a17-b5f0-099a47541359',\n",
       "  '7d235484-c4c2-48df-8c88-6b63567b43c3',\n",
       "  '15268a1b-ce4a-4a3e-b17f-e4ed36f1bb99',\n",
       "  'd737fd08-edf2-4a38-bc25-285f6c85ffd6',\n",
       "  '59317b25-bdb8-44f9-87da-e2051aba3a02',\n",
       "  'a3f359fc-08dd-492a-bd39-f8b9a4435c1d',\n",
       "  '7f19ccf6-cb0c-4cac-97c7-13c5b405d8aa',\n",
       "  '747f5417-189f-403b-b44c-860af7273166',\n",
       "  '1e69222b-0b9a-4721-8eb5-2b655e4600f7',\n",
       "  'cc325b80-2eac-4d9e-ab2d-1abc58d99dc6',\n",
       "  '5bf9e61e-811d-4797-9112-0519dbd9aa4d',\n",
       "  'd5478914-c058-4cd9-98a9-cbe660e55fe5',\n",
       "  'e3f33645-ab3b-47f4-9b33-21d95fef81a6',\n",
       "  '7f2951ce-3f44-4946-9508-cd44a10605d4',\n",
       "  '6b330d36-7d0c-461b-a6b9-ceb2c7be9746',\n",
       "  'e704aa37-0c77-4b33-9e3f-d82da4344997',\n",
       "  '020f3f08-36dc-4326-8267-a55d5fd5d959',\n",
       "  '6d0c17f8-2ca9-4a82-bc5a-afa769fd9722',\n",
       "  '55d89055-f833-4c58-a928-5fd54f22d104',\n",
       "  'e0640023-48af-4f1e-a151-855cd161a89e',\n",
       "  'a17b0cc4-f2f2-4fd7-b21e-adc0a6d12983',\n",
       "  '5291d4f1-c782-4e89-992a-c83f8fbdd8b2',\n",
       "  '0a578d38-66b6-4be4-967c-52b3fa5ba645',\n",
       "  'f1ff525a-7129-40b1-a214-25e317ec0eec',\n",
       "  '4c5bbfec-3e75-49f0-b02e-b7e5f9b69b09',\n",
       "  'fe1bd239-b752-44a3-9caa-34a73c30a7ba',\n",
       "  '067433b5-32a2-4f59-b54e-75e1a855015c',\n",
       "  '09664d22-edee-4c99-b100-7d3de6212782',\n",
       "  '462f2ed0-7267-4637-a48f-be0b5325927f',\n",
       "  'fc04a5c4-427e-40e5-abe1-04a9220b1fd4',\n",
       "  '19b10c00-8603-4e83-b1a4-236f74029ac2',\n",
       "  '63d27c30-b65f-4a92-8c3d-4ffd8f97035a',\n",
       "  '25bb2fb7-59f7-4ad4-8cb6-dec963057a33',\n",
       "  'd0611486-fe4f-408c-9203-772a38b90d06',\n",
       "  '1cc408f4-c921-4688-9981-fe1745b1becd',\n",
       "  'a12804ce-bb1f-471d-9840-0be67bf63ad5',\n",
       "  'e7778946-e28c-4971-b3da-1783f36be5bd',\n",
       "  'b7f1111c-fa69-4339-9d1f-2beb56737367',\n",
       "  '0014dac2-ecda-45c9-85d8-5508479833b8',\n",
       "  '18d89a94-ecbf-4392-9665-d0cb54306422',\n",
       "  '46a99bc2-0411-4bcd-b4d7-214b288e8614',\n",
       "  '8ab99fc0-16be-482b-82ef-e662d7870886',\n",
       "  '2397b01d-336a-463e-9ad1-89fcfa8cc40b',\n",
       "  '3d6233ac-cb0c-4ef6-8865-a7e5019ae18c',\n",
       "  '194a846e-aa1b-4d38-af5a-f6da7751aa02',\n",
       "  '81bd5fa1-9d31-4198-8498-7be48a224e3b',\n",
       "  '1cb5f048-6ffd-4358-b3fa-3b05960fde3f',\n",
       "  '7fa780e4-4a84-496f-921d-b11e0fa23979',\n",
       "  'de8d9093-6d2f-4f39-a114-4f7dfb52d297',\n",
       "  'efa14f2e-bee2-4907-ab24-55268861b366',\n",
       "  'b96d8b5a-e8b9-4ca1-a460-23cd8644f2aa',\n",
       "  'f1dc94e2-4cd8-4d60-9f90-3318446ff9c7',\n",
       "  '3336d285-2c6d-4816-aad2-48c36a8727a0',\n",
       "  '8c782fba-d872-47a7-bb99-913372872d13',\n",
       "  '178c3bcf-7941-443d-9edc-59e87c892656',\n",
       "  '77a2570e-6cb0-4539-8cb8-6c76e9782d31',\n",
       "  'ba504503-e944-4b64-968f-f1b610cc0f5e',\n",
       "  'db85d335-86f3-4809-b7ee-71e87657ff2a',\n",
       "  '1db58f1b-02df-4d56-bb2c-e7cd8313241f',\n",
       "  'b5201ad6-c820-4df7-bcc2-0871bf5e5c62',\n",
       "  '53b652b4-f03f-4295-af2f-c63cc1953f4f',\n",
       "  'dc24998b-3afe-48e1-903d-9dca48a7c7ad',\n",
       "  'd91958fa-ee7c-42e7-8dc5-efffae2bdc34',\n",
       "  'd268310e-99c6-4985-aa23-15f149b6a2d2',\n",
       "  '60f13a3d-69ca-4d6b-b54c-50c85f004e48',\n",
       "  '242a37d6-a5d2-4b9a-b4f7-0313b94a439d',\n",
       "  '2132f000-4449-4e92-8ca3-360142023949',\n",
       "  '7c9654c7-1959-4828-b785-78cc17b7b02a',\n",
       "  'f56dd219-522f-46cf-b16d-27952cc96e71',\n",
       "  'f959d29c-7f0c-4ebe-843f-12e613ad5291',\n",
       "  '7ca3c705-18f6-4be3-a952-607e50e12e6b',\n",
       "  'df2dc35a-b1b3-491c-bf86-fed2239f2dd9',\n",
       "  '7fd1d79e-ff42-40bc-a5ab-e4a8cfcc80b8',\n",
       "  '0279d01a-8e60-4c49-b2d5-9eb5e0cad609',\n",
       "  '209e524a-5111-45a5-9be1-51b138d7d9f5',\n",
       "  '8bf7b2a6-2f9a-428b-8037-74917612ad21',\n",
       "  '11190fcf-e51e-4828-99a8-c9eb8c522b94',\n",
       "  'aed6d012-41d1-495c-8e0c-9bd44431e902',\n",
       "  'a07350b6-3ffe-4088-ba22-5ccb3dd0907d',\n",
       "  '7b3de1b5-4362-4919-9f67-591499887e71',\n",
       "  '4e9e87bf-8f66-4d9a-afb4-18f9813c8869',\n",
       "  '76bc7f63-0c80-4ac0-919d-60b8dd595a76',\n",
       "  '7fc6470b-d2fa-4f34-ae93-e531a0824fcc',\n",
       "  '54930f84-f89e-4247-b730-19ced27ae96a',\n",
       "  'dc89fa3f-184d-4a16-bfaa-8e3201c84488',\n",
       "  '1822c101-6f3e-4a12-9d02-aa989a00ae94',\n",
       "  '5dc92588-d149-4e1d-9307-202545c997bf',\n",
       "  'e5096ecb-4530-4759-9750-78d817054850',\n",
       "  '2fc8a1c6-ff1d-43c8-87b8-6a4056401bef',\n",
       "  '60811b0c-872b-4879-9c18-bd4ed82b578e',\n",
       "  'c03e04db-6b0d-48b1-a0bd-8256001f947d',\n",
       "  'c5252d60-5f56-40ee-b066-05978f2f4404',\n",
       "  'c8388258-59b0-4684-8411-787f5250db31',\n",
       "  '56f3c1df-e877-4a7d-a309-749f25f9ff41',\n",
       "  '75107f44-8c28-4c0f-90f4-c16f572def04',\n",
       "  '037cfe1e-c5d5-4d52-8965-51c7884b92de',\n",
       "  'a1990161-9076-441f-932e-f7d9b266fd5e',\n",
       "  '47f08c20-ebd9-45b0-a230-38a1643c898a',\n",
       "  '5a44ca38-1883-43a9-a478-2e2ea567fe43',\n",
       "  '4f81570e-f017-4eba-8d36-3cab4a45016d',\n",
       "  'bc5675e9-a0b5-4e42-a256-deed13d504b2',\n",
       "  '3c00b82d-da7e-4e44-8219-2ca26b1bf0f3',\n",
       "  'c0d1a8be-60ef-4f4d-918f-9ff7c810529e',\n",
       "  ...],\n",
       " 'embeddings': array([[-0.0426687 ,  0.04407129, -0.03201308, ...,  0.06367762,\n",
       "          0.03452409,  0.05045805],\n",
       "        [-0.08712859,  0.05777363, -0.0023609 , ..., -0.01045535,\n",
       "         -0.01182894,  0.00512709],\n",
       "        [-0.04591894,  0.03955158, -0.00020716, ...,  0.06436347,\n",
       "         -0.01298408,  0.01460039],\n",
       "        ...,\n",
       "        [-0.16577385, -0.00682028, -0.01017098, ..., -0.03867052,\n",
       "         -0.04633434,  0.01404728],\n",
       "        [-0.09438194,  0.04151512, -0.03241562, ...,  0.00618635,\n",
       "          0.00755152,  0.01942936],\n",
       "        [-0.07258344,  0.00387509,  0.04532986, ..., -0.00841682,\n",
       "         -0.0177092 ,  0.00345624]]),\n",
       " 'documents': ['Applied Artiﬁcial Intelligence\\nAn International Journal\\nISSN: 0883-9514 (Print) 1087-6545 (Online) Journal homepage: www.tandfonline.com/journals/uaai20\\nAI Ethics: Integrating Transparency, Fairness, and\\nPrivacy in AI Development\\nPetar Radanliev\\nTo cite this article: Petar Radanliev (2025) AI Ethics: Integrating Transparency, Fairness,\\nand Privacy in AI Development, Applied Artiﬁcial Intelligence, 39:1, 2463722, DOI:\\n10.1080/08839514.2025.2463722\\nTo link to this article:  https://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with\\nlicense by Taylor & Francis Group, LLC.',\n",
       "  'Published online: 07 Feb 2025. Submit your article to this journal \\nArticle views: 34975\\nView related articles \\nView Crossmark data\\nCiting articles: 52 View citing articles \\nFull Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=uaai20',\n",
       "  'AI Ethics: Integrating Transparency, Fairness, and Privacy in \\nAI Development\\nPetar Radanliev\\nDepartment of Computer Science, University of Oxford, Oxford, UK\\nABSTRACT\\nThe expansion of Artificial Intelligence in sectors such as health\\xad\\ncare, finance, and communication has raised critical ethical \\nconcerns surrounding transparency, fairness, and privacy. Addressing these issues is essential for the responsible devel\\xad\\nopment and deployment of AI systems.',\n",
       "  'This research estab\\xad\\nlishes a comprehensive ethical framework that mitigates \\nbiases and promotes accountability in AI technologies. A comparative analysis of international AI policy frameworks \\nfrom regions including the European Union, United States, and \\nChina is conducted using analytical tools such as Venn diagrams \\nand Cartesian graphs. These tools allow for a visual and sys\\xad\\ntematic evaluation of the ethical principles guiding AI develop\\xad\\nment across different jurisdictions. The results reveal significant \\nvariations in how global regions prioritize transparency, fairness, \\nand privacy, with challenges in creating a unified ethical stan\\xad\\ndard. To address these challenges, we propose technical strate\\xad\\ngies, including fairness-aware algorithms, routine audits, and \\nthe establishment of diverse development teams to ensure \\nethical AI practices. This paper provides actionable recommen\\xad\\ndations for integrating ethical oversight into the AI lifecycle, \\nadvocating for the creation of AI systems that are both techni\\xad\\ncally sophisticated and aligned with societal values. The findings \\nunderscore the necessity of global collaboration in fostering \\nethical AI development. ARTICLE HISTORY \\nReceived 11 August 2024  \\nRevised 5 September 2024  \\nAccepted 2 February 2025  \\nIntroduction\\nIn recent years, substantial advancements in AI ethics have emerged, with \\nsignificant contributions addressing transparency, fairness, and privacy in AI \\ndevelopment. Recent research studies (Bender et al.',\n",
       "  '2021) highlight the dan\\xad\\ngers of bias in large language models, raising concerns over the perpetuation of \\nsocietal inequalities within AI systems.',\n",
       "  'Similarly, Bommasani et al.',\n",
       "  '(2023) \\ncritically evaluate compliance of foundation models with the draft EU AI Act, \\nreflecting broader concerns over the accountability of AI systems at \\na foundational level. Moreover, Aldoseri, Al-Khalifa, and Hamouda (2023) \\nCONTACT Petar Radanliev \\npetar.radanliev@cs.ox.ac.uk \\nDepartment of Computer Science, University of \\nOxford, Oxford, UK\\nAPPLIED ARTIFICIAL INTELLIGENCE                    \\n2025, VOL.',\n",
       "  '39, NO.',\n",
       "  '1, e2463722 (41 pages) \\nhttps://doi.org/10.1080/08839514.2025.2463722\\n© 2025 The Author(s). Published with license by Taylor & Francis Group, LLC. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/ \\nlicenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly \\ncited. The terms on which this article has been published allow the posting of the Accepted Manuscript in a repository by the author(s) \\nor with their consent.',\n",
       "  'propose new data strategies for AI development, focusing on the integration of \\nethical principles across diverse datasets to mitigate bias. These works, along\\xad\\nside key regulatory frameworks from bodies such as the European Union \\n(European Parliament 2023) and NIST (2024b), reinforce the imperative of \\ndeveloping AI systems that are not only transparent and fair but also respect \\ndata privacy and societal norms. By situating this study within the context of \\nthese contemporary advancements, the aim is to build upon these discussions \\nby offering a comparative analysis of global AI policy frameworks, extending \\nthe dialog on how ethical AI can be systematically developed and maintained \\nacross diverse geopolitical contexts. While this study references key AI policy frameworks from the European \\nUnion, the United States, and China, the focus of this study is a more granular \\nexamination of the challenges in comparing such diverse frameworks is \\ncrucial. AI ethics policies are deeply influenced by cultural, socio-political, \\nand economic contexts, making cross-regional comparisons inherently com\\xad\\nplex.',\n",
       "  'For instance, the European Union’s AI Act places significant emphasis \\non safeguarding individual rights, prioritizing transparency and human over\\xad\\nsight (European Parliament 2023), reflecting the EU’s regulatory ethos aimed \\nat protecting citizens from the potential harms of AI. In contrast, the United \\nStates’ AI governance is more decentralized, with a focus on promoting \\ninnovation and maintaining global technological leadership, as seen in the \\nNational Institute of Standards and Technology (NIST) AI Risk Management \\nFramework (2023a), which advocates for flexible, non-prescriptive guidelines \\nthat encourage industry-led solutions. China’s AI policy framework, meanwhile, is characterized by its focus on \\nstate security, social harmony, and the integration of AI into national eco\\xad\\nnomic strategies (Roberts et al. 2021).',\n",
       "  'This framework aligns with China’s \\nbroader governmental control over technology, where the state plays a central \\nrole in guiding AI development. These diverging priorities highlight the \\ninherent challenges in creating universal standards for AI ethics. The task of \\ncomparing these frameworks, therefore, requires consideration of their dis\\xad\\ntinct legal, cultural, and economic motivations, as well as the varying levels of \\npublic trust in AI technologies across these regions. This study addresses these complexities by identifying common ethical \\nprinciples such as fairness, transparency, and privacy, and by analyzing how \\neach region interprets and prioritizes these principles.',\n",
       "  'By employing compara\\xad\\ntive tools such as Venn diagrams and Cartesian graphs, the article visually and \\nanalytically demonstrates the differences, but also the common points in these \\nframeworks. The aim of this study was not to look only for the differences, but \\nto find a solution for global AI governance and to promote the potential for \\nharmonization across jurisdictions. This paper explores the pressing need for ethical considerations in the \\nrapidly evolving domain of Artificial Intelligence (AI) (Meissner 2020). This \\ne2463722-2\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'technology has significantly impacted various sectors, including healthcare, \\nfinance, and communication. This study aims to establish a robust ethical \\nframework for AI development by addressing complex issues such as data \\nprivacy, algorithmic transparency, and fairness. Our objectives include analyz\\xad\\ning fundamental ethical principles, comparing international AI policy frame\\xad\\nworks (Helbing et al.',\n",
       "  '2018), proposing strategies for bias mitigation, and \\ncontributing to academic and practical discussions in AI ethics. This paper \\nis structured to systematically dissect these topics, providing an in-depth \\nexploration of AI ethics and its implications for future AI development and \\ngovernance (de Fine Licht and de Fine Licht 2020). The Imperative of Ethical Considerations in AI\\nThe advent of Artificial Intelligence (AI) has inaugurated a new epoch in \\ntechnological evolution, profoundly influencing diverse sectors, including \\nhealthcare, finance, transportation, and communication (Hosny et al.',\n",
       "  '2018; \\nNIST 2023b; Yu, Beam, and Kohane 2018). This unprecedented integration of \\nAI into the societal fabric necessitates the urgent formulation of robust ethical \\nframeworks.',\n",
       "  'These frameworks must address the complexities inherent in AI \\ntechnologies, such as data privacy, algorithmic opacity, equity in decision- \\nmaking, and broader societal impacts. Ethical considerations in AI transcend academic discourse, bearing signifi\\xad\\ncant real-world repercussions. Paramount among these are issues related to \\ndata privacy and the need for informed consent, where personal information \\noften powers AI algorithms.',\n",
       "  'Equally critical is the transparency and explic\\xad\\nability of these algorithms, which are essential for sustaining public trust, \\nespecially in high-stakes scenarios like legal adjudication or medical diagnos\\xad\\ntics. Moreover, the challenge of ensuring equity and circumventing ingrained \\nbiases in AI systems is a pivotal ethical imperative, given these systems’ \\npropensity to mirror and perpetuate existing societal disparities.',\n",
       "  'The need for ethical AI is driven by the imperatives of harm prevention and \\njustice but also by the strategic objective of nurturing sustainable, socially \\nbeneficial, and universally accepted innovation. Aims and Objectives of the Study\\nThe primary goals of this academic study are threefold.',\n",
       "  'First, it seeks to \\nexplore the ethical considerations inherent in the development of AI. This \\ninvolves thoroughly examining the fundamental ethical principles of transpar\\xad\\nency, equity, and privacy within AI systems and understanding how they relate \\nto each other and their significance in isolation. Second, the research aims to \\ncritically analyze various global AI policy frameworks, focusing on those from \\nthe EU, the US, and China. The goal is to discern their similarities and \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-3',\n",
       "  'differences and what they mean for international AI governance. Third, the \\npaper intends to provide a synthesis of approaches and practices to recognize \\nand mitigate bias in AI systems, ensuring their fairness and dependability. This \\nstudy aims to contribute to the ongoing academic and practitioner dialog on \\nAI ethics by offering relevant insights and recommendations to both groups. The overarching goal is to promote a more ethical and responsible path for AI \\ndevelopment. Structure and Content of the Paper\\nThe paper has been methodically structured to explore AI ethics across various \\ndimensions systematically.',\n",
       "  'Section 2, expands into the foundational ethical \\nprinciples in AI: transparency, equity, and privacy. The section uses a Venn \\ndiagram to demonstrate the interaction between these principles, emphasizing \\ntheir interconnectedness and how they relate to AI. Moving on to section 3, the focus is on integrating global AI policy frame\\xad\\nworks within AI development and deployment processes. The section presents \\na flowchart outlining the critical stages in AI projects and the influence of \\nvarious international frameworks. This section examines the role of policies in \\nensuring responsible AI development and the importance of incorporating \\ninternational frameworks to achieve this goal. Section 4, provides a comparative analysis of AI Ethics Policy Frameworks \\nfrom different nations, using a Cartesian graph for evaluation based on \\ntransparency, accountability, equity, and privacy. This seciton highlights the \\nvarying approaches different countries take to AI ethics policies and how they \\ncompare.',\n",
       "  'Section 5, proposes a range of strategies for addressing and reducing bias in \\nAI systems. It emphasizes the significance of data diversity, rigorous audits, \\nethical training, and algorithmic clarity in reducing bias in AI systems. Finally, the paper concludes with section 6, which summarizes the key \\nfindings, discusses their implications for the future trajectory of AI develop\\xad\\nment, and suggests avenues for further scholarly inquiry. The paper provides \\na comprehensive, multifaceted examination of AI ethics through this struc\\xad\\ntured approach, contributing substantive insights to the ongoing scholarly \\ndialog in this critically pivotal domain.',\n",
       "  'Ethical Considerations in AI Development\\nEthical considerations are of the utmost importance in the development of AI \\nto ensure that these systems are safe, fair, and transparent. A Venn diagram \\nillustrates the interplay between three key aspects: transparency, fairness, and \\nprivacy.',\n",
       "  'e2463722-4\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Transparency, Explainability, and Clarity refer to making AI systems clear \\nand understandable to users. Fairnessrequires that AI systems be designed \\nequitably and without biases.',\n",
       "  'Privacy, or “Data Protection and Consent,” \\nfocuses on protecting personal data and obtaining informed consent for its \\nusage.',\n",
       "  'These aspects are interconnected, and the intersections of the Venn diagram \\nillustrate how they overlap.',\n",
       "  'For example, the intersection of transparency and \\nfairness is called accountability, which emphasizes the importance of trans\\xad\\nparent and fair AI decision-making. The intersection of transparency and \\nprivacy, called user trust, emphasizes the need for transparency in data use \\nand protection (Aldoseri, Al-Khalifa, and Hamouda 2023; Bécue, Praça, and \\nGama 2021; Malhotra 2018; Mijwil, Aljanabi, and ChatGPT 2023). The inter\\xad\\nsection of fairness and privacy, known as nondiscriminatory data practices, \\nhighlights the need for privacy considerations to align with fairness to avoid \\ndiscrimination.',\n",
       "  'The center intersection of the Venn diagram represents the ideal of respon\\xad\\nsible AI use that balances transparency, fairness, and privacy. A flowchart \\nprovides a clear, step-by-step guide to embed ethical considerations into AI \\ndevelopment. The process starts with a commitment to ethical AI development and \\ndefining ethical principles such as transparency, fairness, and privacy. Data \\nuse and AI training guidelines should then be implemented to adhere to these \\nprinciples. Regular audits should be conducted to identify and correct biases \\nand ensure compliance with ethical standards. Clear lines of responsibility and accountability should be established in AI- \\ndriven decisions. Continuous improvement based on feedback from users and \\nstakeholders should be a regular practice. The goal is the realization of AI \\nsystems that fully embody ethical principles and ensure the safety and well- \\nbeing of all users.',\n",
       "  'The framework from Figure 1 is discussed in more detail in the next section. Transparency, Explainability, and Clarity\\nIn developing artificial intelligence, it is essential to prioritize transparency, \\nexplainability, and clarity to ensure ethical development and deployment.',\n",
       "  'Transparency refers to the accessibility of AI systems and their workings to \\nusers and stakeholders. Explainability, closely linked to transparency, pertains \\nto the ability of AI systems to be understood and interpreted by human beings, \\nideally in non-technical language. Meanwhile, clarity ensures that AI systems’ \\npurposes and outcomes are communicated in a straightforward and under\\xad\\nstandable manner.',\n",
       "  'The importance of these elements cannot be overstated. They are crucial in \\nbuilding and maintaining user trust, ensuring that AI systems operate \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-5',\n",
       "  'Figure 1. Transparency, Explainability, and Clarity: A framework for developing AI systems that \\nembody ethical principles and ensure the safety and well-being of all users. e2463722-6\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'understandably and predictably. Furthermore, transparency and explainability \\nplay a pivotal role in establishing accountability, ensuring that AI developers \\nand users are held responsible for the outcomes of AI systems (European \\nParliament 2023; ISO 2023; McCorduck and Cfe 2004; MeitY 2023; NIST  \\n2023b; Office for Artificial Intelligence 2023). Fairness and Bias Prevention\\nEnsuring fairness in AI systems requires creating programs that make deci\\xad\\nsions without prejudice or partiality (Bender et al.',\n",
       "  '2021). This necessitates \\na conscientious effort to design AI systems that do not perpetuate existing \\nbiases or create new ones (Shu, Zhang, and Yu 2021). However, achieving \\nfairness in AI poses significant challenges, as these systems often learn from \\nreal-world data, which can be inherently biased. The intersection of fairness, privacy, and accountability is a complex but \\nessential consideration. Ensuring fairness often involves careful handling of \\nsensitive data while also maintaining transparency and accountability in \\ndecision-making processes. This balancing act is critical in mitigating biases \\nand ensuring that AI systems are equitable and just. Privacy and Data Protection\\nPrivacy and data protection are critical ethical considerations that must be \\nconsidered during AI development. It involves protecting personal and sensi\\xad\\ntive information from unauthorized access and ensuring that data is used \\nresponsibly. Regulations and standards, such as the General Data Protection \\nRegulation (GDPR) (GDPR 2018; ICO 2018) in the European Union, play \\na significant role in shaping AI ethics by setting strict guidelines for data use.',\n",
       "  'Privacy, fairness, and user trust are closely linked.',\n",
       "  'Protecting privacy is crucial \\nin building and maintaining user trust, which is essential for the acceptance \\nand success of AI systems.',\n",
       "  'Furthermore, ensuring the proper handling of data \\nis vital for fairness, as data misuse can lead to biased outcomes. Interconnectedness of Ethical Aspects\\nThe ethical dimensions of AI, including transparency, fairness, and privacy, \\nare not isolated but deeply interconnected (Partnership on AI 2023; Roberts \\net al. 2021).',\n",
       "  'We can visualize this interconnectedness using a Venn diagram \\nthat shows how these aspects overlap and influence each other. For instance, \\nwhen transparency and fairness intersect, it leads to accountability.',\n",
       "  'Similarly, \\nwhen fairness and privacy overlap, it underscores the need for nondiscrimi\\xad\\nnatory data practices. The idea of responsible AI use is represented by the \\ncentral intersection of these aspects in the Venn diagram. This is where all \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-7',\n",
       "  'three principles are balanced, leading to AI systems that are ethical, reliable, \\nand trustworthy. We can visualize this in Figure 2. Implementation Strategies\\nIncorporating ethical considerations into the development of AI requires \\na systematic approach. A step-by-step guide for doing this involves first \\ncommitting to ethical principles. Next, data use and AI training guidelines \\nshould be implemented to align with these principles. Regular audits are \\nnecessary to detect and correct biases to ensure compliance with ethical \\nstandards. Establishing clear lines of responsibility and accountability in AI-driven \\ndecisions is also crucial. Continuous improvement, based on feedback from \\nFigure 2. Interconnected concepts of the Framework for developing AI systems that embody \\nTransparency, Explainability and Clarity.',\n",
       "  'e2463722-8\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'users and stakeholders, should be an integral part of AI development. The \\nultimate goal is to create AI systems that embody ethical principles, ensuring \\nall users’ safety and well-being. Responsible AI Frameworks\\nThis section will explore AI frameworks from the European Union, the \\nUnited States, and China. Using a detailed flowchart, we will examine how \\nthese frameworks impact each stage of an AI project, from initiation to \\npost-deployment. Additionally, we will use a Venn diagram analysis to \\ncompare the EU AI Act (Bommasani et al. 2023), US AI Principles \\n(Tabassi 2023), and China AI Ethics guidelines (Roberts et al.',\n",
       "  '2021), high\\xad\\nlighting their unique features and areas of overlap. This approach will \\ndemonstrate how these frameworks share a commitment to ethical stan\\xad\\ndards, privacy protection, and fairness while also providing distinct per\\xad\\nspectives on AI development and governance. This analysis is crucial for \\nunderstanding the multifaceted nature of global AI frameworks and their \\nimplications for responsible AI practices.',\n",
       "  'While this paper primarily focuses on theoretical frameworks and policy \\nanalysis, it is important to acknowledge the growing need for empirical \\nresearch to substantiate the claims made within the scope of AI ethics. In \\nresponse, we introduce two key case studies that provide concrete examples of \\nhow AI ethics frameworks are applied in practice. These case studies were \\nderived from in-depth analyses of recent AI implementations in both the \\nFigure 3. Key elements found in global AI frameworks.',\n",
       "  'APPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-9',\n",
       "  'healthcare and financial sectors, which serve as critical areas where ethical \\nconsiderations are paramount. The first case study examines the deployment of AI diagnostic systems in \\nthe European healthcare industry, particularly focusing on IBM Watson \\nHealth’s use of AI in oncology diagnostics. By conducting structured inter\\xad\\nviews with healthcare practitioners and reviewing compliance reports, we \\nobserved how the stringent transparency and accountability requirements \\nmandated by the European Union’s AI Act (European Parliament 2023) \\ninfluenced the AI system’s design and operational transparency. This empiri\\xad\\ncal evidence demonstrates how AI systems were modified to meet regulatory \\nstandards, particularly concerning the explainability of diagnostic recommen\\xad\\ndations provided to medical professionals and patients. The second case study involves an empirical assessment of AI fraud \\ndetection systems in the financial services sector within the United States.',\n",
       "  'Through direct engagement with industry experts and an analysis of \\nFigure 4. Applied design of responsible and ethical AI practices by integrating global AI frame\\xad\\nworks into different stages of AI development and deployment.',\n",
       "  'e2463722-10\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'internal auditing processes, we explored how JP Morgan’s AI-powered \\nfraud detection system aligns with the flexible, innovation-driven guidelines \\nof the NIST AI Risk Management Framework (NIST 2023a; Tabassi 2023). The study reveals how these frameworks permit adaptive risk management \\nstrategies, providing companies with the autonomy to tailor their ethical \\nstandards while maintaining a balance between innovation and \\naccountability.',\n",
       "  'These case studies provide empirical evidence to support the theoretical and \\npolicy analysis discussed in this paper. They illustrate how global AI ethics \\nframeworks are not just abstract concepts but operational guidelines that have \\ntangible impacts on AI design, deployment, and compliance. By incorporating \\nthese real-world applications, we aim to bridge the gap between theory and \\npractice, offering a more robust foundation for understanding the dynamics of \\nAI governance across various industries. Figure 5.',\n",
       "  'Different global AI frameworks overlap in some areas but maintain unique characteristics \\nin others. APPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-11',\n",
       "  'Development and Deployment Flowchart\\nCreating and implementing AI systems is a complex process that requires \\ncompliance with international frameworks to ensure ethical and responsible \\noutcomes. This section provides a comprehensive flowchart that integrates AI \\nframeworks from the European Union, the United States, and China, mapping \\ntheir application throughout the AI project lifecycle. The flowchart is based on the EU AI Act (European Parliament 2023), US \\nAI Principles (NIST 2024c), and China AI Ethics guidelines (Provisions on the \\nFigure 6. The complexity of AI frameworks across different regions and the potential for collabora\\xad\\ntion and divergence highlights the need for global collaboration and harmonisation of AI ethics \\nand regulations.',\n",
       "  'e2463722-12\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Administration of Deep Synthesis Internet Information Services, Personal \\nInformation Protection Law of the People’s Republic of China PRC 2022). It \\nbegins with initiating an AI project, where objectives are defined and relevant \\ndata is collected.',\n",
       "  'The EU AI Act’s guidelines on ethical data use and transpar\\xad\\nency are essential at this stage. The US AI Guidelines come into play as the \\nproject progresses to data processing and AI model development, emphasizing \\ninnovation, fairness, and accountability. During the validation and testing phase, the model must align with the set \\nobjectives and comply with ethical standards per these frameworks. The \\ndeployment phase sees the integration of China’s AI Ethics guidelines, which \\nprioritize social harmony, national security, and global cooperation. After \\ndeployment, continuous monitoring and maintenance are essential to ensure \\nthe AI system functions as intended and adheres to ethical standards.',\n",
       "  'This \\nphase is critical for incorporating feedback and insights, allowing for iterative \\nimprovements based on real-world performance and impact. Figure 3 illustrates how global AI frameworks are necessary and applicable \\nat different AI development and deployment stages. This integration ensures \\na holistic approach to responsible AI practices that align with global standards \\nand ethical considerations.',\n",
       "  'Figure 3 shows the steps involved in developing and deploying AI systems. It incorporates the latest AI frameworks worldwide, including those from the \\nEU, the US (NAIAC 2024, NIST 2024a), and China (Interim Measures for the \\nManagement of Generative Artificial Intelligence Services, Personal \\nInformation Protection Law of the People’s Republic of China PRC 2023; Li  \\n2017; Provisions on the Administration of Deep Synthesis Internet \\nInformation Services, Personal Information Protection Law of the People’s \\nRepublic of China PRC 2022; The State Council People Republic of China  \\n2017), and highlights critical points where these frameworks intersect. The \\nprocess flow begins with the start of the AI project and moves on to identifying \\nobjectives and collecting relevant data. The EU AI Act’s guidelines may be \\nconsidered at this stage. The collected data is then processed, and the AI model \\nis developed according to the principles outlined in the US AI Guidelines.',\n",
       "  'The \\nmodel is then validated and tested to meet the set objectives. Deployment of \\nthe AI system in a real-world environment is the next step, and considerations \\nfrom China’s AI Ethics guidelines come into play here.',\n",
       "  'Continuously mon\\xad\\nitoring and maintaining the AI system post-deployment is essential.',\n",
       "  'The \\nflowchart also includes a feedback loop that involves revisiting objectives \\nand processes based on feedback and new insights. Once all the steps are \\ncompleted, the AI project cycle ends. The flowchart in Figure 4 ensures \\nresponsible and ethical AI practices by integrating global AI frameworks \\ninto different stages of AI development and deployment. Figure 4 visualizes how various AI frameworks integrate with the AI devel\\xad\\nopment and deployment process. Each stage of the AI process is marked, from \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-13',\n",
       "  'the beginning to the end. The EU, the US, and China AI frameworks are \\nhighlighted with individual annotations. The arrows linking these frameworks \\nto specific stages in the process are designed to avoid text overlap, ensuring \\nclarity and readability. The flowchart effectively illustrates the integration of global AI frame\\xad\\nworks into the AI development lifecycle, emphasizing the importance of \\nconsidering these guidelines at different stages for responsible AI practices. The annotations for the AI frameworks from the EU, US, and China are \\nstrategically positioned to avoid obstructing any other flowchart elements. The connecting arrows are designed with a specific arc to neatly link the \\nframeworks to their respective stages in the AI process without crossing \\nover any text. The Venn diagram represents the AI frameworks from the EU, the US, and \\nChina. Each circle in the diagram represents a different region’s AI framework: \\nEU AI Act, US AI Principles, and China AI Ethics. The overlaps between the \\ncircles indicate areas of common focus or principles shared between these \\nframeworks.',\n",
       "  'Individual sections highlight unique aspects of each framework. This Venn diagram in Figure 5 visually demonstrates how different global \\nAI frameworks overlap in some areas while maintaining unique characteristics \\nin others. It reflects the diverse approaches to AI governance and ethics across \\nthese regions. The Venn diagram in Figure 5 comprehensively analyses the AI frameworks \\nfrom the European Union, the United States, and China. It highlights the areas \\nof collaboration and divergence between the three regions and demonstrates \\nthe complexity of AI frameworks across different regions. The European Union’s AI Act focuses on human oversight, nondiscrimina\\xad\\ntion, and regulatory compliance.',\n",
       "  'The US AI Principles emphasize innovation \\nencouragement, public trust, and open collaboration. China’s AI Ethics prior\\xad\\nitizes social harmony, national security, and global cooperation.',\n",
       "  'These unique \\nelements reflect the individual priorities and cultural perspectives of each \\nregion. The EU and the US share values in transparency and ethical standards. The \\nEU and China both emphasize privacy protection and ethical standards. The \\nUS and China find common ground in ethical standards and a focus on \\nfairness. These common areas between the two frameworks indicate the \\npotential for global collaboration and harmonization of AI ethics and \\nregulations. The central overlap in the Venn diagram highlights the areas where the EU, \\nUS, and China share common principles such as ethical standards and privacy \\nprotection. These areas offer opportunities for global collaboration and har\\xad\\nmonization of AI ethics and regulations.',\n",
       "  'Each region has unique focus areas that reflect its priorities and cultural \\nperspectives. These divergent approaches could lead to different approaches in \\ne2463722-14\\nP. RADANLIEV',\n",
       "  'AI development and governance. The Venn diagram demonstrates the poten\\xad\\ntial for collaboration and divergence in the global AI landscape. The Venn diagram analysis in Figure 6 shows the complexity of AI frame\\xad\\nworks across different regions and the potential for collaboration and diver\\xad\\ngence. It highlights the need for global collaboration and harmonization of AI \\nethics and regulations to ensure that AI development and governance align \\nwith ethical and societal values. The Venn diagram in Figure 6 shows the EU AI Act i in light blue, the US AI \\nPrinciples are in light green, and the China AI Ethics framework in coral. Real-World Applications of Global AI Frameworks\\nIn order to provide a clearer understanding of how AI frameworks function in \\npractice, it is essential to examine real-world applications of these frameworks \\nacross different regions. A relevant example can be found in the application of \\nthe European Union’s AI Act within the healthcare sector.',\n",
       "  'The use of AI- \\npowered diagnostic tools, such as IBM’s Watson Health, was subject to \\nscrutiny under the EU’s stringent regulations on transparency and explain\\xad\\nability. Under the AI Act, companies deploying such AI systems in high-risk \\nsectors are required to provide detailed documentation of the algorithms used, \\nas well as explainability mechanisms that allow medical professionals and \\npatients to understand AI-driven decisions.',\n",
       "  'This regulatory requirement has \\nled to the modification of AI models to ensure compliance, particularly by \\nproviding more transparent decision-making processes that can be audited by \\nhealthcare regulators. In contrast, the United States’ more innovation-centric approach, as embo\\xad\\ndied by the NIST AI Risk Management Framework, can be observed in the \\ndeployment of AI in the financial services sector. For instance, JP Morgan’s \\nAI-powered fraud detection system operates within a framework that empha\\xad\\nsizes risk mitigation through best practices and industry standards rather than \\nrigid regulatory oversight. The NIST framework encourages companies to \\ndevelop internal policies tailored to their operational risks, allowing for greater \\nflexibility in AI implementation. As a result, JP Morgan has developed pro\\xad\\nprietary methods for continuous monitoring and auditing of AI models to \\nensure they remain effective while balancing the need for innovation with \\nethical considerations.',\n",
       "  'China’s AI governance, which prioritizes state control and societal \\nharmony, can be seen in the government’s use of facial recognition \\nsystems for public security.',\n",
       "  'The deployment of such systems, governed \\nby China’s Provisions on the Administration of Deep Synthesis Internet \\nInformation Services, Personal Information Protection Law of the \\nPeople’s Republic of China (PRC) (2022), illustrates how the state \\nleverages AI under a framework that prioritizes national security. In this \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-15',\n",
       "  'case, AI technologies are used to monitor public spaces, but their ethical \\nimplications, particularly in terms of privacy, are handled within \\na governance model that differs significantly from those in Western \\ndemocracies. The Chinese government’s emphasis on AI as a tool for \\nsocietal stability underscores the unique application of their framework in \\npractice. These examples highlight the varied approaches of AI frameworks across \\ndifferent regions and sectors, demonstrating how global AI policies are shaped \\nby contextual factors and applied in practical, high-impact scenarios. By \\nexamining such real-world cases, we can better understand the strengths and \\nlimitations of these frameworks and the challenges in harmonizing AI ethics \\non a global scale. Specific Recommendations for Real-World Applications of Global AI Frameworks \\nInclude\\nTechnical Solutions for Embedding Ethical Principles in AI Systems. Embedding \\nethical principles such as fairness, transparency, and accountability into AI \\nsystems requires sophisticated algorithmic approaches that ensure these objec\\xad\\ntives are met without compromising the system’s performance. Algorithmic \\nfairness can be addressed using methods such as differential fairness and fair \\nrepresentation learning. For instance, algorithms like the Fair Representation \\nLearning (FRL) model aim to mitigate bias by transforming raw data into \\na latent representation that is invariant to sensitive attributes, such as race or \\ngender, without losing important predictive power. The FRL method applies \\nadversarial learning to ensure the model cannot easily infer sensitive attri\\xad\\nbutes, thus reducing bias while maintaining accuracy. This can be particularly \\nuseful in sectors like finance, where historical biases in credit scoring datasets \\noften lead to unfair outcomes.',\n",
       "  'Incorporating these fairness constraints during \\nthe model training phase ensures that discriminatory patterns in the data are \\nnot propagated by the AI system. Transparency is enhanced through the use of explainable AI (XAI) tech\\xad\\nniques. One common approach is the implementation of Local Interpretable \\nModel-agnostic Explanations (LIME), which provides users with interpreta\\xad\\nble approximations of complex models, enabling end-users and auditors to \\nunderstand and evaluate individual predictions.',\n",
       "  'LIME works by perturbing \\ninput data and observing how changes impact predictions, thus constructing \\nsimpler, interpretable models locally around specific instances. This method is \\nparticularly valuable in high-stakes fields like healthcare, where understanding \\nthe rationale behind AI-driven diagnoses is critical for building trust and \\naccountability. For example, LIME has been effectively applied in medical \\nimaging to explain how AI systems identify tumor regions, offering transpar\\xad\\nency to both clinicians and patients.',\n",
       "  'e2463722-16\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Obtaining Real-World Probabilistic Data for Legislation.',\n",
       "  'To create more robust \\nAI legislation that addresses real-world challenges, probabilistic data collec\\xad\\ntion is necessary. A critical solution lies in data-driven simulations that use \\nreal-world probabilistic distributions of AI outcomes across various domains. These simulations can leverage Bayesian inference models to analyze the \\nprobability of ethical failures, such as biased decisions or transparency \\nbreaches, under different regulatory scenarios. For example, Bayesian models \\ncan assess the likelihood of biased outputs in loan approval systems based on \\nvarying regulatory constraints, allowing policymakers to quantitatively evalu\\xad\\nate the trade-offs between stringent regulation and innovation. By incorporat\\xad\\ning such probabilistic assessments, policymakers can develop legislation \\ngrounded in empirical evidence, ensuring that ethical guidelines are both \\npractical and enforceable in diverse sectors. Moreover, quantitative data collection from real-world AI deployments \\ncould utilize techniques such as differential privacy to protect sensitive \\ninformation while still gathering meaningful insights. For instance, in health\\xad\\ncare, collecting large-scale patient data from AI diagnostic tools while main\\xad\\ntaining patient privacy can be achieved through differential privacy algorithms \\nthat introduce noise into datasets, ensuring that individual records cannot be \\nre-identified. This allows regulators to gather accurate statistics on AI system \\nperformance, such as prediction accuracy and error rates, without violating \\nprivacy laws.',\n",
       "  'These real-world data points can then be used to fine-tune \\nlegislative frameworks to ensure they are reflective of practical AI use and \\ncompliant with privacy standards. Algorithmic Solutions to Ensure Fair and Ethical AI for End-Users.',\n",
       "  'To ensure \\nAI systems are perceived as fair and ethical by end-users, several algorithmic \\napproaches can be integrated into the development lifecycle. One promising \\nmethod is the use of fairness constraints in model optimization, such as \\nEqualised Odds and Demographic Parity. The Equalised Odds algorithm \\nensures that an AI system has equal true positive and false positive rates across \\ndifferent demographic groups, ensuring that no group disproportionately \\nbenefits or suffers from the system’s decisions. This technique has been \\nsuccessfully implemented in judicial systems where AI models are used for \\nbail and sentencing recommendations, reducing the racial disparities com\\xad\\nmonly observed in earlier models. Fairness-aware learning algorithms can also be embedded into machine \\nlearning pipelines to monitor and adjust for bias during the training process. For example, the Fairness through Awareness (FTA) framework adjusts \\ndecision boundaries within models to ensure that similar individuals are \\ntreated similarly, thereby reducing unfair bias.',\n",
       "  'This algorithm calculates dis\\xad\\ntances in a fairness-sensitive space and ensures that individuals who are close \\nin this space receive similar predictions. This has been applied in hiring \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-17',\n",
       "  'algorithms to ensure that applicants with similar qualifications, regardless of \\ndemographic attributes, are treated equitably. Furthermore, end-user engagement with AI systems can be improved \\nthrough interactive transparency mechanisms.',\n",
       "  'For instance, counterfactual \\nexplanations can be used to provide users with actionable insights into how \\ndecisions could change if certain inputs were modified. In credit scoring \\nsystems, for example, a counterfactual explanation might inform a user that \\ntheir loan was denied due to a low credit score and suggest specific steps, such \\nas reducing credit card debt, that would lead to approval. By providing users \\nwith clear, actionable insights, these systems not only increase trust but also \\nempower users to engage more meaningfully with AI-driven decisions. Algorithmic Accountability and Continuous Monitoring.',\n",
       "  'To maintain ongoing \\nfairness and ethical standards, continuous monitoring of AI systems is essen\\xad\\ntial. This can be achieved through algorithmic auditing frameworks that \\nregularly assess AI systems for adherence to ethical principles post- \\ndeployment. Post-hoc fairness auditing tools, such as AI Fairness 360 \\n(AIF360), provide an open-source toolkit that measures and mitigates bias \\nin deployed models. These tools can be integrated into AI governance pro\\xad\\ncesses, ensuring that models remain fair and unbiased as they encounter new \\ndata in real-world environments. AIF360 evaluates fairness through multiple \\nmetrics, such as disparate impact and statistical parity, and enables continuous \\nrecalibration of models to maintain ethical performance.',\n",
       "  'Incorporating algorithmic accountability systems with real-time feedback \\nloops ensures that biases introduced by shifts in data distributions (data drift) \\nare swiftly detected and mitigated. Techniques such as drift detection algo\\xad\\nrithms, including ADWIN (Adaptive Windowing), continuously monitor the \\nperformance of AI models and trigger retraining when significant deviations \\nfrom expected behavior are detected.',\n",
       "  'By automating the detection of ethical \\nbreaches and recalibrating models in response, these systems ensure that AI \\nremains both effective and ethically compliant over time. Comparative Venn Diagram Analysis\\nThis section uses a Venn diagram analysis to compare the AI frameworks of \\nthe European Union (EU), the United States (US), and China. The diagram \\nrepresents each framework, highlighting their unique features and areas of \\noverlap, revealing both collaborative potentials and divergent approaches. The EU’s AI Act prioritizes human oversight, nondiscrimination, and strict \\nregulatory compliance.',\n",
       "  'It reflects the EU’s emphasis on protecting citizens’ \\nrights in the digital age. The US AI Principles prioritize fostering innovation, \\nensuring public trust, and promoting open collaboration.',\n",
       "  'This mirrors the \\nUS’s emphasis on market-driven and innovation-led AI development. On the \\ne2463722-18\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'other hand, China’s AI Ethics framework emphasizes the importance of social \\nharmony, national security, and global cooperation. It reflects China’s \\napproach to balancing technological advancement with social stability and \\nstate security. The intersections of these frameworks in the Venn diagram highlight \\nshared principles and potential areas for international cooperation.',\n",
       "  'For exam\\xad\\nple, the EU and the US emphasize ethical standards and transparency. The EU \\nand China share a common focus on privacy protection and the ethical use of \\nAI. The US and China converge on encouraging ethical standards and fairness \\nin AI. At the central intersection of the Venn diagram, where all three frame\\xad\\nworks overlap, lies a shared commitment to ethical standards, privacy \\nprotection, and ensuring fairness. This common ground suggests opportu\\xad\\nnities for global collaboration and the harmonization of AI ethics and \\nregulations. However, each framework’s divergent aspects reflect each region’s \\nvarying priorities and cultural perspectives.',\n",
       "  'These differences could \\nlead to distinct approaches in AI development and governance globally. Therefore, the Venn diagram highlights the potential for collaboration \\nand underscores the need to understand and respect diverse perspectives \\nin the global AI landscape. Policy Frameworks\\nThis section expands into a thorough analysis of AI Ethics Policy \\nFrameworks worldwide.',\n",
       "  'It covers significant regions such as the European \\nUnion, the United States, China, Canada, Japan, India, and Australia. Figure 7 presents the data in Cartesian graphs, comparing these frame\\xad\\nworks across four key ethical dimensions: Transparency, Accountability, \\nFairness, and Privacy.',\n",
       "  'Each framework is evaluated in detail and rated, \\nproviding an insightful understanding of how countries prioritize these \\ndimensions in their AI policies. This graph highlights each framework’s \\nunique priorities and focus areas and emphasizes the diversity and com\\xad\\nmonalities in global approaches to AI ethics. Policymakers can leverage \\nthese insights to identify areas for improvement and develop comprehen\\xad\\nsive, ethically aligned AI policies. Moreover, the section expands upon this \\nanalysis to include additional critical dimensions such as human oversight \\nand national security, broadening the scope to encompass broader socio- \\npolitical implications of AI technology. Figure 8 analyses global trends and \\npresents a roadmap for harmonization in AI ethics, advocating for ongoing \\ninternational dialogue and cooperation to foster a responsible and ethical \\nglobal AI ecosystem. APPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-19',\n",
       "  'Criteria for Rating AI Ethics Policy Frameworks Across Countries\\nThe ratings for each country’s AI ethics policy framework were based on \\na detailed analysis of public documents, policy white papers, regulatory guide\\xad\\nlines, and academic literature. The scores for each dimension, transparency, \\naccountability, fairness, privacy, human oversight, ethical standards, \\nFigure 7. Cartesian graph - Comparison of Al Ethics Policy Frameworks Across Countries. Figure 8. Cartesian graph of Global AI Ethics Policy Frameworks: a tool for policymakers to \\nunderstand the priorities of different countries regarding AI ethics. e2463722-20\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'innovation encouragement, and national security, were evaluated according to \\nthe following specific criteria:\\nTransparency (1–10 Scale)\\nTransparency was assessed by the extent to which each country’s framework \\nmandates openness regarding the design, implementation, and decision- \\nmaking processes of AI systems. ●9-10: Countries with explicit, legally enforced transparency requirements \\nfor AI systems, including mandates for explainability and public account\\xad\\nability (e.g., the European Union).',\n",
       "  '●6-8: Countries that encourage transparency but do not mandate it as \\na legal requirement across all AI applications (e.g., the United States). ●4-5: Countries where transparency is mentioned in policies but with few \\npractical enforcement mechanisms (e.g., China). ●1-3: Minimal or no formal focus on transparency in the AI framework. Accountability (1–10 Scale)\\nAccountability measures the robustness of legal and regulatory mechanisms \\nthat hold developers, companies, and governments responsible for the out\\xad\\ncomes of AI systems. ●9-10: Countries with well-defined liability frameworks that assign clear \\nresponsibility to AI developers or operators (e.g., the EU AI Act).',\n",
       "  '●6-8: Countries where accountability is encouraged through voluntary \\ncompliance frameworks but lacks mandatory enforcement (e.g., the US \\nwith the NIST AI Risk Management Framework). ●4-5: Countries with vague accountability measures, often handled at the \\ndiscretion of private entities or lacking centralized regulation (e.g., Japan).',\n",
       "  '●1-3: Countries where accountability frameworks are non-existent or still \\nin early development phases. Fairness (1–10 Scale)\\nFairness was evaluated based on how well a country’s policy framework \\naddresses bias in AI algorithms and ensures equitable outcomes across demo\\xad\\ngraphic groups. ●9-10: Countries with explicit fairness requirements for AI systems, man\\xad\\ndating fairness audits and bias mitigation techniques (e.g., Canada, EU).',\n",
       "  '●6-8: Countries that encourage fairness but with less stringent or optional \\nauditing practices (e.g., the US).',\n",
       "  '●4-5: Countries where fairness is an aspirational goal with limited practical \\nimplementation (e.g., India). APPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-21',\n",
       "  '●1-3: Minimal or no focus on fairness in AI regulation. Privacy (1–10 Scale)\\nPrivacy was assessed by how each country’s framework protects user \\ndata in the context of AI and how it aligns with global standards like \\nthe GDPR. ●9-10: Countries with robust privacy regulations, including explicit rules \\non AI data use (e.g., EU, Canada).',\n",
       "  '●6-8: Countries with general data privacy laws but limited AI-specific \\nguidelines (e.g., the US, Japan). ●4-5: Countries with privacy regulations that are inconsistently applied or \\nunderdeveloped in relation to AI (e.g., India, Australia). ●1-3: Countries with minimal focus on privacy in AI contexts, or where \\ndata protection laws are not enforced effectively (e.g., China). Human Oversight (1–10 Scale)\\nThis criterion assessed the role of human oversight in AI decision- \\nmaking, particularly in high-risk sectors like healthcare or autonomous \\nvehicles.',\n",
       "  '●9-10: Countries mandating human oversight in high-risk AI decisions, \\nensuring human intervention in critical areas (e.g., EU AI Act). ●6-8: Countries that recommend but do not legally enforce human over\\xad\\nsight (e.g., the US). ●4-5: Countries where human oversight is mentioned, but enforcement \\nmechanisms are vague or absent (e.g., Japan, India).',\n",
       "  '●1-3: Little to no emphasis on human oversight in AI policy frameworks. Ethical Standards (1–10 Scale)\\nEthical standards were scored based on how well a country’s AI policies adhere \\nto global ethical frameworks (such as UNESCO’s AI ethics recommendations) \\nand promote ethical AI development. ●9-10: Countries with a clearly defined, internationally aligned ethical \\nframework for AI (e.g., the EU, Canada). ●6-8: Countries with ethical guidelines for AI but limited in scope or \\nenforcement (e.g., Japan, the US). ●4-5: Countries that mention ethical AI but lack a coherent, enforceable \\nframework (e.g., India). ●1-3: Minimal or no formal focus on AI ethics in public policy.',\n",
       "  'e2463722-22\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Innovation Encouragement (1–10 Scale)\\nThis criterion measured the balance between promoting AI innovation and \\nenforcing ethical guidelines. Countries that foster innovation while maintain\\xad\\ning a robust ethical framework scored higher. ●9-10: Countries with innovation-centric policies that support AI research \\nand development while integrating ethical guidelines (e.g., US, Canada). ●6-8: Countries with strong innovation policies but less rigorous ethical \\nenforcement (e.g., Japan). ●4-5: Countries where innovation is promoted but at the cost of ethical \\nstandards (e.g., China). ●1-3: Countries where innovation in AI is stifled due to excessive regula\\xad\\ntion or a lack of resources (e.g., minimal focus). National Security (1–10 Scale)\\nNational security was evaluated based on how countries incorporate AI within \\ntheir national security strategies, including defense, cybersecurity, and \\nsurveillance. ●9-10: Countries where AI plays a significant role in national security \\nframeworks, with clear policies on military AI, surveillance, and cyber \\ndefense (e.g., China, the US). ●6-8: Countries that include AI in national security policies but with fewer \\nexplicit regulations on its use in defense (e.g., Australia, Japan). ●4-5: Countries with some mention of AI in national security contexts but \\nlacking concrete policies (e.g., India). ●1-3: Minimal focus on AI for national security, or policies that are still in \\nearly development (e.g., the EU). Justification for the Selection of Criteria and Scores\\nThe chosen criteria for evaluating AI ethics policy frameworks, transparency, \\naccountability, fairness, privacy, human oversight, ethical standards, innova\\xad\\ntion encouragement, and national security, were carefully selected to reflect \\nthe core dimensions that are essential for ethically sound, socially beneficial, \\nand technologically responsible AI systems. These dimensions are well- \\nestablished in policy discourse and academic literature as the pillars of AI \\nethics and governance, ensuring that AI development aligns with societal \\nvalues and mitigates potential harms. Transparency\\nTransparency is a cornerstone of AI ethics, as highlighted in both academic \\nand regulatory discussions (Floridi et al.',\n",
       "  '2018).',\n",
       "  'Transparent AI systems allow \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-23',\n",
       "  'stakeholders to understand how decisions are made and ensure accountability. The choice of transparency as a criterion is supported by regulatory frame\\xad\\nworks such as the European Union’s GDPR and AI Act, which place explicit \\ndemands on AI systems to be explainable and open to public scrutiny. Studies \\nhave shown that lack of transparency is one of the main causes of public \\ndistrust in AI systems (Wachter, Mittelstadt, and Russell 2023). Therefore, \\ncountries with clear legal mandates for transparency received higher scores, \\nwhile those with voluntary or vague transparency guidelines scored lower.',\n",
       "  'Accountability\\nAccountability ensures that AI developers, operators, and users are held \\nresponsible for the outcomes produced by AI systems. This criterion is \\njustified by the recognition in the literature that without clear accountability \\nstructures, it becomes difficult to address failures or harms caused by AI \\nsystems (Mittelstadt 2019). The EU AI Act introduces comprehensive provi\\xad\\nsions that assign legal responsibility, providing a strong model for account\\xad\\nability.',\n",
       "  'Countries like the United States, with voluntary compliance through \\nframeworks like the NIST AI Risk Management Framework, received inter\\xad\\nmediate scores due to the lack of enforceability. The necessity of accountability \\nis also a major theme in academic literature, particularly in the context of \\ncomplex AI systems where multiple stakeholders are involved in the design \\nand deployment (de Bruin and Floridi 2017; Floridi et al. 2018; Turilli and \\nFloridi 2009).',\n",
       "  'Fairness\\nFairness in AI systems addresses concerns about bias and discrimination, \\nwhich are well-documented issues in AI applications (Binns 2018).',\n",
       "  'Countries with explicit fairness requirements in their AI policies, such as the \\nEuropean Union and Canada, received higher scores because their frameworks \\nmandate fairness audits and bias mitigation practices. Literature on fairness in \\nAI often points to the limitations of algorithmic systems to ensure equitable \\noutcomes across demographic groups without specific regulatory intervention \\n(Du 2023; IBM 2018). Nations with minimal or non-enforceable fairness \\nprovisions, such as India and China, scored lower due to the absence of robust \\nbias-mitigation mechanisms.',\n",
       "  'Privacy\\nPrivacy is a critical concern in AI, especially in systems that rely on vast \\namounts of personal data. The GDPR in the EU sets a high global benchmark \\nfor data protection and privacy, justifying the high score for the EU in this \\ndimension. In contrast, countries like the United States, where privacy regula\\xad\\ntions such as HIPAA are domain-specific and not universally applicable to AI \\nsystems, scored lower (HIPAA 1996). Privacy as a criterion is grounded in the \\ne2463722-24\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'principle that ethical AI must protect individuals’ rights to control their data, \\na concern that is pervasive in academic and policy literature (Mittelstadt 2019). Human Oversight\\nHuman oversight in AI decision-making is essential to prevent over-reliance \\non automated systems, particularly in critical sectors like healthcare and law \\nenforcement (European Parliament 2023).',\n",
       "  'The EU AI Act again leads the way \\nby mandating human oversight in high-risk AI applications. The literature \\nemphasizes the importance of preserving human judgment in AI-assisted \\ndecision-making, especially in cases involving moral or legal consequences \\n(Jobin, Ienca, and Vayena 2019). Countries that recommend but do not \\nmandate human oversight scored lower, as voluntary oversight often fails in \\nreal-world applications, particularly where operational efficiency is prioritized \\nover human intervention.',\n",
       "  'Ethical Standards\\nEthical standards are increasingly seen as vital for aligning AI development \\nwith human values. UNESCO’s Recommendation on the Ethics of AI and \\nsimilar initiatives by the OECD have provided blueprints for ethical AI, \\nfocusing on principles such as beneficence, non-maleficence, autonomy, and \\njustice (UNESCO 2023). Countries like Canada and the EU, which have \\nadopted comprehensive ethical guidelines, scored highly.',\n",
       "  'These standards \\nare crucial for ensuring that AI operates within moral and legal boundaries. In contrast, countries that lack specific ethical frameworks for AI, such as \\nIndia and China, received lower scores, reflecting the underdevelopment of \\nethical considerations in their AI policies. Innovation Encouragement\\nThe balance between encouraging AI innovation and enforcing ethical stan\\xad\\ndards is a key concern for policymakers (Brynjolfsson and Mcafee 2014; Evans  \\n2015). Countries like the United States scored high in this dimension due to \\ntheir innovation-centric policies, such as the NIST AI Risk Management \\nFramework, which promotes industry-led solutions and fosters a favorable \\nenvironment for AI research and development. The literature supports the \\nnotion that innovation thrives when there is flexibility and minimal regulatory \\noverhead, but with the caveat that ethical guardrails must not be neglected \\n(Bostrom and Yudkowsky 2014).',\n",
       "  'Countries that focus excessively on regula\\xad\\ntion, potentially stifling innovation, or that lack sufficient incentives for AI \\nresearch, scored lower. National Security\\nNational security considerations, particularly regarding the development of \\nautonomous weapons systems (AWS) and AI-enhanced cybersecurity, are \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-25',\n",
       "  'becoming a critical component of AI policy (Singer 2009). Countries like the \\nUS and China, where AI plays a substantial role in national defense strategies, \\nscored highly. The literature on autonomous systems highlights the impor\\xad\\ntance of regulating AI to prevent unintended consequences in military appli\\xad\\ncations (Singer 2009). Countries that have not yet integrated AI into their \\nnational security strategies or have underdeveloped AI governance in this area, \\nsuch as the EU, scored lower. Justification for Scoring.',\n",
       "  'The scores for each dimension were derived from \\na combination of the following sources:\\n●Policy Documents and Regulations: Key regulatory frameworks such as \\nthe EU AI Act (Bommasani et al. 2023; European Parliament 2023, FACT \\nSHEET: Biden-Harris Administration Announces New Actions to \\nPromote Responsible AI Innovation That Protects Americans’ Rights \\nand Safety | The White House, 2023; Mozumder et al. 2022), GDPR \\n(GDPR 2018; ICO 2018), NIST AI Risk Management Framework \\n(NIST 2024a, 2024c; Tabassi 2023), and national AI strategies were \\ndirectly analyzed to assess the strength and comprehensiveness of each \\ncountry’s AI governance mechanisms. ●Academic Literature: Foundational texts on AI ethics, fairness, transpar\\xad\\nency, and accountability were referenced to establish baseline expecta\\xad\\ntions for what constitutes best practices in each dimension (Binns 2018; \\nFloridi et al.',\n",
       "  '2018; Jobin, Ienca, and Vayena 2019).',\n",
       "  '●Real-World Case Studies: Examples of AI implementation in various \\nsectors, including healthcare, finance, and national security, were exam\\xad\\nined to contextualize the practical impacts of each policy framework. The criteria were selected to ensure a comprehensive assessment of each \\ncountry’s approach to AI ethics, focusing on both regulatory stringency and \\nthe practical application of ethical principles. Each score reflects the extent to \\nwhich the country’s framework addresses key challenges associated with AI \\ngovernance, ensuring a balanced evaluation that is informed by both policy \\nanalysis and academic insights. Comparative Analysis Using Cartesian Graphs\\nThis section provides a detailed comparative analysis of AI Ethics Policy \\nFrameworks from a global perspective. The analysis covers major regions \\nsuch as the European Union, the United States, China, Canada, Japan, India, \\nand Australia.',\n",
       "  'The study examines these frameworks against key ethical \\ndimensions, including Transparency, Accountability, Fairness, and Privacy, \\nusing a series of Cartesian graphs. e2463722-26\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Each framework is rated on a scale of 1 to 10 across these dimensions. The \\nCartesian graph format helps to visualize how each country’s framework \\nmeasures up in these critical areas. For instance, the EU’s framework prior\\xad\\nitizes privacy and accountability, reflected in its high scores in these areas.',\n",
       "  'On \\nthe other hand, the US framework might score higher on transparency because \\nof its focus on open data and innovation.',\n",
       "  'China’s framework, emphasizing \\nsocial harmony and national security, might have different strengths and \\nweaknesses. This comparative analysis provides insights into the priorities and focus \\nareas of different countries and highlights the diversity and commonalities in \\napproaches to AI ethics globally. The graphical representation aids in under\\xad\\nstanding the complexities of each framework, offering a clear view of how \\nnations are navigating the ethical landscape of AI development. The Cartesian graph in Figure 7 compares AI Ethics Policy Frameworks \\nacross countries, such as the EU, the US, China, Canada, Japan, India, and \\nAustralia. It evaluates them on Transparency, Accountability, Fairness, and \\nPrivacy. Figure 7 presents an overview of how countries prioritize various elements \\nof AI ethics in their policy frameworks. The four aspects used for rating are \\ntransparency, accountability, fairness, and privacy.',\n",
       "  'Each aspect is rated on \\na scale from 1 to 10. The Blue Line represents transparency, which reflects how policies are \\nopenly communicated and implemented. The Green Line indicates account\\xad\\nability, measuring the extent to which AI developers and users are held \\naccountable for their systems as per the frameworks. The Red Line represents \\nfairness, measuring the extent to which the policies ensure fair and unbiased \\nAI systems.',\n",
       "  'Lastly, the Purple Line shows the importance of user privacy and \\ndata protection in the policies. The graph offers valuable insights into the global landscape of AI ethics. It \\nhighlights similarities and differences in national approaches to regulating AI \\nthat can help inform future policy development. The ratings of each aspect for \\neach country can help policymakers identify areas for improvement in their \\npolicies. This graph provides a tool for policymakers to understand the priorities of \\ndifferent countries regarding AI ethics. By focusing on transparency, account\\xad\\nability, fairness, and privacy, policymakers can create policies that address the \\nconcerns of stakeholders and help establish ethical guidelines for AI develop\\xad\\nment and use.',\n",
       "  'Extended Analysis and Global Implications\\nIn this section, we expand into an evaluation matrix that incorporates addi\\xad\\ntional dimensions that are increasingly relevant to AI ethics. These dimensions \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-27',\n",
       "  'include the roles of human oversight and national security, reflecting AI \\ntechnology’s broader socio-political implications. Including human oversight highlights the necessity for human intervention \\nand judgment in AI systems, a principle strongly supported by the EU frame\\xad\\nwork.',\n",
       "  'Conversely, national security is crucial in the frameworks of countries \\nlike China and the US, where AI’s involvement in defense and intelligence is \\na pivotal consideration. This comprehensive analysis emphasizes the global trends in AI policy \\nframeworks and the potential for harmonizing AI ethics. The section \\nexplores the possibility of converging principles and standards despite \\neach region’s diverse cultural, political, and social contexts. While complete \\nuniformity may be unattainable, the potential for international collabora\\xad\\ntion and consensus-building on core principles is significant.',\n",
       "  'Such harmo\\xad\\nnization could facilitate the establishment of universally accepted norms \\nand standards, ensuring that AI development aligns with ethical and socie\\xad\\ntal values. The analysis of similarities and differences in Figure 8 concludes that we \\nneed continued dialogue and cooperation among nations to cultivate \\na responsible and ethical global AI ecosystem. The Cartesian graph in Figure 8 includes lines representing concepts \\ninspired by earlier Venn diagrams that explored the issues surrounding AI \\ngovernance.',\n",
       "  'The lines on the graph represent different aspects of AI ethics that \\nhave been identified as necessary. The blue line represents transparency, \\nwhich refers to the openness and clarity in AI policy communication. The green line represents accountability, which signifies the extent of \\nresponsibility in AI development and use. The red line represents fair\\xad\\nness, which denotes the importance of ensuring unbiased AI systems. The purple line represents privacy, which highlights the importance of \\nuser privacy and data protection. The orange line represents human \\noversight, which signifies humans’ involvement and oversight in AI \\nprocesses. The brown line represents ethical standards, which means \\nadherence to ethical guidelines in AI. The pink line represents innova\\xad\\ntion encouragement, which reflects support for innovative AI develop\\xad\\nment. The gray line represents national security, emphasizing AI’s role \\nin national security. These lines provide a comprehensive view of how different countries \\naddress multiple facets of AI ethics in their policies. They reflect a broader \\nrange of considerations in the global discourse on AI governance.',\n",
       "  'By con\\xad\\nsidering the different aspects of AI ethics, policymakers can create fair, trans\\xad\\nparent, and accountable policies while encouraging innovation and protecting \\nuser privacy and data.',\n",
       "  'This is essential to building trust in AI and ensuring that \\nit is used to benefit society.',\n",
       "  'e2463722-28\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Strategies to Mitigate Bias\\nThis section expands into the issue of bias in Artificial Intelligence (AI) \\nsystems and its significant impact on fairness and effectiveness. We introduce \\nFigure 8, a network diagram visually representing various interconnected \\nstrategies crucial for mitigating bias. These include ensuring data diversity to \\navoid biased AI models, conducting regular audits to detect and correct biases, \\nemploying bias detection tools, and emphasizing the importance of algorith\\xad\\nmic transparency. In addition, we emphasize the importance of integrating \\ndiverse development teams and providing ethical AI training to reduce uncon\\xad\\nscious bias in AI design and development. The section then transitions to \\nFigure 10, which provides a more detailed network representation, highlight\\xad\\ning the synergies and dependencies among these strategies. This section \\ndemonstrates how a collective, multifaceted approach, comprising both tech\\xad\\nnical and organizational measures, is vital for developing AI systems that are \\nequitable, fair, and aligned with ethical standards. It is essential to note that \\nmitigating bias in AI is an ongoing process that requires continuous vigilance \\nand adaptation to evolving AI technologies and societal norms.',\n",
       "  'Comprehensive Mitigation Strategies\\nBias in AI systems is of utmost importance, as it can significantly impact the \\nfairness and effectiveness of these technologies. This section reviews various \\nstrategies aimed at mitigating bias, presented as a network diagram showcas\\xad\\ning the interconnectedness and collective importance of these strategies.',\n",
       "  'The network diagram encompasses a range of approaches, each linked to \\ndemonstrate how they complement and reinforce one another. Key strategies \\ninclude ensuring data diversity and using datasets representing all relevant \\ndemographics to prevent biased AI models.',\n",
       "  'Regular audits are crucial to \\nidentifying and addressing biases that may develop over time.',\n",
       "  'Additionally, \\nthe network emphasizes the importance of using bias detection tools, which \\nemploy specialized algorithms to uncover and address biases in AI systems. The diagram in Figure 9 emphasizes the significance of collectively imple\\xad\\nmenting these strategies, indicating that the most effective approach to miti\\xad\\ngating bias involves a multifaceted effort. This includes technical solutions and \\norganizational and procedural measures to ensure that AI systems are devel\\xad\\noped and operated in a manner that minimizes bias and promotes fairness. The diagram in Figure 9, shows the connections and assigned weights \\nbetween different strategies to mitigate bias in AI. The thickness of the lines \\ncorresponds to the strength of the connection, and the weights are labeled on \\nthe diagram.',\n",
       "  'The strategies are arranged circularly to emphasize their inter\\xad\\nconnectedness and importance. By implementing them collectively, the risk of \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-29',\n",
       "  'Figure 9. The significance of a collective implementation of AI Strategies: connections and Weights \\nin Strategies to Mitigate Bias in Al (Colour-coded by Strength). Figure 10.',\n",
       "  'Strategies to Mitigate AI Bias.',\n",
       "  'e2463722-30\\nP. RADANLIEV',\n",
       "  'bias in AI systems can be significantly reduced, leading to more equitable and \\ntrustworthy AI solutions. One key strategy is to ensure data diversity and representation.',\n",
       "  'This \\ninvolves using diverse data representing all relevant demographics to avoid \\nbiased models in AI systems. Regular audits are also essential to identify and \\nrectify any biases that may have crept in over time.',\n",
       "  'Bias detection tools are \\nanother essential strategy. These tools utilize specialized software to detect \\nbiases in AI algorithms, which can then be corrected. Diverse development \\nteams can also help minimize unconscious biases in designing and developing \\nAI systems. Providing ethical AI training to AI professionals is another way to mitigate \\nbias in AI. This training educates AI professionals on ethical considerations \\nand avoiding bias.',\n",
       "  'Algorithmic transparency is also crucial to reducing bias in \\nAI. By making the workings of AI algorithms transparent, biases can be \\nidentified and corrected more quickly. Involving various stakeholders, includ\\xad\\ning those from underrepresented groups, to provide feedback on AI systems \\nand their outputs can also significantly reduce bias. Finally, continuously \\nmonitoring AI systems is essential to quickly identify and address any biases \\nthat may emerge over time.',\n",
       "  'Collectively implementing these strategies can \\nsignificantly reduce the risk of bias in AI systems, leading to more equitable \\nand trustworthy AI solutions. The flowchart in Figure 10 outlines a network representation of strategies \\nfor mitigating bias in AI. The enhanced diagram provides more context and \\ndisplays the connections between these strategies, providing a comprehensive \\napproach to addressing this issue. As shown in Figure 9, data diversity is one of the primary strategies to \\nmitigate bias in AI. It is essential to ensure that data is collected from diverse \\ndemographics. This strategy is linked to bias detection tools, emphasizing the \\nimportance of having a wide range of data to identify and correct biases. Regular audits are another crucial component of mitigating bias in AI systems. Periodic reviews of AI systems for biases are required and are connected to \\ncontinuous monitoring. This highlights the need for ongoing assessments to \\nensure the AI system remains unbiased.',\n",
       "  'Using bias detection tools is also essential in mitigating bias in AI systems. This strategy links to algorithm transparency, underscoring detection tools’ \\nrole in making AI decisions more straightforward. Ensuring that the AI \\nalgorithm is transparent makes it easier to identify and correct any potential \\nbiases. Diverse teams are also vital to mitigating bias in AI systems.',\n",
       "  'Having diverse \\nbackgrounds in development teams can significantly reduce unconscious \\nbiases. This strategy is connected to ethical training, showing the importance \\nof diverse perspectives in ethical AI development.',\n",
       "  'This ensures that the AI \\nsystem is developed ethically and unbiasedly. APPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-31',\n",
       "  'Ethical training is another crucial strategy in mitigating bias in AI systems. Training on ethical AI development practices is vital, and this connects to \\nstakeholder feedback. This illustrates the role of ethical considerations in \\nincorporating diverse viewpoints, ensuring that the AI system is developed \\nwith the interests of all stakeholders in mind. Making AI algorithms’ decisions clear is another essential strategy for \\nidentifying biases. This is connected to regular audits, highlighting the need \\nfor transparency in ongoing assessments. Ensuring that the AI algorithm’s \\ndecisions are transparent makes it easier to identify and correct any potential \\nbiases. Incorporating feedback from all groups, including underrepresented ones, \\nis essential in developing an unbiased AI system. This relates back to data \\ndiversity, emphasizing the role of inclusive feedback in ensuring diverse data \\nrepresentation. Diverse feedback and ongoing surveillance for emerging biases \\nis necessary to mitigate bias in AI systems. This relates to diverse teams, \\nunderscoring the need for continuous oversight by teams with varied back\\xad\\ngrounds and perspectives.',\n",
       "  'Diverse teams make it easier to identify and correct \\npotential biases. The network diagram in Figure 9 illustrates the interconnected nature of \\nthese strategies, showing how each contributes to a comprehensive approach \\nto mitigating bias in AI systems. By implementing these strategies, AI systems \\ncan be developed more ethically and unbiasedly, with the interests of all \\nstakeholders in mind. Ethical Training and Diverse Teams\\nProviding ethical training to AI professionals is crucial to making them \\naware of potential biases and fostering an ethical culture in AI devel\\xad\\nopment. This training should cover the ethical implications of AI, the \\nsignificance of diversity in datasets, and ways to detect and mitigate \\nbias. Forming diverse teams is also a vital strategy. Teams composed of people \\nfrom diverse backgrounds bring unique perspectives to the AI development \\nprocess, which can help identify biases that a more homogeneous group may \\noverlook. The diversity here refers to demographic factors and variations in \\nexpertise, experience, and viewpoints.',\n",
       "  'By combining ethical training, diversity in teams, and technical strate\\xad\\ngies such as data diversity and regular audits, AI systems can be devel\\xad\\noped in a way that is more equitable, fair, and aligned with ethical \\nstandards. Countering bias is a continuous process that necessitates \\nongoing attention and adaptation as AI technologies progress.',\n",
       "  'e2463722-32\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Emerging AI Technologies and Their Ethical Challenges\\nLarge language models (LLMs), such as GPT-3 and GPT-4, exemplify the \\ngrowing power of generative AI. These models are trained on vast datasets, \\noften scraping data from the internet, which raises significant concerns over \\ndata provenance, copyright infringement, and privacy violations. The opa\\xad\\nque nature of these models complicates efforts to ensure that they are free from \\nbiases present in the training data, such as discriminatory language, misinfor\\xad\\nmation, or unintentional perpetuation of harmful stereotypes.',\n",
       "  'Despite \\nemploying fine-tuning and debiasing techniques, these models are still prone \\nto producing biased outputs due to the inherent limitations of the training \\ndata and the probabilistic nature of their generation processes. For instance, \\ntechniques such as reinforcement learning from human feedback (RLHF) \\nhave been deployed to mitigate harmful outputs, but they remain insufficient \\nin addressing the deeper systemic biases embedded within the underlying \\ndatasets. This calls for more sophisticated techniques, such as adversarial \\ntraining, where adversarial examples are used to iteratively refine models \\nand expose hidden biases. Additionally, federated learning presents \\na promising approach for enhancing the ethical training of LLMs by allowing \\nmodels to learn from decentralized, anonymized data, thus reducing the \\nethical risks associated with data centralization and privacy violations. Another critical issue with LLMs lies in their ability to produce convincing \\nbut factually incorrect or hallucinatory outputs. This problem, often referred \\nto as the “hallucination problem,” presents ethical challenges in high-stakes \\ndomains such as healthcare or law, where accurate information is paramount. Current mitigation strategies include truth-verification models that cross- \\ncheck generated content against verified databases and automated fact- \\nchecking systemsintegrated into the model’s inference pipeline.',\n",
       "  'However, \\nthese methods are still evolving and are far from fully resolving the issue. Ethical frameworks for LLM deployment must, therefore, include rigorous \\npost-deployment monitoring and real-time validation mechanisms to ensure \\nthe integrity of the outputs, particularly in applications where misinformation \\ncould have profound societal impacts.',\n",
       "  'Autonomous systems, including autonomous vehicles, drones, and \\nrobotic systems, pose additional ethical challenges related to safety, \\naccountability, and decision-making autonomy. A key ethical dilemma \\narises in the context of autonomous decision-making in unpredictable \\nenvironments. For instance, in the case of autonomous vehicles, ethical \\nframeworks must account for the so-called trolley problem scenarios, \\nwhere the system must make life-and-death decisions in the event of an \\nunavoidable accident.',\n",
       "  'Traditional rule-based ethical systems, such as \\ndeontological or utilitarian approaches, often fail to provide clear solu\\xad\\ntions in these nuanced scenarios. Consequently, emerging solutions \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-33',\n",
       "  'involve the use of ethical AI algorithms like multi-objective optimiza\\xad\\ntion, which allows systems to balance competing ethical principles – such \\nas minimizing harm and respecting human autonomy – by assigning \\ndynamic weights to different ethical outcomes based on real-time envir\\xad\\nonmental factors. Furthermore, accountability in autonomous systems presents a unique \\nchallenge, especially in cases where systems operate with minimal human \\noversight. Explainable AI (XAI) plays a critical role here, enabling transpar\\xad\\nency in decision-making processes by providing interpretable insights into \\nhow the system reached a specific decision. Techniques such as attention \\nmechanisms and saliency maps can be employed to highlight the features that \\nmost influenced an autonomous system’s decision, making it easier for reg\\xad\\nulators and auditors to understand and assess the fairness and safety of these \\ndecisions. However, the effectiveness of XAI in highly complex, real-time \\nautonomous systems remains limited, necessitating the development of causal \\ninference models that can provide a more comprehensive understanding of \\ndecision-making pathways and their underlying ethical implications. The issue of algorithmic accountability in autonomous weapons systems \\n(AWS) presents perhaps the most acute ethical challenge. The development \\nand deployment of AWS raise profound concerns over autonomous lethality \\n—the ability of a system to make life-or-death decisions without human \\nintervention.',\n",
       "  'Current discussions on international AI governance focus on \\nthe need to restrict the deployment of AWS through legally binding treaties, \\nbut enforcement mechanisms remain elusive. From a technical standpoint, \\none proposed solution involves embedding human-in-the-loop (HITL) \\nmechanisms that ensure critical decisions, particularly those involving the \\nuse of lethal force, require human validation before execution.',\n",
       "  'This integration \\nof human oversight into decision-making processes is critical to preventing \\nunintended harm and ensuring compliance with international humanitarian \\nlaw. Additionally, ongoing research into ethical-by-design architectures aims \\nto build ethical constraints directly into the system’s operational framework, \\nlimiting the scope of actions that an autonomous system can take based on \\npredefined ethical guidelines. Finally, the deployment of swarm intelligence in autonomous drones and \\nrobots introduces challenges related to collective decision-making and dis\\xad\\ntributed accountability. In swarm systems, decisions are often made collec\\xad\\ntively by a distributed group of agents, with no single agent being responsible \\nfor the final outcome. This creates significant ethical ambiguity in determining \\naccountability when swarm systems malfunction or cause harm.',\n",
       "  'Solutions \\nsuch as distributed ledger technologies (DLT), including blockchain, have \\nbeen proposed to ensure that every decision made within the swarm is \\nrecorded in a transparent and immutable way, providing a traceable log of \\nactions that can be audited for accountability purposes. e2463722-34\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Discussion\\nThe introduction of fairness, transparency, and accountability into AI systems, \\nwhile crucial for ensuring ethical standards, introduces a significant financial \\nburden and operational complexity, especially in sectors where fast innovation \\nis a competitive necessity. One of the primary economic costs arises from the increased complexity in \\ndeveloping AI systems that adhere to ethical guidelines. Implementing fair\\xad\\nness-aware learning algorithms, such as demographic parity or equalized odds, \\nrequires additional computational resources and extensive testing during the \\ntraining phase.',\n",
       "  'These fairness constraints are not simply add-ons but require \\na fundamental rethinking of the algorithmic design, particularly in cases where \\nperformance optimization conflicts with fairness. For instance, in financial \\nservices, ensuring that loan approval algorithms do not exhibit bias may \\nnecessitate retraining models with diverse datasets and applying fairness con\\xad\\nstraints throughout the development cycle. This extended development pro\\xad\\ncess incurs higher labor costs, requires greater infrastructure investment, and \\noften results in longer timeframes to achieve regulatory compliance. Additionally, privacy-preserving techniques, such as differential privacy and \\nfederated learning, add further complexity. Federated learning, which enables \\nmodel training across distributed datasets without centralizing sensitive data, \\nrequires more sophisticated system architectures and secure communication \\nchannels, increasing both the cost and technical difficulty of implementation. Operationally, the impact of strict ethical guidelines is felt through the need \\nfor ongoing compliance and continuous monitoring of AI systems.',\n",
       "  'Ethical \\nframeworks such as the European Union’s AI Act mandate that high-risk AI \\napplications, particularly in fields like healthcare and criminal justice, undergo \\ncontinuous auditing to ensure ethical standards are maintained post- \\ndeployment. These operational costs are amplified by the need to integrate \\nreal-time fairness monitoring tools, such as AI Fairness 360, which check for \\nbias drift or decision-making anomalies as AI systems encounter new data.',\n",
       "  'These tools require continuous computational resources, infrastructure sup\\xad\\nport, and personnel dedicated to auditing and model recalibration. For indus\\xad\\ntries such as financial services, where AI systems are deployed in real-time \\nenvironments like high-frequency trading, maintaining fairness and compli\\xad\\nance adds layers of complexity to the operational workflow.',\n",
       "  'This constant \\nneed for recalibration can also result in downtime, during which systems must \\nbe reevaluated and updated, leading to delays in decision-making processes \\nand potential disruptions to business continuity. The financial impact of these ethical requirements also affects innovation \\ncycles and speed to market.',\n",
       "  'In highly competitive sectors like autonomous \\ndriving or AI-driven diagnostics, time-to-market is often crucial for gaining \\na first-mover advantage. Companies that invest heavily in ethical compliance – \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-35',\n",
       "  'such as model transparency, fairness audits, and explainability – may experi\\xad\\nence delays in bringing products to market. For instance, the requirement to \\nintegrate explainability mechanisms, such as SHAP (Shapley Additive \\nExplanations) or LIME (Local Interpretable Model-agnostic Explanations), \\ninto AI models often necessitates additional development and testing phases. This extends the overall project timeline and may place companies at \\na competitive disadvantage against those who prioritize rapid deployment \\nover ethical oversight. The delay not only impacts short-term revenue but \\nalso affects long-term strategic positioning, particularly in industries where \\ntechnological leadership is key to maintaining market share.',\n",
       "  'Beyond development and operational costs, legal compliance and regula\\xad\\ntory risk are significant financial considerations for companies implementing \\nstrict ethical guidelines. Regulatory frameworks like the GDPR and the \\nupcoming EU AI Act impose severe penalties for noncompliance, with fines \\nthat can reach up to 4% of a company’s global revenue for violations of data \\nprivacy and transparency requirements. To mitigate these risks, companies \\noften need to invest heavily in legal teams, external audits, and compliance \\ninfrastructures.',\n",
       "  'This introduces an additional cost layer as companies must \\nallocate resources not just for initial development but also for ongoing com\\xad\\npliance management. The cyclical nature of compliance – where systems must \\nbe continuously updated, audited, and re-certified to meet evolving stan\\xad\\ndards – creates long-term financial commitments that extend well beyond \\nthe initial implementation of AI systems. Despite these costs, emerging technologies offer potential solutions \\nthat could mitigate some of the financial and operational burdens \\nassociated with ethical AI. Automated machine learning (AutoML) sys\\xad\\ntems are increasingly capable of incorporating fairness and transparency \\nchecks into their development pipelines, reducing the need for manual \\nintervention and thus lowering labor costs. Additionally, distributed \\nledger technologies (DLT), such as blockchain, can help track AI deci\\xad\\nsions in a transparent and immutable way, thereby simplifying post- \\ndeployment audits and reducing the cost of maintaining ethical stan\\xad\\ndards. Nevertheless, while these technologies offer some relief, they \\ncome with their own set of technical challenges and infrastructural \\ncosts, which require additional investment and expertise to implement \\neffectively. The implementation of strict ethical guidelines in AI development signifi\\xad\\ncantly impacts economic and operational aspects of AI projects.',\n",
       "  'While these \\nguidelines are crucial for ensuring fairness, transparency, and accountability, \\nthey introduce substantial costs at every stage of the AI lifecycle, from devel\\xad\\nopment through to post-deployment monitoring and compliance. Balancing \\nthese ethical obligations with the need for innovation and market competi\\xad\\ntiveness remains a challenge, particularly for companies operating in highly \\ne2463722-36\\nP. RADANLIEV',\n",
       "  'dynamic and competitive sectors. The evolving landscape of AI governance, \\ncoupled with emerging cost-saving technologies, will be critical in determining \\nhow companies navigate the financial and operational implications of ethical \\nAI development. Conclusion\\nThis study has undertaken an examination of the ethical imperatives sur\\xad\\nrounding AI, particularly the principles of transparency, fairness, and privacy, \\nin the context of its prevalent influence across sectors such as healthcare, \\nfinance, and communication.',\n",
       "  'The deployment of AI technologies in these \\ndomains brings with it profound ethical challenges that necessitate a strong \\nand inclusive framework to safeguard individual rights and societal interests. Through a comparative analysis of international AI policy frameworks from \\nthe European Union, the United States, and China, this research has clarified \\nthe conflicting ethical priorities that shape AI governance globally. This work clarifies the ethical principles of privacy, transparency, and fair\\xad\\nness, addressing regional challenges and interdependencies.',\n",
       "  'By distinguishing \\nhow these principles operate independently yet interactively across frame\\xad\\nworks, the paper offers a refined conceptual foundation necessary for global \\ngovernance. A primary contribution is the proposed set of integration criteria \\n(Interoperability, Normative Cohesion, Cultural Adaptability, and \\nTransparency of Process). These criteria provide a structured foundation for \\naligning ethical principles across diverse international frameworks, supporting \\ncross-border AI compatibility while respecting region-specific values and \\nregulatory approaches.',\n",
       "  'The graphical representations represent the individual \\nand corelated interdependencies and conflicts among frameworks, avoiding \\noversimplification and enhancing analytical clarity. This provides a visual tool \\nfor understanding ethical elationships in global AI governance.',\n",
       "  'The analysis reveals marked variations in how different regions balance the \\ndemands of innovation against the ethical principles of privacy, fairness, and \\naccountability. While certain jurisdictions, such as the European Union, \\nemphasize stringent regulatory oversight and data protection, others, includ\\xad\\ning the United States, adopt a more flexible, innovation-centric approach.',\n",
       "  'These divergences underscore the complexities involved in striving for \\na harmonized global standard for ethical AI governance. Nevertheless, this \\nstudy has articulated several strategic interventions to mitigate algorithmic \\nbias, including the deployment of fairness-aware algorithms, regular audits, \\nand the incorporation of diverse development teams. These interventions are \\nessential in fostering equitable and trustworthy AI systems.',\n",
       "  'Furthermore, this research has highlighted the critical need for sustained \\ninternational collaboration and dialogue to bridge the gaps in global AI ethics \\nframeworks. It is increasingly evident that no single jurisdiction can fully \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-37',\n",
       "  'address the multi-faceted ethical challenges posed by AI in isolation. Instead, \\nthe path forward demands a concerted, cooperative effort that leverages shared \\nprinciples while respecting regional variations in regulatory and cultural \\npriorities. This study advances the argument that ethical considerations must be \\nembedded at every stage of the AI development lifecycle, from inception \\nthrough to deployment and beyond. The recommendations herein aim to \\ninform policymakers, regulators, and AI developers, encouraging the pursuit \\nof AI systems that are not only innovative and technologically advanced but \\nalso aligned with the highest ethical standards. As AI continues to evolve and \\nexert its transformative potential, the need for vigilance, adaptability, and \\ncross-border cooperation remains paramount in ensuring that these technol\\xad\\nogies serve the common good, promoting fairness, accountability, and trust in \\ntheir application. Disclosure Statement\\nNo potential conflict of interest was reported by the author(s).',\n",
       "  'Funding\\nThe work was supported by the Engineering and Physical Sciences Research Council [EP/ \\nS035362/1]. ORCID\\nPetar Radanliev \\nhttp://orcid.org/0000-0001-5629-6857\\nData Availability Statement\\nThe datasets generated and analyzed during the current study are available from the corre\\xad\\nsponding author upon reasonable request. Due to the sensitive nature of the data related to AI \\nethics and privacy considerations, access to the data may be restricted.',\n",
       "  'Specific details regard\\xad\\ning the data sources, including international AI policy frameworks from the EU, US, China, \\nCanada, Japan, India, and Australia, are documented within the study. All data shared will be \\ncompliant with ethical guidelines and privacy standards as outlined in the General Data \\nProtection Regulation (GDPR) and other relevant data protection laws. References\\nAldoseri, A., K.',\n",
       "  'N. Al-Khalifa, and A. M. Hamouda.',\n",
       "  '2023. Re-Thinking data strategy and \\nintegration for artificial intelligence: Concepts, opportunities, and challenges. Applied \\nSciences 13 (12):7082. doi: 10.3390/APP13127082  . e2463722-38\\nP.',\n",
       "  'RADANLIEV',\n",
       "  'Bécue, A., I. Praça, and J. Gama.',\n",
       "  '2021. Artificial intelligence, cyber-threats and industry 4.0: \\nChallenges and opportunities. Artificial Intelligence Review 54 (5):3849–86.',\n",
       "  'doi: 10.1007/ \\ns10462-020-09942-2  . Bender, E.',\n",
       "  'M., T. Gebru, A. McMillan-Major, and S.',\n",
       "  'Shmitchell.',\n",
       "  '2021.',\n",
       "  'On the dangers of \\nstochastic parrots: Can language models be too big? FAccT 2021 - Proceedings of the 2021 \\nACM Conference on Fairness, Accountability, and Transparency, 610–23. doi: 10.1145/ \\n3442188.3445922  .',\n",
       "  'Binns, R.',\n",
       "  '2018. Fairness in machine learning: Lessons from political philosophy. Proceedings of \\nMachine Learning Research, vol. 81, 149–59, PMLR. https://proceedings.mlr.press/v81/ \\nbinns18a.html . Bommasani, R., K.',\n",
       "  'Klyman, D. Zhang, and P. Liang.',\n",
       "  '2023. Do foundation model providers \\ncomply with the draft EU AI act? Center for Research on Foundation Models (CRFM): \\nStanford Center for Research on Foundation Models. Bostrom, N., and E.',\n",
       "  'Yudkowsky. 2014. The ethics of artificial intelligence. The Cambridge \\nHandbook of Artificial Intelligence 316–34. doi: 10.1017/CBO9781139046855.020  .',\n",
       "  'Brynjolfsson, E., and A.',\n",
       "  'Mcafee.',\n",
       "  '2014. The second machine age: Work, progress, and prosperity \\nin a time of brilliant technologies. In The second machine age: Work, progress, and prosper\\xad\\nity in a time of brilliant technologies.',\n",
       "  'Worldwide: W.W. Norton & Company 978-0-393- \\n35064-7 https://wwnorton.com/books/the-second-machine-age/ . de Bruin, B., and L.',\n",
       "  'Floridi. 2017. The ethics of cloud computing. Science and Engineering \\nEthics 23 (1):21–39. doi: 10.1007/s11948-016-9759-0  . de Fine Licht, K., and J.',\n",
       "  'de Fine Licht. 2020. Artificial intelligence, transparency, and public \\ndecision-making. AI & Society 35 (4):917–26. doi: 10.1007/s00146-020-00960-w  . Du, M.',\n",
       "  '2023. Awesome-Fairness-in-AI. GitHub Repository. https://github.com/datamllab/awe \\nsome-fairness-in-ai . European Parliament. 2023. AI act: A step closer to the first rules on artificial intelligence | news | \\nEuropean Parliament. https://www.europarl.europa.eu/news/en/press-room \\n/20230505IPR84904/ai-act-a-step-closer-to-the-first-rules-on-artificial-intelligence .',\n",
       "  'Evans, K.',\n",
       "  '2015. The second machine age: Work, progress, and prosperity in a time of brilliant \\ntechnologies by eric Brynjolfsson and Andrew McAfee. Journal of Business & Finance \\nLibrarianship 20 (3):244–46.',\n",
       "  'doi: 10.1080/08963568.2015.1044355  . FACT SHEET: Biden-Harris Administration Announces New Actions to Promote Responsible \\nAI Innovation That Protects Americans’ Rights and Safety | The White House. 2023. https:// \\nwww.whitehouse.gov/briefing-room/statements-releases/2023/05/04/fact-sheet-biden- \\nharris-administration-announces-new-actions-to-promote-responsible-ai-innovation-that- \\nprotects-americans-rights-and-safety/ . Floridi, L., J.',\n",
       "  'Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge, \\nR. Madelin, U. Pagallo, F. Rossi, et al.',\n",
       "  '2018. AI4People—an ethical framework for a good \\nAI society: Opportunities, risks, principles, and recommendations. Minds and Machines \\n28 (4):689–707. doi: 10.1007/s11023-018-9482-5  . GDPR.',\n",
       "  '2018. What is GDPR, the EU’s new data protection law? - Gdpr.Eu.',\n",
       "  'https://gdpr.eu/ \\nwhat-is-gdpr/ . Helbing, D., B. S. Frey, G. Gigerenzer, E. Hafen, M. Hagner, Y. Hofstetter, J. Van Den Hoven, \\nR. V. Zicari, and A. Zwitter. 2018. Will democracy survive big data and artificial intelligence? In Towards digital enlightenment: Essays on the dark and light sides of the digital revolu\\xad\\ntion, 73–98. Springer International Publishing. doi: 10.1007/978-3-319-90869-4_7  . HIPAA.',\n",
       "  '1996. Health insurance portability and accountability act of 1996 (HIPAA) | CDC. https://www.cdc.gov/phlp/publications/topic/hipaa.html .',\n",
       "  'Hosny, A., C.',\n",
       "  'Parmar, J. Quackenbush, L. H. Schwartz, and H. J. W. L.',\n",
       "  'Aerts. 2018. Artificial \\nintelligence in radiology. Nature Rev Cancer 18 (8):500. doi: 10.1038/S41568-018-0016-5  . APPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-39',\n",
       "  'IBM. 2018. AI fairness 360 – open source. Open Project. https://www.ibm.com/opensource/ \\nopen/projects/ai-fairness-360/ . ICO.',\n",
       "  '2018. Information commissioner’s office (ICO): The UK GDPR. UK GDPR Guidance and \\nResources. https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful- \\nbasis/a-guide-to-lawful-basis/lawful-basis-for-processing/consent/ . Interim Measures for the Management of Generative Artificial Intelligence Services, Personal \\nInformation Protection Law of the People’s Republic of China (PRC). 2023.',\n",
       "  'ISO. 2023. ISO/IEC DIS 42001 - information technology — artificial intelligence — management \\nsystem. https://www.iso.org/standard/81230.html . Jobin, A., M.',\n",
       "  'Ienca, and E. Vayena.',\n",
       "  '2019. The global landscape of AI ethics guidelines. Nature \\nMachine Intelligence 2019 1 (9):389–99. doi: 10.1038/s42256-019-0088-2  . Li, L.',\n",
       "  '2017. China’s manufacturing locus in 2025: With a comparison of “Made-in-China 2025” \\nand “Industry 4.0”. Technological Forecasting & Social Change 135:66–74.',\n",
       "  'doi: 10.1016/J.',\n",
       "  'TECHFORE.2017.05.028  .',\n",
       "  'Malhotra, Y.',\n",
       "  '2018. Cognitive computing for anticipatory risk analytics in intelligence, surveil\\xad\\nlance, & reconnaissance (ISR): Model risk management in artificial intelligence & machine \\nlearning (presentation slides). SSRN Electronic Journal.',\n",
       "  'doi: 10.2139/ssrn.3111837  . McCorduck, P., and C.',\n",
       "  'Cfe.',\n",
       "  '2004. Machines who think: A personal inquiry into the history and \\nprospects of artificial intelligence. CRC Press. https://books.google.com/books?hl=en&lr= \\n&id=r2C1DwAAQBAJ&oi=fnd&pg=PP1&dq=Pamela+McCorduck+%22Machines+Who \\n+Think&ots=UnmXIiuRtM&sig=JAh90Eu07MGvjS5OgFq1CMy6-gc . Meissner, G.',\n",
       "  '2020. Artificial intelligence: Consciousness and conscience. AI & Society \\n35 (1):225–35. doi: 10.1007/s00146-019-00880-4  .',\n",
       "  'MeitY.',\n",
       "  '2023. Artificial intelligence committees reports | Ministry of electronics and informa\\xad\\ntion technology, Government of India. Artificial Intelligence Committees Report. https:// \\nwww.meity.gov.in/artificial-intelligence-committees-reports . Mijwil, M.',\n",
       "  'M., M. Aljanabi, and ChatGPT. 2023. Towards artificial intelligence-based cyberse\\xad\\ncurity: The practices and ChatGPT generated ways to combat cybercrime. Iraqi Journal for \\nComputer Science and Mathematics 4 (1):65–70. doi: 10.52866/IJCSM.2023.01.01.0019  . Mittelstadt, B.',\n",
       "  '2019. Principles alone cannot guarantee ethical AI. Nature Machine Intelligence \\n1 (11):501–07. doi: 10.1038/s42256-019-0114-4  .',\n",
       "  'Mozumder, M.',\n",
       "  'A. I., M. M. Sheeraz, A. Athar, S. Aich, and H.-C. Kim.',\n",
       "  '2022. Overview: \\nTechnology roadmap of the future trend of metaverse based on IoT, blockchain, AI \\ntechnique, and medical domain metaverse activity. International Conference on Advanced \\nCommunication Technology (ICACT) 256–61. doi: 10.23919/ICACT53585.2022.9728808  . NAIAC.',\n",
       "  '2024. AI safety: National AI advisory committee. https://ai.gov/wp-content/uploads/ \\n2024/06/FINDINGS-RECOMMENDATIONS_AI-Safety.pdf . NIST. 2023a. AI risk management framework | NIST. National Institute of Standards and \\nTechnology. https://www.nist.gov/itl/ai-risk-management-framework . NIST. 2023b. Artificial intelligence | NIST. https://www.nist.gov/artificial-intelligence . NIST. 2024a. AI risk management framework | NIST. NIST. 2024b. AI standards | NIST. https://www.nist.gov/artificial-intelligence/ai-standards . NIST.',\n",
       "  '2024c. Department of commerce announces new guidance, tools 270 days following \\npresident Biden’s executive order on AI | NIST. https://www.nist.gov/news-events/news/ \\n2024/07/department-commerce-announces-new-guidance-tools-270-days-following . Office for Artificial Intelligence and Department for Science, Innovation & Technology.',\n",
       "  '2023. A pro-innovation approach to AI regulation 978-1-5286-4009-1 (London: Crown copyright). Partnership on AI.',\n",
       "  '2023. Partnership on AI and the ethical AI framework for social good. https:// \\npartnershiponai.org/ . e2463722-40\\nP. RADANLIEV',\n",
       "  'Provisions on the Administration of Deep Synthesis Internet Information Services, Personal \\nInformation Protection Law of the People’s Republic of China (PRC). 2022.',\n",
       "  'Roberts, H., J. Cowls, J. Morley, M. Taddeo, V. Wang, and L. Floridi.',\n",
       "  '2021. The Chinese \\napproach to artificial intelligence: An analysis of policy, ethics, and regulation. AI & Society \\n36 (1):59–77. doi: 10.1007/s00146-020-00992-2  . Shu, Y., J.',\n",
       "  'Zhang, and H. Yu.',\n",
       "  '2021.',\n",
       "  'Fairness in design: A tool for guidance in ethical artificial \\nintelligence design. Lecture Notes in Computer Science (Including Subseries Lecture Notes in \\nArtificial Intelligence and Lecture Notes in Bioinformatics) 12774:500–10. doi: 10.1007/978- \\n3-030-77626-8_34  .',\n",
       "  'Singer, P.',\n",
       "  'W.',\n",
       "  '2009. Wired for war: The robotics revolution and conflict in the twenty-first \\ncentury, 499. https://books.google.com/books/about/Wired_for_War.html?id= \\nAJuowQmtbU4C . The State Council People Republic of China.',\n",
       "  '2017. Made in China 2025; the state council people \\nRepublic of China. http://english.gov.cn/2016special/madeinchina2025/ . Tabassi, E.',\n",
       "  '2023. AI risk management framework | NIST.',\n",
       "  'doi: 10.6028/NIST.AI.100-1  . Turilli, M., and L.',\n",
       "  'Floridi. 2009.',\n",
       "  'The ethics of information transparency. Ethics and Information \\nTechnology 11 (2):105–12. doi: 10.1007/s10676-009-9187-9  . UNESCO.',\n",
       "  '2023. Recommendation on the ethics of artificial intelligence | UNESCO. https://www. unesco.org/en/articles/recommendation-ethics-artificial-intelligence . Wachter, S., B.',\n",
       "  'Mittelstadt, and C.',\n",
       "  'Russell. 2023. Health care bias is dangerous. But so are \\nfairness’ algorithms | WIRED. Wired. https://www.wired.com/story/bias-statistics-artificial- \\nintelligence-healthcare/ . Yu, K.',\n",
       "  'H., A. L. Beam, and I. S. Kohane. 2018. Artificial intelligence in healthcare. Nature \\nBiomedical Engineering 2 (10):719–31. doi: 10.1038/S41551-018-0305-Z. APPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-41',\n",
       "  'IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL.',\n",
       "  '4, NO.',\n",
       "  '4, AUGUST 2023\\n799\\nAn Overview of Artiﬁcial Intelligence Ethics\\nChangwu Huang\\n, Member, IEEE, Zeqi Zhang, Bifei Mao, and Xin Yao\\n, Fellow, IEEE\\nAbstract—Artiﬁcial intelligence (AI) has profoundly changed\\nand will continue to change our lives. AI is being applied in more\\nand more ﬁelds and scenarios such as autonomous driving, med-\\nical care, media, ﬁnance, industrial robots, and internet services. The widespread application of AI and its deep integration with\\nthe economy and society have improved efﬁciency and produced\\nbeneﬁts.',\n",
       "  'At the same time, it will inevitably impact the existing\\nsocial order and raise ethical concerns. Ethical issues, such as\\nprivacy leakage, discrimination, unemployment, and security risks,\\nbrought about by AI systems have caused great trouble to people. Therefore, AI ethics, which is a ﬁeld related to the study of ethical\\nissues in AI, has become not only an important research topic\\nin academia, but also an important topic of common concern for\\nindividuals, organizations, countries, and society. This article will\\ngive a comprehensive overview of this ﬁeld by summarizing and\\nanalyzingtheethicalrisksandissuesraisedbyAI,ethicalguidelines\\nand principles issued by different organizations, approaches for\\naddressing ethical issues in AI, and methods for evaluating the\\nethics of AI. Additionally, challenges in implementing ethics in\\nAI and some future perspectives are pointed out. We hope our\\nwork will provide a systematic and comprehensive overview of AI\\nethics for researchers and practitioners in this ﬁeld, especially the\\nbeginners of this research discipline. Impact Statement—AI ethics is an important emerging topic\\namong academia, industry, government, society, and individuals.',\n",
       "  'In\\nthe past decades, many efforts have been made to study the ethical\\nissues in AI. This article offers a comprehensive overview of the AI\\nethics ﬁeld, including a summary and analysis of AI ethical issues,\\nManuscript received 5 September 2021; revised 22 February 2022 and 3\\nMay 2022; accepted 23 July 2022. Date of publication 28 July 2022; date\\nof current version 21 July 2023. This work was supported in part by the\\nResearch Institute of Trustworthy Autonomous Systems (RITAS), in part by\\nthe Guangdong Provincial Key Laboratory under Grant 2020B121201001, in\\npart by the Program for Guangdong Introducing Innovative and Enterpreneurial\\nTeams under Grant 2017ZT07X386, in part by Shenzhen Science and Tech-\\nnology Program under Grant KQTD2016112514355531, and in part by a joint\\nproject between Huawei and Southern University of Science and Technology\\nunder Project FA2019061021. This paper was recommended for publication\\nby Associate Editor J.',\n",
       "  'Torresen upon evaluation of the reviewers’ comments. (Corresponding author: Xin Yao.)\\nChangwu Huang is with the Research Institute of Trustworthy Autonomous\\nSystems, Southern University of Science and Technology, Shenzhen 518055,\\nChina, and also with the Guangdong Provincial Key Laboratory of Brain-\\ninspired Intelligent Computation, Department of Computer Science and En-\\ngineering, Southern University of Science and Technology, Shenzhen 518055,\\nChina (e-mail: huangcw3@sustech.edu.cn). Zeqi Zhang and Bifei Mao are with the Trustworthiness Theory Research\\nCenter, Huawei Technologies Company, Ltd., Shenzhen 518055, China (e-mail:\\nzhangzeqi@huawei.com; maobifei@huawei.com). Xin Yao is with the Research Institute of Trustworthy Autonomous Systems,\\nSouthern University of Science and Technology, Shenzhen 518055, China, with\\nGuangdong Provincial Key Laboratory of Brain-inspired Intelligent Computa-\\ntion, Department of Computer Science and Engineering, Southern University\\nof Science and Technology, Shenzhen 518055, China, and also with the School\\nof Computer Science, University of Birmingham, B15 2TT Birmingham, U.K.',\n",
       "  '(e-mail: xiny@sustech.edu.cn).',\n",
       "  'This\\narticle\\nhas\\nsupplementary\\ndownloadable\\nmaterial\\navailable\\nat\\nhttps://doi.org/10.1109/TAI.2022.3194503, provided by the authors. Digital Object Identiﬁer 10.1109/TAI.2022.3194503\\nethical guidelines and principles, approaches to address AI ethical\\nissues, and methods to evaluate the ethics of AI technologies.',\n",
       "  'Addi-\\ntionally, research challenges and future perspectives are discussed. This article will help researchers to gain a birds eye view of AI\\nethics, and thus facilitate their further investigation and research\\nof AI. Index Terms—Artiﬁcial intelligence (AI), AI ethics, ethical issue,\\nethical theory, ethical principle.',\n",
       "  'I. INTRODUCTION\\nA\\nRTIFICIAL intelligence (AI) [1] has achieved rapid and\\nremarkable development during the last decade. AI tech-\\nnologies such as machine learning (ML), natural language pro-\\ncessing, and computer vision are increasingly permeating and\\nspreading to various disciplines and aspects of our society.',\n",
       "  'AI\\nis increasingly taking over human tasks and replacing human\\ndecision-making.',\n",
       "  'It has been widely used in a variety of sectors,\\nsuch as business, logistics, manufacturing, transportation, health\\ncare, education, state governance, etc. The application of AI has brought about efﬁciency improve-\\nment and cost reduction, which are beneﬁcial for economic\\ngrowth, social development, and human well-being [2]. For\\ninstance, the AI chatbot can respond to clients’ inquiries at\\nany time, which will improve the customers’ satisfaction and\\nthe company’s sales [3]. AI allows doctors to serve patients in\\nremote locations through telemedicine services [4].',\n",
       "  'It is no doubt\\nthattherapiddevelopmentandwideapplicationofAIarealready\\naffecting our daily life, humanity, and society. However, at the same time, AI also poses many signiﬁcant\\nethical risks or issues for users, developers, humans, and society.',\n",
       "  'Over the past few years, many cases in which AI produced\\npoor outcomes have been observed. For instance, in 2016, the\\ndriver of an electric Tesla car was killed in a road accident after\\nits Autopilot mode failed to recognize an oncoming lorry [5]. Microsoft’s AI chatting bot, Tay.ai,was taken down because it\\nbecame racist and sexist only less than a day after she joined\\nTwitter [6].',\n",
       "  'There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].',\n",
       "  'More seriously, AI technology has begun to be\\nused by criminals to harm others or the society. For example,\\ncriminals used AI-based software to impersonate a chief exec-\\nutive’s voice and demand a fraudulent transfer of $243 000 [8]. Therefore, it is urgent and critical to address the ethical issues\\nor risks of AI so that AI can be built, applied, and developed\\nethically. AI ethics or machine ethics [9] is an emerging and interdisci-\\nplinary ﬁeld concerned with addressing ethical issues of AI [10]. AI ethics involves the ethics of AI, which studies the ethical\\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/',\n",
       "  '800\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL.',\n",
       "  '4, NO. 4, AUGUST 2023\\ntheories, guidelines, policies, principles, rules, and regulations\\nrelated to AI, and the ethical AI, that is, the AI that can uphold\\nethical norms and behaves ethically [11]. The ethics of AI is a\\nprerequisite to building ethical AI or to making AI behave in\\nan ethical manner.',\n",
       "  'It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.',\n",
       "  'With\\nappropriate ethics of AI, ethical AI can be built or implemented\\nthrough some methodologies and technologies. Even though AI ethics has been extensively discussed by\\ninterdisciplinary researchers for several years, it is still in its\\ninfancy [11]. AI ethics is a very broad and rapidly develop-\\ning research area that has received increasing attention from\\nresearchers in recent years.',\n",
       "  'Although several review papers\\nhave been published during the past few years, each of them\\nfocuses on a certain aspect(s) of AI ethics, and there is still\\na lack of comprehensive reviews to provide a full picture of\\nthis ﬁeld.',\n",
       "  'For instance, a brief review of ethical issues in AI\\nwas provided in [11], AI ethics guidelines and principles were\\ninvestigated in [12], [13], Mehrabi et al. [14] focused on bias\\nand fairness in ML, García and Fernández [15] only reviewed\\nthe safety in reinforcement learning, Mothukuri et al. [16] re-\\nviewed the security and privacy of federated learning, Liu et\\nal. [17] dedicated to a survey of privacy and security issues in\\ndeep learning, Arrieta et al. [18] concentrated on explainable\\nAI, and Zhang et al.',\n",
       "  '[19] covered the key ethical and privacy\\nissues in AI and traced how such issues have changed over\\nthe past few decades using the bibliometric approach. Thus,\\nthis article is dedicated to presenting a systematic and compre-\\nhensive overview of AI ethics from diverse aspects (or topics),\\nthereby providing informative guidance for the community to\\npractice ethical AI in the future. We hope it will inform sci-\\nentists, researchers, engineers, practitioners, and other relevant\\nstakeholders, and provides sufﬁcient background, comprehen-\\nsive domain knowledge and a bird’s eye view for interested\\npeople, especially for the beginners of this research discipline,\\nso that further investigation and improvement can be pursued\\nby them.',\n",
       "  'The main contributions of this article are as follows. 1) A comprehensive overview of AI ethics, including ethical\\nissues and risks of AI, ethical guidelines and principles\\nfor AI, approaches for addressing ethical issues in AI, and\\nmethods for evaluating ethical AI, is provided in this re-\\nview. This overview can provide a sufﬁcient background,\\ncomprehensive domain knowledge, and a roadmap for\\nresearchers and practitioners. 2) The ethical issues and risks caused by AI are summarized,\\nand a new categorization of AI ethical issues is proposed in\\nSection III. The proposed new categorization is helpful for\\nrecognizing, understanding, and analyzing ethical prob-\\nlems in AI and then developing solutions to solve these\\nproblems. Additionally, the ethical issues associated with\\ndifferent stages of AI system’s lifecycle are discussed. 3) An up-to-date global landscape of the AI ethics guidelines\\nand principles is presented in Section IV, based on 146\\nguidelines related to AI ethics released by companies,\\norganizations, and governments around the world. These\\nguidelines and principles provide a high-level guidance\\nfor the planning, development, production, and usage of\\nAI and directions for addressing AI ethical issues.',\n",
       "  '4) A review of multidisciplinary approaches to addressing\\nAI ethical problems, including ethical, technological, and\\nlegal approaches, is given in Section V. This not only\\nprovides an informative summary about the approaches to\\nethical AI but also suggests potentially different solutions\\nto AI ethical issues from a variety of perspectives rather\\nthan relying solely on technological approaches. 5) Methods for assessing or evaluating AI ethics are reviewed\\nin Section VI.',\n",
       "  'Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics.',\n",
       "  'However, this aspect is often overlooked in\\nthe existing literature.',\n",
       "  'To the best of our knowledge, this\\narticle is the ﬁrst to summarize the aspect of evaluating\\nethical AI. 6) Lastly, some challenges in AI ethics and several future\\nperspectives are pointed out, which provide some research\\nquestions and directions for further research in the future. This will be helpful for interested researchers and practi-\\ntioners to pursue further research in AI ethics ﬁeld.',\n",
       "  'The rest of the article is organized as follows. After this\\nintroductory section, we brieﬂy describe the review scope and\\nmethodology of this article in Section II. A comprehensive\\nsummary of the ethical issues and risks raised from AI is given\\nin Section III. Section IV reviews and analyzes the AI ethical\\nguidelines and principles that have been released during the last\\nfew years. Section V describes the paradigms or approaches\\nfor addressing ethical issues in AI. Section VI discusses the\\napproaches to evaluate the morality or ethics of AI systems or\\nproducts. Section VII outlines the challenges in implementing\\nethics in AI and gives some future perspectives on designing\\nethical AI. Section VIII brieﬂy concludes this article.',\n",
       "  'II. SCOPE AND METHODOLOGY\\nInthis section, weﬁrst clarifytheaspects andtopics coveredin\\nthis review and the links between these topics. Then, we describe\\nthe methodology followed in conducting this survey, including\\nthe literature search strategy and selection criteria.',\n",
       "  'A.',\n",
       "  'Scope\\nThe scope and topics of this article is described as follows. Investigation of ethical issues and risks of AI is the starting\\npoint of this review, since it is because of the existence of\\nethical issues in AI that the research ﬁeld of AI ethics exists. Thus, it is necessary and important to clarify and understand\\nthe ethical problems existed in AI.',\n",
       "  'Then, the ethical guidelines\\nand principles, which direct the development and use of AI,\\nare reviewed. As the ethical issues of AI have attracted more\\nand more attention from various sectors of our society, many\\norganizations (including academia, industry, and governments)\\nhave begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI. After clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI. We',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n801\\nFig. 1.',\n",
       "  'Topics covered in this article and the links between them. covered ethical, technological, and legal approaches, but focus\\nmore on the ﬁrst two kinds of approaches (ethical and tech-\\nnological approaches) since the researchers in AI community\\nmay be more interested in these two categories of approaches. Last but not least, we summarize how to evaluate ethical AI,\\nwhich is to assess the ethicality or morality of AI, i.e., how well\\nthe ethical problems are addressed or whether an AI system\\nmeets the ethical requirements or not.',\n",
       "  'Apparently, these four\\naspects are essential for solving ethical issues in AI. Thus, the\\nabove four aspects constitute the main content of this article and\\nprovide a systematic overview of AI ethics. The topics or aspects\\ncovered in this article and the links between them are illustrated\\nin Fig.',\n",
       "  '1.',\n",
       "  'B. Methodology\\nThis review covers a wide variety of documents, including\\nacademic, organizational, government grey literature sources,\\nand news report.',\n",
       "  'The search of relevant literature was conducted\\nin two phases.',\n",
       "  'In the ﬁrst phase, the entries or keywords that\\nreﬂect different terms related to AI ethics are used to search on\\nGoogle Scholar, Web of Science, IEEE Xplore, ACM Digital\\nLibrary, Science Direct, Springer Link, arXiv, and Google. The\\nentries or keywords used include: (ethics, ethical, responsibility,\\nresponsible, trustworthiness, trustworthy, transparent, explain-\\nable, fair, beneﬁcial, robust, safe, private, sustainable) AND/OR\\n(issues, risks, guideline, principle, approach, method, evalua-\\ntion, assessment, challenge) AND (artiﬁcial intelligence, AI,\\nmachine learning, ML, intelligent system, intelligent agent). We\\nmainly consider the literature published or released since 2010\\nand included as many related keywords as possible in titles. In\\nthesecondphase,wecheckedtherelatedworkofliteraturefound\\nin the ﬁrst phase, such as the cited articles and other work by the\\nsame authors of phase one. As for the ethical AI guidelines, we only collected these\\ndocuments in English (or with ofﬁcial English translations) and\\ncan be visited or downloaded on the internet. A full list with\\nURL links of collected ethical AI guidelines is provided in the\\nSupplementary Materials of this article.',\n",
       "  'III. ETHICAL ISSUES AND RISKS OF AI\\nTo address the ethical problems of AI, we must ﬁrst recognize\\nand understand the potential ethical issues or risks that AI may\\nbring. Then, the necessary AI ethical guidelines, policies, prin-\\nciples, rules (i.e., Ethics of AI) can be formulated appropriately. With the adequate ethics of AI, we can design and build AI\\nthat behaves ethically (i.e., Ethical AI) [8]. The ethical issue\\nof AI generally refers to the morally bad things or problematic\\noutcomes relevant to AI (i.e., these issues and risks that are raised\\nby the development, deployment, and use of AI) that need to be\\naddressed.',\n",
       "  'Many ethical issues, such as lack of transparency,\\nprivacy and accountability, bias and discrimination, safety and\\nsecurity problems, the potential for criminal and malicious use,\\nand so on, have been identiﬁed from the applications and studies. This section focuses on ethical issues and risks of AI.',\n",
       "  'First,\\nfour different categorizations of AI ethical issues in the literature\\nare reviewed in Section III-A. Since these four categorizations\\neither ignore some ethical issues or are too complicated to\\nunderstand, we proposed a new categorization that classiﬁes\\nAI ethical issues into individual, societal, and environmental\\nlevels in Section III-B. Our proposed categorization compre-\\nhensively covers the existing ethical issues and is easy to un-\\nderstand, which is helpful for understanding and analyzing the\\nethical problems caused by AI. Besides, we attempt to map\\nthe ethical issues associated with the stages of AI system’s\\nlifecycle in Section III-C. This would be beneﬁcial for ﬁguring\\nout these issues during the AI system development process. The main goal of this section is to discuss and clarify the\\nethical issues of AI so that practitioners can recognize and\\nunderstand these issues, and then help them to further study\\nhow to address AI ethical issues. The main contribution in\\nthis section is that we proposed a new categorization of AI\\nethical issues, which covers the ethical issues discussed in a clear\\nand easy-to-understand manner. Additionally, the ethical issues\\nassociated with the stages of AI system’s lifecycle is discussed.',\n",
       "  'A.',\n",
       "  'Review of Categorizations of AI Ethical Issues\\nThis section describes the ethical concerns or issues of AI\\nfrom different perspectives by reviewing four different cate-\\ngorizations that were found in our collected literature. Two\\nof them are from government reports and the other two are\\nfrom academic publications.',\n",
       "  'From different perspectives and\\ncategorizations, the ethical issues involved are also somewhat\\ndifferent. In the following, four different categorizations of AI\\nethical issues are reviewed subsequently. The four reviewed cat-\\negorizations of AI ethical issues and our proposed categorization\\nare listed in Table I. 1) Categorization Based on Features of AI, Human Factors\\nand Social Impact: In [11], AI ethical issues are mainly dis-\\ncussed in three categories: ethical issues caused by the features\\nof AI, ethical risks caused by human factors, and social impact\\nof ethical AI issues. a) Ethical issues caused by features of AI: Transparency:\\nML is the core technology of current AI, especially (deep) neural\\nnetworks.',\n",
       "  'However, it is hard to explain and understand the\\ninference procedure of ML, which is commonly known as the',\n",
       "  '802\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL.',\n",
       "  '4, NO. 4, AUGUST 2023\\nTABLE I\\nLIST AND DISCUSSION OF THE REVIEWED CATEGORIZATION OF ETHICAL ISSUES OF AI AND OUR PROPOSED CATEGORIZATION\\n“black-box.” The opacity of ML makes the algorithms or models\\nmysterious to users and even developers. This mainly leads to the\\ntransparency issue [20].',\n",
       "  'The lack of transparency not only leads\\ntotheexplanatoryproblem,butalsoleadstodifﬁcultiesinhuman\\nmonitoring and guidance of ML or AI.',\n",
       "  'Thus, transparency or\\nexplainability is one of the most widely discussed downside of\\nAI. Data Security and Privacy: The performance of current AI\\nstrongly depends on the training data. Usually, a huge amount\\nof data, which probably includes personal data and private\\ndata, is required to train an AI model, particularly the deep\\nlearning model. The misuse and malicious use of data, such as\\n(personal) information leakage or tampering, are serious ethical\\nissues that are closely related to every individual, institution,\\norganization, and even the country.',\n",
       "  'Data security and privacy\\nare key issues encountered in the development and application\\nof AI technology [21]. Autonomy, Intentionality, and Responsibility: With the\\nadvancement of AI, current AI systems or agents, such as health-\\ncare robots, have a certain degree of autonomy, intentionality,\\nand responsibility [22]. Here, the autonomy of AI refers to an AI\\nsystem’s ability to operate without human intervention or direct\\ncontrol. Intentionality refers to the ability that an AI system can\\nact in a way that is morally harmful or beneﬁcial and the actions\\nare deliberate and calculated [11]. Responsibility indicates that\\nthe AI system fulﬁll some social rule and some assumed respon-\\nsibilities. However, how much autonomy, intentionality, and\\nresponsibility should an AI system be allowed is a challenging\\nquestion and issue. b) Ethical issues caused by human factors: Accountabil-\\nity: When an AI system or agent fails in a speciﬁed task and\\nresults in bad consequences, who should be responsible.',\n",
       "  'The\\nundesirable consequence may be caused by many factors, such\\nas the programming codes, input data, improper operation, or\\nother factors. This brings about the so-called “the problem of\\nmany hands” [23].',\n",
       "  'Thus, accountability is an ethical issue that\\nconcerns the human factors involved in the designing, imple-\\nmentation, deployment, and usage of AI. EthicalStandards:AstheultimategoalofAIethicsistocreate\\nethical AI that can follow ethical principles and behave ethically\\n[10], it is crucial to form comprehensive and unbiased ethical\\nstandards for training or regulating AI to be ethical. To formulate\\nethical standards for AI, researchers and practitioners should\\nwell understand the existing ethical theories and principles [13],\\n[24]. Human Rights Laws: The designer, software engineers, and\\nother participants in AI system design and application should be\\ntaught human rights laws [25]. Without training in human rights\\nlaws, they may infringe and breach essential human rights with-\\nout even realizing it.',\n",
       "  'The human rights laws or acts followed by\\ndifferent countries or regions are often different. Many different\\nhuman rights laws, for instance, International Human Rights\\nLaw, International Covenant on Civil and Political Rights, In-\\nternational Covenant on Economic, Social and Cultural Rights,',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n803\\nUniversal Declaration of Human Rights, Charter of the United\\nNations, the European Convention for the Protection of Human\\nRights and Fundamental Freedoms, etc.',\n",
       "  '[26] have been released\\nby different governments. c) Social impact of ethical AI issues: Automation and Job\\nReplacement: As more and more factory workers are being\\nreplaced by automated systems and robots, AI will disrupt and\\ntransform the labor market. Hence, many people worry about\\nautomation and job replacement [27].',\n",
       "  'Accessibility: The accessibility or availability of emerging\\ntechnologies, such as AI, will have a direct impact on human\\nwell-being. However, it will be unethical and unfair if only a\\nportion of the population beneﬁt from AI. Consideration must be\\ngiven to developing AI products and services that are accessible\\nto everyone, and thus the beneﬁts of AI can be spread equally\\nto everyone [28]. Democracy and Civil Rights: Unethical AI will distort the\\ntruthandeventuallyleadtotheloss of trust andpublicsupport for\\nAI technology [11]. The strengths of democracies are harmed by\\nthe loss of informed and trusting communities.',\n",
       "  'As democracies\\nsuffer and structural biases exacerbated, the free enjoyment\\nof civil rights is no longer consistently available to all.',\n",
       "  'Thus,\\ndemocracy and civil rights must be taken into consideration in\\nAI ethics. 2) Categorization Based on Vulnerabilities of AI and Human:\\nIn [29], Liao distinguished the ethical issues of AI into 1) ethical\\nissues that arise because of limitations of current ML systems,\\nwhich is named as “vulnerabilities in AI (especially ML),” and\\n2) ethical issues that arise because current ML systems may be\\nworking too well and humans can be vulnerable in the presence\\nof or interaction with these intelligent systems, which is referred\\nto as “human vulnerabilities.”\\na) Ethical issues from the vulnerabilities of AI: ML is data\\nhungry: Usually, ML requires a large amount of data to work\\nwell [30]. Therefore, this motivates companies and organiza-\\ntions to collect or purchase data, including sensitive personal\\ndata, even if doing so may violate the individual’s right to\\nprivacy. Garbage in/garbage out: The performance of a ML algorithm\\nheavily depends on the data from which it learns. If one ML\\nalgorithm is trained on insufﬁcient or inaccurate data, it will\\nprovide undesirable results even it is well designed [31]. Faulty algorithms: Even if a ML algorithm is input with\\nenough and accurate data, if the algorithm itself is bad, it will\\nalso make bad predictions.',\n",
       "  'For example, a bad ML algorithm\\nmay not be able to recognize a pattern even if there is one or\\nit may recognize a pattern even if there is not one, where are\\nknown as “underﬁtting” and “overﬁtting,” respectively [32]. Deep learning is a black box: Deep learning is a black\\nbox, which raises issues such as explainability, interpretability,\\nand trust [33]. Even for the designers and developers of deep\\nlearning, the model is incomprehensible since it usually involves\\nthousands or millions of connections between different neurons.',\n",
       "  'Therefore, it is difﬁcult to explain how these connections interact\\nand why the model makes certain predictions. b) Ethical issues from the vulnerabilities of human: Abuse\\nof AI: AI technologies, such as facial recognition and image\\ngeneration, can work better than humans [34]. However, ethical\\nissues exist because people may be tempted to use them for\\nill. For instance, a government could use facial recognition\\ntechnology to monitor its citizens, and ML can be used to\\nfabricate photos or videos so realistic that humans cannot tell\\nthat they are fake [35]. This brings the concern about the abuse\\nof AI technologies. Job replacement: Since intelligent robots can perform certain\\ntasks faster and better than humans, many people worry that\\nrobots and other AI technologies will replace a large part of\\ncurrent human labor in the near future [36]. Thus, people may\\nbe in fear of job replacement.',\n",
       "  'Issues about robotic companions: As AI robots become more\\nand more sophisticated, they have begun to be regarded as\\ncompanions of humans. This raises some ethical issues about\\nthe relationship between human and robotic companions [37]. 3) Categorization Based on Algorithm, Data, Application,\\nand Long-Term and Indirect Ethical Risks: In the analysis report\\nof AI ethical risks [38] released by the Chinese National AI\\nStandardization General Working Group, AI ethical issues are\\ncategorized into the following four aspects:\\n1) ethical issues related to AI algorithms;\\n2) ethical issues related to data;\\n3) ethical issues related to the application of AI;\\n4) long-term and indirect ethical risks. a) Ethical issues related to algorithms: Algorithm secu-\\nrity: The AI algorithms pose several security issues.',\n",
       "  'First, there\\nis a risk of algorithm or model leakage [39], [40].',\n",
       "  'Generally, the\\nmodel is achieved by training it on the training data through op-\\ntimizing its parameters. If the model parameters of an algorithm\\nare leaked, a third party may be able to copy the model. This will\\ncause economic loss to the owner of the model, since a third party\\nobtains the same model without paying the cost of obtaining\\nthe training data. Second, the parameters of the AI algorithm\\nmodel may be modiﬁed illegally by an attacker, which will cause\\nthe performance deterioration of the AI model and may lead\\nto undesirable consequences. Additionally, in many scenarios,\\nthe output of the model is closely related to personal safety,\\nsuch as in the medical and autonomous driving ﬁelds.',\n",
       "  'Once\\nthere are loopholes or mistakes in the application of algorithms\\nin these ﬁelds, it will directly harm humans and cause serious\\nconsequences [41]. Algorithm explainability: Due to the black-box characteristic\\nof many ML algorithms [33], especially the popular deep learn-\\ning or neural networks, the decision process of AI algorithms\\nis hard to understand. The interpretability or explainability of\\nalgorithms is an essential ethical issue of AI [42], since it\\nconcerns the human right to know. Algorithmic decision dilemma: After obtaining the AI model,\\nthe result of the algorithm is usually unpredictable for us. In\\nother words, even though we have designed an AI model well,\\nwe cannot foresee or predict the decisions of the algorithm and\\nthe consequence it will produce. This leads to the algorithmic\\ndecision risk or dilemma of AI. For instance, autonomous ve-\\nhicles should reduce trafﬁc accidents, but sometimes they have\\nto choose between two evils, such as crushing pedestrians or\\nsacriﬁcing themselves and passengers to save pedestrians [43].',\n",
       "  '804\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL.',\n",
       "  '4, NO. 4, AUGUST 2023\\nb) Ethical issues related to data: Privacy protection: With\\nthe development of big data and AI, the tension between AI\\ntechnology and user privacy protection has become more and\\nmore serious. Criminals have more ways to obtain personal\\nprivacy data with lower costs and greater beneﬁts. Data security\\nincidents have commonly occurred in recent years.',\n",
       "  'Privacy\\nprotection has become a well-recognized and serious ethical\\nissue involved by using AI [44]. Recognizing and processing personal and sensitive infor-\\nmation: Traditional laws and regulations only focus on the\\nprotection of personal and sensitive information. If the personal\\nor sensitive information is deidentiﬁed [45] through randomiza-\\ntion, data synthesis, and other technologies, it will no longer be\\nregarded as personal or sensitive information and not protected\\nby traditional laws. The subsequent usage, sharing, and transfer\\nof such information arise some ethical issues. c) Ethical issues related to application: Algorithm dis-\\ncrimination: The execution results of algorithms directly affect\\nthe decision-making of AI systems. However, algorithm dis-\\ncrimination or bias has been seen in many applications of AI. For instance, the racial bias in criminal justice systems [46], and\\ngender discrimination in hiring [47]. Algorithm abuse: Algorithm abuse [48] refers to the situation\\nwhere people use algorithms for analysis, decision-making,\\ncoordination, and other activities, but their use purpose, use\\nmethod, use range, etc., have deviations and cause adverse\\neffects. For example, facial recognition algorithms can be used\\nto improve the level of public security and speed up the discovery\\nof criminal suspects, but if they are applied to detect potential\\ncriminals, or to determine whether someone has criminal poten-\\ntial based on their face, it is an algorithm abuse. d) Long-term and indirect ethical risks: Employment:\\nWith the fast advancement and widespread application of AI,\\nmore and more work can be completed by some AI products\\n[27].',\n",
       "  'This will have a signiﬁcant inﬂuence on the employment\\nproblem.',\n",
       "  'Ownership: As AI continues to improve, the intellectual dif-\\nferences between AI agents and humans will gradually shrink. A series of debates on ownership will follow, such as whether\\nthe AI agent should be considered as “legal subject,” whether AI\\nproducts have property rights (copyrights or patent rights) [49],\\nand so forth. Competition: Unfair competition, malicious competition, and\\nmonopolistic behaviors with technological advantages will all\\nhave an impact on social stability and market freedom, fair-\\nness, and equal value, and will seriously damage the interests\\nof consumers and hinder the improvement of social welfare\\n[38]. When companies, organizations or individuals use AI\\nalgorithms, they should follow competitive ethics and not go\\nbeyond legal boundaries.',\n",
       "  'Responsibility: With the widespread application of AI, many\\ncases in which AI products violate the laws or ethics, such\\nas personal injury and algorithmic bias, have been observed. A fundamental problem that arises in these cases is who is\\nresponsible for these bad consequences [50].',\n",
       "  'For example, as\\nautonomous driving involves multiple subjects, such as car\\nowners, drivers, passengers, car manufacturers, autonomous\\ndriving system providers, pedestrians, etc., how should they bear\\nresponsibilities after a trafﬁc accident. 4) Categorization Based on the Deployment of AI: In Euro-\\npean Parliamentary Research Service’s latest study on the ethical\\nimplications and moral questions brought by AI [51], the ethical\\nissues are mapped into different categories according to the ethi-\\ncalimpactsofAIonhumansociety,humanpsychology,ﬁnancial\\nsystem, legal system, environment and the planet, and trust.',\n",
       "  'a) Impact on society: The labor market: AI has already\\nbeen applied in ﬁnance, advanced manufacturing, transporta-\\ntion, energy development, healthcare, and many other sectors. We have already seen the impact of automation on “blue collar”\\njobs. As AI agents or robots become more and more sophis-\\nticated, creative, versatile, and intelligent, more jobs will be\\naffected by AI technologies and more positions will be obsolete. Therefore, AI technologies may put current job classes at risk,\\neliminate positions, cause mass unemployment in many job\\nsectors [36]. Furthermore, discrimination in the labor market\\nmay also be an issue, for instance, people without high-skill\\ntraining will be disproportionately affected by the application\\nof AI. Inequality: AI technologies are expected to enable companies\\nto streamline their business operations and make them more\\nefﬁcient and productive.',\n",
       "  'However, some people argue that this\\nwill come at the expense of their human workforces. Thus, this\\nwill inevitably indicate that revenues will be split across fewer\\npeople and individuals with ownership in AI-driven companies\\nwill receive disproportionate beneﬁts, which indeed increase\\nsocial inequalities [52]. Privacy, human rights, and dignity: AI is already affecting\\nprivacy, human rights, and dignity in many ways. For example,\\nthe intelligent personal assistants (IPA), such as Apple’s Siri,\\nAmazon’s Echo, and Google’s Home, can learn the interests\\nand behavior of their users, but, at the same time, the users\\nraise concerns about the fact that they are always running and\\nlistening in the background [53]. The IPA obviously affects our\\nprivacy.',\n",
       "  'AI has an important impact on democracy and people’s\\nright to private life and dignity. For instance, if AI can be used\\nto determine people’s political beliefs, then individuals may\\nbe vulnerable to manipulation. Political strategists can use this\\ninformation to determine which voters are likely to be persuaded\\nto change party afﬁliation and then use resources to persuade\\nthem to do so. Bias: Human bias, such as gender prejudice and racism bias,\\nmay be inherited by AI.',\n",
       "  'The bias of AI may arise as a result\\nof the training data, the value held by the developers and users,\\nor acquired from the learning process of AI itself. Many cases\\nof AI bias, machine bias or algorithmic bias have been reported\\n[54].',\n",
       "  'The bias of AI will promote unexpected social bias or\\ndiscrimination. Thus, bias is an ethical issue that is often talked\\nabout by the public. Democracy.',\n",
       "  'The implementation and adoption of AI can\\nthreaten democracy in several ways. First, the concentration\\nof technological, economic, and political power related to AI\\namong a few mega corporations could allow them to pose undue\\ninﬂuence over the government. Second, AI may damage democ-\\nracy by affecting political elections [55]. With the aid of AI and',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n805\\nbig data, politicians have access to huge amounts of information\\nthat allow them to target speciﬁc voters and develop messages\\nthat will resonate with them most. Third, the increasing use of\\nAI-based new recommenders, which present readers with news\\nstories based on their previous reading history, reduces readers’\\nchances of encountering different and undiscovered content,\\noptions, and viewpoints [56]. This could result in increasing\\nsocietal polarization. b) Impact on human psychology: Relationships: AI is\\ngetting better and better at imitating human thought, experi-\\nence, action, dialogue, and relationships. In the future, we will\\nfrequently interact with machines or AI products as if they are\\nhumans. This will have impacts on real human relationships and\\nthus bring some ethical issues [57]. Personhood: AI systems are increasingly taking on tasks\\nand decisions that are traditionally performed by humans.',\n",
       "  'An\\nessential and ethical question that arise from this is that whether\\nAI system should be endowed with “personhood” and moral or\\nlegal agency rights [58]. c) Impact on the ﬁnancial system: The application of AI\\nin ﬁnancial markets has signiﬁcantly improved transaction efﬁ-\\nciencyandtradingvolume.Marketsareverysuitableforautoma-\\ntion, because they now operate almost entirely electronically\\nand a huge amount of data is generated at a high rate, which\\nrequires the employment of algorithms to digest and analyze\\nit.',\n",
       "  'Additionally, due to the dynamic of markets, fast reaction to\\ninformation is critical [59], which provides considerable incen-\\ntives to replace slow people’s decision process with algorithmic\\ndecision-making. Furthermore, the rewards for effective trading\\ndecisions are considerable, which explains why companies have\\ninvested so much in AI technology. However, the AI-based automatic trading agents may also be\\nusedmaliciouslytodestabilizethemarketsorharminnocentpar-\\nties in other ways.',\n",
       "  'Even if they are not intended to be malicious,\\nthe autonomy and ﬂexibility of algorithmic trading strategies,\\nincluding the increasing use of ML techniques, make it difﬁcult\\nfor people to predict how they will perform in unexpected\\nsituations. d) Impact on the legal system: Criminal law: According to\\ncurrent criminal law, a crime consists of two elements, that is, a\\nvoluntary act (or omission) and an intention to commit a crime.',\n",
       "  'If\\nAI products or robots are shown to have sufﬁcient consciousness\\nor awareness, thentheymaybethedirect perpetrators of criminal\\noffenses or responsible for negligent crimes. If we admit that AI\\nproducts have their own mind, human-like free will, autonomy,\\nor moral sense, then our criminal law and even the entire legal\\nsystem will have to be revised [60].',\n",
       "  'Tort law: Tort law covers situations such as one person’s\\nbehavior case injury, suffering, unfair loss, or harm to another\\nperson. When an accident involving self-driving car(s) occurs,\\nthere are two legal areas that are relevant—negligence and\\nproduct liability.',\n",
       "  'While, today, most accidents result from driver\\nerror, which indicates that liability for accidents are governed\\nby the negligence principle. So, in the future, the tort law, which\\nincludes many different types of personal injury claims, will be\\nsigniﬁcantlyaffected[61] sinceAI products (suchas self-driving\\ncars or other intelligent robots) will involve in personal injury\\nclaims, such as the accident between self-driving cars or the\\ninjury claim where a robot harm human. e) Impact on the environment and the planet: Use of nat-\\nural resources: The development and application of AI will\\nincrease the demand of many natural resources, such as rare\\nearth metals like nickel, cobalt, graphite, and so on. As the\\nexisting supply decreases, operators may be forced to work in\\nnew and more complex environments to mine. This will increase\\nthe production and consumption rate of rare earth metals, and\\nfurther damage the environment [62].',\n",
       "  'Pollution and waste: The increase in production and con-\\nsumption of AI technological devices such as robots will ex-\\nacerbate pollution and waste, such as the accumulation of heavy\\nmetals and toxic materials in the environment [63].',\n",
       "  'Energy concerns: Employing AI technology, particularly\\ndeep learning, generally involves training ML models on a\\nhuge amount of data, which usually consumes large amounts\\nof energy. According to listed data in [64], the carbon footprint\\nof training a natural language processing model (a Transformer\\nmodel) is roughly 5 times the carbon footprint of an average car\\nacross its entire lifetime.',\n",
       "  'f) Impact on trust: AI promises numerous changes and\\nbeneﬁts to individual’s lives and the society.',\n",
       "  'It is changing\\nour daily lives in many domains, such as transportation, ser-\\nvice industry, healthcare, education, public safety and secu-\\nrity, and entertainment.',\n",
       "  'Nevertheless, these AI systems must\\nbe introduced in ways that foster trust and understanding and\\nrespect human and civil rights [65]. The consensus among the\\nresearch community is that trust in AI can only be achieved\\nthrough fairness, transparency, accountability, and regulation\\n(or control). Fairness: In order to trust AI, it must be fair and impartial.',\n",
       "  'As\\nmore and more decisions are delegated to AI, we must ensure\\nthat these decisions are free from bias and discrimination [66]. Whether it is ﬁltering through CVs for job interviews, deciding\\non admissions to the university, or conducting credit ratings for\\nloan companies, it is essentially vital that decisions made by AI\\nare fair. Transparency: Transparency is important for building trust\\nin AI since it should be a must to know why an AI system\\nmade a particular decision, especially if that decision caused\\nundesirable consequences or harm. In view of the fact that the\\nautopilot of an intelligent car has led to several fatal accidents,\\nit is clear that transparency is urgently needed to discover how\\nand why these accidents occur, and to correct any technical or\\noperational failures. The opacity in ML, which is well-known as\\nblack-box, is one of the main impediments to the transparency of\\nAI [51]. Accountability: Accountability [67] ensures that if an AI\\nsystem makes a mistake or hurts someone, then someone can\\nbe held responsible, whether it is the designer, developer, or\\ncompany selling the AI. In the event of damages, accountability\\nis essential to establish a remedial mechanism so that victims can\\nreceive adequate compensation.',\n",
       "  'Thus, accountability is crucial\\nto ensure the trust of AI.',\n",
       "  'Control: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68]. This is largely related to people’s fear',\n",
       "  '806\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL.',\n",
       "  '4, NO. 4, AUGUST 2023\\nabout the idea of “super-intelligence,” that is, as the intelligence\\nof AI increases to the point that it surpasses human abilities, AI\\nmay come to take control over our resources and outcompete our\\nspecies, and even leading to human extinction. A related concern\\nis that even if an AI agent is carefully designed to align its\\ngoals with human needs, it may develop unpredictable subgoals\\non its own.',\n",
       "  'Therefore, in order to maintain trust in AI, it is\\nimportant that humans must have ultimate oversight or control\\non AI technology.',\n",
       "  'B. Our Proposed Categorization: Ethical Issues At Individual,\\nSocietal and Environmental Levels\\nIn the previous section, we have reviewed the AI ethical issues\\ndescribed and categorized in the literature (see Table I). How-\\never, the above presented categorizations have obvious ﬂaws. Speciﬁcally, the categorization based on features of AI, human\\nfactors, and social impact [11] obviously ignores the impact\\nof AI on the environment, such as natural resource consump-\\ntion and environmental pollution. The categorization based on\\nvulnerabilities of AI and human [29] omits several important\\nissues, such as responsibility, safety, and environmental prob-\\nlems. The categorization based on algorithm, data, application,\\nand long-term and indirect ethical risks [38] misses the con-\\nsiderations of fairness, autonomy and freedom, human dignity,\\nenvironmental problems, etc. Although the categorization based\\non the deployment of AI [51] covers ethical issues comprehen-\\nsively, this classiﬁcation is too cumbersome and some issues,\\nincluding responsibility, safety, and sustainability, are omitted. ThismotivatesustofurtheranalyzeandsortoutAIethicalissues.',\n",
       "  'It is of no doubt that AI systems mainly serve individuals\\nor the public of society. Hence, we can analyze and clarify AI\\nethical issues from individual and societal perspectives. At the\\nsame time, as entities on the planet, AI products will inevitably\\nhave impacts on the environment. So, the ethical issues related to\\nthe environmental aspects also need to be considered. Therefore,\\nin this section, we proposed to classify AI ethical issues at three\\ndifferent levels, that is, ethical issues at individual, societal, and\\nenvironmental levels. Ethical issues at individual level mainly\\ninclude issues that have undesirable consequence for individual\\nhuman beings, their rights, and their well-being [69]. AI ethical\\nissues at societal level consider the societal consequence that AI\\nhas brought or may bring for groups or society as a whole [69]. AI ethical issues at the environmental level focus on the impacts\\nof AI on the natural environment.',\n",
       "  'Our proposed categorization\\nis shown in Fig.',\n",
       "  '2. 1) Ethical Issues at Individual Level: At individual level, AI\\nhas brought inﬂuence on the safety, privacy, autonomy, and hu-\\nmandignityofindividuals.TheapplicationofAIhasposedsome\\nrisks on the safety of individuals. For instance, person injury\\naccidents involving autonomous cars and robots have occurred\\nand reported in the past few years. Privacy issue is one of the\\nserious risks that AI brings to us. To achieve good performance,\\nAI systems usually require a huge amount of data, which often\\ninclude users’ private data. However, there are serious risks\\nassociated with this data collection. One of the main issues is\\nprivacy and data protection. Additionally, as described in the\\nFig.',\n",
       "  '2.',\n",
       "  'Proposed categorization of AI ethical issues. previous section, the application of AI may bring challenges to\\nhuman rights, such as autonomy, and dignity. Autonomy refers\\nto the capacity of thinking, deciding, and acting independently,\\nfreely and without inﬂuence of others [70]. When AI-based\\ndecision-making are widely adopted in our daily life, three is\\nbig danger of restricting the autonomy of us. Human dignity,\\nwhich is one of the principal human rights, is about the right of\\na person to be respected and treated in an ethical manner [71]. The protection of dignity is crucial in the context of AI. Human\\ndignity should be one of the basic concepts for protecting human\\nbeings from harm and should be respected when developing AI\\ntechnologies.',\n",
       "  'For instance, a lethal autonomous weapon system\\n[72] may violate the principle of human dignity. 2) Ethical Issues at Societal Level: When considering the\\nAI ethical issues at societal level, we mainly focus on the\\nbroad consequences and impacts that AI brings for society and\\nthe well-being of communities and nations around the world. Under the categorization of ethical issues at societal level, we\\ndiscuss fairness and justice, responsibility and accountability,\\ntransparency, surveillance and dataﬁcation, controllability of\\nAI, democracy and civil rights, job replacement, and human\\nrelationship. The existence of bias and discrimination in AI has posed\\nchallenges on fairness and justice. The biases and discrimination\\nembedded in AI might increase societal gaps and cause harm to\\ncertain societal groups [70]. For instance, in the US criminal\\njustice system, AI algorithms that are used to assess the risk of\\ncommitting crime has been noticed to exhibit racial bias [73]. Responsibility means being responsible for or in charge of some-\\nthing.',\n",
       "  'Assigning responsibilities to participants is important for\\nshaping the governance of algorithmic decision-making. Based\\non this concept, accountability is the principle that the one who\\nis legally or politically responsible for the damage must provide\\nsome form of justiﬁcation or compensation and is reﬂected by\\nthe liability to provide legal remedies [70]. Thus, mechanisms',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n807\\nshould be established to ensure responsibility and accountability\\nof AI systems and their outcomes both before and after their\\nimplementations. Due to the black-box nature of AI algorithms,\\nlack of transparency has become one of the widely discussed\\nissues.',\n",
       "  'Transparency, i.e., the understanding of how AI systems\\nwork, is crucial for accountability as well. Surveillance and\\ndataﬁcation [74] is one of the common concerns as we live in the\\nso-called digital and intelligent age.',\n",
       "  'Data is collected from users’\\ndaily lives via smart devices, and we live in mass surveillance.',\n",
       "  'As the power of AI has increased quickly, the development of\\nAI systems must have safeguards to ensure the controllability\\nof AI systems by humans.',\n",
       "  'Other previously discussed issues,\\nincluding democracy and civil rights, job replacement, and\\nhuman relationship, also fall into this category. 3) Ethical Issues at Environmental Level: AI ethical issues\\nat environmental level focus on the impacts of AI on the envi-\\nronment and the planet.',\n",
       "  'AI can bring a lot of convenience to\\nour lives and can help us to address some challenges, but it also\\ncomes at a cost to the planet. The widespread application of AI\\noften requires the deployment of a large number of hardware\\nterminal devices, including chips, sensors, storage devices, etc. The production of these hardware consumes a lot of natural\\nresources, especially some rare elements. In addition, at the\\nend of these hardware’s life cycle, they are usually discarded,\\nwhich will cause serious environmental pollution. Another sig-\\nniﬁcant aspect is that AI systems usually require considerable\\ncomputing power, which comes with high energy consumption. Furthermore, from a long-term and global view, the development\\nof AI should be sustainable, i.e., AI technology must meet\\nthe human development goals while simultaneously sustain the\\nability of natural systems to provide the natural resources and\\necosystem services on which the economy and society depend\\n[2]. In summary, natural resource consumption, environmental\\npollution, energy consumption costs, and sustainability involved\\nin the development of AI are the main issues and concerns at the\\nenvironmental level. Our proposed categorization clariﬁes ethical issues from three\\nmain levels, that is, the impact of AI on individual, society, and\\nthe environment. No matter which ﬁeld or sector AI is used\\nin, we can consider the corresponding ethical issues from these\\nthree levels. Obviously, this classiﬁcation method is simple and\\nclear, and it comprehensively covers AI ethical issues.',\n",
       "  'C. Key Ethical Issues Associated With Each Stage of the AI\\nSystem’s Lifecycle\\nAfter reviewing the ethical issues and risks discussed in\\nthe literature, we discuss the ethical issues associated with\\nthe different stages of an AI system’s lifecycle. If we know\\nthe existing ethical problems are prone to be caused by or be\\nraised in which stages or steps of the AI system’s lifecycle, this\\nwill be greatly beneﬁcial for us to eliminate these problems. This\\nis the motivation to discuss the potential ethical issues in each\\nstage of the lifecycle of an AI system. The general lifecycle or development process of an ML-based\\nAI system [75] or product [76] often involves the follow-\\ning stages: business analysis, data engineering, ML modeling,\\nTABLE II\\nETHICAL CONSIDERATIONS ALONG EACH STAGE OF THE AI LIFECYCLE\\nmodel deployment, and operation and monitoring. Usually, the\\nlifecycle of AI products starts from the business analysis, which\\nmainly involves identifying and understanding the business\\nproblem to be solved and business metrics (or criteria of suc-\\ncess). These metrics should include model performance metrics\\nas well as business key performance indicators to be improved\\nby leveraging AI models. The next step is about data engineering\\nthat concerns with data collection, data labeling, data cleaning,\\ndata structuring, feature engineering, and other operations re-\\nlated to data. After this, the process enters into the so-called ML\\nmodeling step. This step generally involves the iterative process\\nof algorithm design or selection, model training, and model\\nevaluation. If the build model is satisfying, then the process\\ngoes to the model deployment step, which makes the ML model\\navailable to other systems within the organization or the web so\\nthat the model can receive data and return their predictions. The\\noperation and monitoring step involves operating the AI system\\nand continuously evaluating its performance and impacts.',\n",
       "  'This\\nstep identiﬁes problems and adjusts or evolves the AI system by\\nreverting to other steps or, if necessary, retiring the AI system\\nfrom production. We attempt to establish a map that links ethical issues with\\nthe stages of AI lifecycle, where the connection means that the\\nethical issue is more likely to occur in a certain step of AI\\nlifecycle, or it is often caused by some reason in this step. This\\nmapping is presented in Table II, where several vital ethical\\nproblems are associated with the ﬁve steps of AI lifecycle. This mapping will be useful for addressing the ethical prob-\\nlem in a proactive fashion during the design process of an\\nAI system.',\n",
       "  '808\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO.',\n",
       "  '4, AUGUST 2023\\nTABLE III\\nNUMBER OF DOCUMENTS ISSUED EACH YEAR FROM 2015 TO 2021\\nIV. ETHICAL GUIDELINES AND PRINCIPLES FOR AI\\nAs the ethical issues of AI have received more and more\\nattention and discussions from various sectors of society, many\\norganizations (including academia, industry, and government)\\nhave begun to discuss and seek the possible frameworks, guide-\\nlines and principles for solving AI ethics issues [78].',\n",
       "  'These\\nguidelines and principles provide useful directions for practicing\\nethical AI. This section is dedicated to giving an up-to-date\\nglobal landscape of the AI ethics guidelines and principles,\\nwhich is achieved through the investigation of 146 reports,\\nguidelines and recommendations related to AI ethics released\\nby companies, organizations, and governments around the world\\nsince 2015. These guidelines and principles provide high-level\\nguidance for the planning, development, production, and usage\\nof AI and directions for addressing AI ethical issues.',\n",
       "  'A. Guidelines for AI Ethics\\nAn excellent survey and analysis of the current principles and\\nguidelines on ethical AI has been given in 2019 by Jobin et al. [12],whoconductedareviewof84ethicalguidelinesreleasedby\\nnational or international organizations from various countries.',\n",
       "  'Jobin et al.',\n",
       "  '[12] found strong widespread agreement on ﬁve key\\nprinciples, that is, transparency, justice and fairness, nonmaleﬁ-\\ncence, responsibility, and privacy, among many.',\n",
       "  'However, many\\nnew guidelines and recommendations for AI ethics have been\\nreleased in the past two years, making Jobin’s paper obsolete\\nbecause many important documents were not included. For\\ninstance, on November 24, 2021, UNESCO (the United Nations\\nEducational, Scientiﬁc and Cultural Organization) adopted the\\nRecommendation on the Ethics of Artiﬁcial Intelligence, which\\nis the ﬁrst ever global agreement on the ethics of AI [79]. To\\nupdate and enrich the investigation on ethical AI guidelines and\\nprinciples, based on the table of ethics guidelines for AI given\\nin Jobin’s paper [12] (only included 84 documents), we have\\ncollected many newly released AI ethical guidelines that are not\\nincluded in Jobin’s review. Finally, a total of 146 AI ethics guide-\\nlines have been collected. A list of all the collected guidelines or\\ndocuments is given in Table V of the Supplementary Materials. The number of guidelines issued each year from 2015 to 2021 is\\ncounted and listed in Table III. It is apparent that the majority of\\nthe guidelines are released in the last ﬁve years, i.e., from 2016\\nto 2020. The number of guides published in 2018 was the largest,\\nwith 53, accounting for 36.3% of the total number. Additionally,\\nthe number of AI guidelines issued by each country is listed in\\nTable IV. Furthermore, the percentages of guidelines released\\nby different types of issuers (including government, industry,\\nacademia, and other organizations) are shown in Fig. 3.',\n",
       "  'It can\\nbe seen from Fig. 3 that governments, companies, and academia\\nall have shown strong concerns about AI ethics. TABLE IV\\nNUMBER OF GUIDELINES ISSUED BY EACH COUNTRY OR REGION\\nFig.',\n",
       "  '3. Percentage of guidelines released by different types of issuers.',\n",
       "  'B. Principles for AI Ethics\\nThe ethical principles that are featured in the collected 146\\nguidelines are listed in Table I of the Supplementary Materials. Accordingtothetable,thereisanobviousconvergenceemerging\\naround ﬁve important ethical principles: transparency, fairness\\nand justice, responsibility, nonmaleﬁcence, and privacy. The 11\\nethical principles identiﬁed in the existing AI guidelines are\\ndescribed and explained in the following. 1) Transparency: Transparency is one of the most widely\\ndiscussed principles in the AI ethics debate. The transparency\\nof AI mainly involves the transparency of the AI technology\\nitself, and the transparency of the developing and adopting of\\nthe AI [13]. On one hand, transparency of AI involves the\\ninterpretability of a given AI system, that is, the ability to\\nknow how and why a model performed the way it did in a\\nspeciﬁc context and thus to understand the rationale behind\\nits decision or behavior. This aspect of transparency is usually\\nmentioned as the metaphor of “opening the black box of AI.”\\nIt concerns interpretability, explainability, or understandability. On the other hand, transparency of AI includes the justiﬁability\\nor rationality of the design and implementation process of the\\nAI system and that of its outcome.',\n",
       "  'In other words, the design\\nand implementation process of the AI system and its decision or\\nbehavior must be justiﬁable and visible. 2) Fairness & Justice: The principle of justice and fairness\\nstates that the development, deployment, and use of AI must\\nbe just and fair so that the AI system should not result in\\ndiscriminations or bias against individuals, communities, or\\ngroups [80]. Discrimination and unfair outcomes brought by AI\\nalgorithms have become a hot topic in the media and academia. Consequently, fairness and justice principle has attracted con-\\nsiderable attention during the last few years.',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n809\\n3) Responsibility and Accountability: The principle of respon-\\nsibility and accountability requires that AI must be auditable,\\nthat is, the designers, developers, owners, and operators of AI\\nare responsible and accountable for an AI system’s behaviors\\nor decisions, and are therefore considered responsible for harms\\nor bad outcomes it might cause [51]. The designers, builders,\\nand users of AI systems are stakeholders in the moral or ethical\\nimplications of their use, misuse, and behavior, and they have the\\nresponsibility and opportunity to shape these implications.',\n",
       "  'This\\nrequires that appropriate mechanisms should be established to\\nensure responsibility and accountability for AI systems and their\\nresults, both before and after their development, deployment,\\nand use. 4) Nonmaleﬁcence: The nonmaleﬁcence basically means to\\ndo no harm or avoid imposing risks of harm to others [81], [82]. Thus, the nonmaleﬁcence principle of AI generally refers to\\nthat AI systems should not cause or exacerbate harm to humans\\nor adversely affect human beings. This entails the protection\\nof human dignity as well as mental and physical integrity. The nonmaleﬁcence principle requires that AI systems and the\\nenvironments in which they operate must be safe and secure so\\nthat they are not open to malicious use. With some of the fatal\\naccidents coming from autonomous cars and robots, avoiding\\nharmtohumanbeingsisoneofthegreatestconcernsinAIethics. Hence, most of the ethical guidelines put a strong emphasis\\non ensuring no harm to human beings through the safety and\\nsecurity of AI. 5) Privacy: The privacy principle aims to ensure respect for\\nprivacy and data protection when using AI systems.',\n",
       "  'AI systems\\nshould preserve and respect privacy rights and data protection as\\nwell as maintain data security. This involves providing effective\\ndata governance and management for all data used and generated\\nbytheAIsystemthroughoutitsentirelifecycle[83].Speciﬁcally,\\ndata collection, usage and storage must comply with laws and\\nregulations related to privacy and data protection.',\n",
       "  'Data and\\nalgorithms must be protected against theft.',\n",
       "  'Once information\\nleakage occurs, employers or AI providers need to inform\\nemployees, customers, partners, and other relevant individuals\\nas soon as possible to minimize the loss or impact caused by the\\nleakage. 6) Beneﬁcence: The principle of beneﬁcence states that AI\\nshall do people good and beneﬁt humanity [82].',\n",
       "  'This principle\\nindicates that AI technology should be used to bring beneﬁcial\\noutcome and impact to individuals, society, and the environment\\n[84]. When developing an AI system, its objectives should be\\nclearly deﬁned and justiﬁed.',\n",
       "  'The use of AI technology to help\\naddress global concerns should be encouraged, such as using AI\\nto help us to handle food security, pollution, and contagion like\\nAIDS and COVID 19. 7) Freedom and Autonomy: Freedom and autonomy, which\\ngenerally refers to the ability of a person to make decisions\\nrespect to his goals and wishes, is the core value for citizens\\nin democratic societies. Therefore, it is important that the use\\nof AI does not harm or encumber the freedom and autonomy\\nfor us.',\n",
       "  'When we apply AI agents, we are willing to give up\\npart of our decision-making authority to AI machines. Thus,\\nupholding the principle of freedom and autonomy in the context\\nof AI means to strike a balance between the decision-making\\npower we maintain for ourselves and that which we cede to\\nAI [84]. 8) Solidarity: The solidarity principle entails that the devel-\\nopment and application of an AI system must be compatible\\nwith maintaining the bounds of solidarity among people and\\ngenerations.',\n",
       "  'In other words, AI should promote social security\\nand cohesion, and should not jeopardize social bonds and rela-\\ntionships [13]. 9) Sustainability: Due to climate change and ongoing envi-\\nronmental damage, the importance of sustainability has received\\nmore and more attention. Like other ﬁelds and disciplines, AI\\nis affected and needs to be included in the sustainable devel-\\nopment agenda. The sustainability principle represents that the\\nproduction, management, and implementation of AI must be\\nsustainable and avoid environmental harm. In other words, AI\\ntechnology must meet the requirements of ensuring the contin-\\nued prosperity of mankind and preserving a good environment\\nfor future generations [85]. AI systems promise to help tackling\\nsome of the most pressing societal concerns, but it must be\\nensured that this happens in the most environmentally friendly\\nway possible. 10) Trust: Trustworthiness is a prerequisite for people and\\nsocieties to adopt AI, since trust is a basic principle for in-\\nterpersonal interactions and social operation. The trust in the\\ndevelopment, deployment and use of AI systems is not only\\nrelated to the inherent characteristics of the technology, but also\\nrelated to the quality of the socio-technical system involving\\nAI applications. Therefore, moving toward trustworthy AI not\\nonly concerns the trustworthiness of the AI system itself, but\\nalso requires a holistic and systematic approach that covers the\\ntrustworthiness of all participants and processes that are the\\nentire life cycle of the system [86]. 11) Dignity: Human dignity encompasses the belief that all\\npeople possess an intrinsic value that is tied solely to their\\nhumanity, i.e., it has nothing to do with their class, race, gender,\\nreligion, abilities, or any other factor other than them being\\nhuman, and this intrinsic value should never be diminished,\\ncompromised, or repressed by other people nor by technologies\\nlike AI. It is important that AI should not infringe or harm the\\ndignity of end-users or other members of society. As a result,\\nrespecting human dignity is an important principle that should\\nbe considered in AI ethics. AI system should hence be developed\\nin a way that respects, supports, and protects people’s physical\\nand mental integrity, personal and cultural sense of identity, and\\nsatisfaction of their basic needs [13].',\n",
       "  'V.',\n",
       "  'APPROACHES TO ADDRESS ETHICAL ISSUES IN AI\\nThis section reviews the approaches to address or mitigate\\nethical issues of AI. As AI ethics is a broad and multidisciplinary\\nﬁeld, we attempt to provide a comprehensive overview of the\\nexisting and potential approaches for addressing AI ethical\\nissues, including ethical, technological, and legal approaches,\\nrather than solely focusing on technological approaches that are\\nof interest to the ﬁeld of AI/ML community. This review of\\nmultidisciplinary approaches for addressing AI ethical problems',\n",
       "  '810\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO. 4, AUGUST 2023\\nFig.',\n",
       "  '4.',\n",
       "  'Branches of ethical theories [91]. not only provides an informative summary about the approaches\\ntoethicalAIbutalsosuggeststheresearchersinAIcommunityto\\nseek solutions to AI ethical issues from a variety of perspectives\\nrather than relying solely on technological approaches. As AI\\nethical issues are complex with multidisciplinary problems, it\\nmay be possible to solve these problems effectively only through\\nthe cooperation of different methods. Ethical approaches dedicate to developing ethical AI systems\\nor agents, which are able to reason and act ethically according\\nto ethical theories [87], by implementing or embedding ethics\\nin AI. Technological approaches are designed to develop new\\ntechnologies (especially ML technologies) to eliminate or mit-\\nigate the shortcomings of current AI. For instance, research on\\nexplainable ML intends to develop new approaches to explain\\nthe reason and work mechanism of ML algorithms. Fair ML\\nstudies techniques that enable ML to make fair decisions or\\npredictions, that is, to reduce the bias or discrimination of ML. Legal approaches intend to regulate or govern the research,\\ndeployment, application, and other aspects of AI through leg-\\nislation and regulation, with the goal of avoiding previously\\ndiscussed ethical issues.',\n",
       "  'A.',\n",
       "  'Ethical Approaches: Implementing Ethics in AI\\nDesigning ethical AI systems, which can reason and act\\nethically, demands the understanding of what ethical behavior\\nis. This involves judgments of right and wrong, good and bad,\\nas well as matters of justice, fairness, virtue, and other ethical\\nprinciples. Thus, ethical theories, which are concerned with\\nconcepts of right and wrong behavior, are closely related to AI\\nethics. This section is dedicated to approaches for implementing\\nethics into AI systems based on the existing ethical theories.',\n",
       "  'First, ethical theories, particularly the normative ethics which\\nare relevant to AI ethics, are reviewed. Then, three main types\\nof approaches for designing ethical AI systems are summarized.',\n",
       "  '1) Ethical Theories: The ﬁeld of ethics (also known as moral\\nphilosophy) is concerned with systematizing, defending, and\\nrecommending concepts of right and wrong behavior. Ethics\\nfocus on judging and determining which action would be good\\nor moral in given circumstances [88]. The philosophical study\\nof ethics usually includes three main subject areas: metaethics,\\nnormative ethics, and applied ethics [89]. The branches of ethical\\ntheories are shown in Fig.',\n",
       "  '4. 1) Metaethics investigates the nature, scope, and meaning\\nof ethical principles or moral judgment. It consists in the\\nattempt to understand the meaning and the origin of ethical\\nterms, the role of reason in ethical judgements, and the\\nissues of universal truths or human values [90]. 2) Normative ethics seeks to arrive at moral standards and\\nrules that regulate right and wrong behavior.',\n",
       "  'That is, it\\naims to establish a set of rules that govern human behavior\\nor how things should be by examining how humans value\\nthings and judge right from wrong or good from bad. 3) Applied ethics is the ethics of particular application ﬁelds,\\nwhich consists of the analysis of speciﬁc, controversial\\nmoral issues, such as abortion, capital punishment, animal\\nrights, environmental concerns, nuclear war, etc. a) Normative ethics: Normative ethics is particularly per-\\ntinent to understanding and applying ethical principles to the\\ndesign, deployment, and usage of AI systems [89] since it is\\na normative practical philosophical discipline that concerned\\nwith how humans or agents should act toward others. Three\\nnormative ethical branches, that is, virtue, deontological, and\\nconsequentialist ethics, are presented and summarized below. Virtue ethics: Virtue ethics emphasizes the virtues or moral\\ncharacter and stresses the importance of cultivating good habits\\nof character, such as benevolence [92]. Hence, virtue ethics\\nfocuses on the agent’s intrinsic character rather than the conse-\\nquences of actions conducted by the agent. Virtue ethics deﬁnes\\nthe action of an agent as morally good if the agent acts and thinks\\naccording to some moral values [93]. In other words, according\\nto virtue theories, an agent is ethical if it manifests some moral\\nvirtues through its actions [94], [95]. Deontological ethics: Deontological theories, which are\\nsometimes called duty theories, judge the morality of an action\\nusing certain moral rules that serve as foundational principles\\nof obligation. Deontology is a kind of normative ethics theory\\nregarding which choices or actions are morally required, forbid-\\nden, or permitted. In other words, deontology is a moral theory\\nthat guides and assesses our decisions about what we ought to\\ndo [96].',\n",
       "  'Deontologists deﬁne a morally good action as one that\\nadheres to some obligations, which may be applicable moral\\nrules or duties, regulations, and norms. There are three main schools of deontological theories, that is,\\nagent-centered, patient-centered (also called victim-centered),\\nand contractarian deontological theories.',\n",
       "  'Agent-centered deon-\\ntological theories place the agent at the center and focus on\\nagent-relative duties.',\n",
       "  'Patient-centered deontological theories, as\\ndistinguished from agent-centered deontology, are rights-based\\nrather than duty-based. It focuses on the rights of patients or\\npotential victims, such as the right of not be used as a means to an\\nend by someone else. Contractualist deontological theories are\\ndifferentfrombothagent-centeredandpatient-centeredtheories.',\n",
       "  'In contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be\\nforbidden by principles that such people could not “reasonably\\nreject” [96]. Consequentialist ethics: Consequentialist ethics, as its name\\nsuggests, emphasizes the utilitarian outcomes of actions [97].',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n811\\nTABLE V\\nCOMPARISON OF THE THREE NORMATIVE ETHICAL THEORIES [126]\\nConsequentialist ethics assess the morality of an action solely\\non the basis of its outcome or consequences. In other words, in\\nconsequentialist theories, the ethical correctness of an action\\nis determined according to the action’s outcome or results.',\n",
       "  'According to consequentialist, an action is morally right if the\\nconsequence of that action is viewed as beneﬁcial, i.e., more\\nfavorable than unfavorable. Suppose a simple case where one\\nfaces with a choice between several possible actions, conse-\\nquentialism speciﬁes the morally right action is the one with the\\nbest overall consequences. Consequentialist ethics is a historically important and still\\npopular theory because it embodies the basic intuition that what\\nis good or right is whatever makes the world best in the future\\nsince we cannot change the past.',\n",
       "  'Consequentialist theories can\\nbe divided into the following [98], [99]. 1) Ethical Egoism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable only to the agent executing the action. 2) Ethical Altruism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable to everyone except the agent. 3) Utilitarianism states that an action is morally good if the\\nconsequences or effects of that action are more favorable\\nthan unfavorable to everyone. All three of these theories focus on the consequences of\\nactions for different groups of people. But, like all normative\\ntheories, the above three theories are rivals of each other.',\n",
       "  'They\\nalso yield different conclusions.',\n",
       "  'b) Summaryonnormativeethics: Itisclearfromtheabove\\ndescriptions that different normative ethical theories will result\\nin different judgement for an action or decision. Consider the\\nfollowing illustration [100]: An elderly gentleman is tormented\\nby a group of arrogant teenagers on the subway and a resolute\\nwoman comes to his aid.',\n",
       "  'The virtue ethicist will deem her\\naction morally appropriate since it instantiates the virtues of\\nbenevolence and courage.',\n",
       "  'The deontologist will consider her\\naction commendable as it is in conformity with the rule to\\nhelp those in need.',\n",
       "  'The consequentialist will defend her action\\nas good, since she maximized the overall well-being of all\\nparties involved—the elderly gentleman is spared suffering and\\ndisgrace, which surpasses the teenagers’ amusement. A brief\\ncomparison between three normative ethical theories is given in\\nTable V.',\n",
       "  '2) Approaches for Implementing Ethics in AI: In the previ-\\nous section, we have discussed the ethical theories relevant to\\nAI ethics.',\n",
       "  'This section brieﬂy reviews the methodologies and\\napproaches to implement ethics in AI systems, i.e., to design\\nethical AI systems. The existing methodologies or approaches\\nfor implanting ethics in AI can be divided into three main\\ntypes: top-down approaches, bottom-up approaches, and hybrid\\napproaches [101]. a) Top-down approaches: A top-down approach refers to\\nany approach that adopts a speciﬁc ethical theory and analyzes\\nits computational requirements to guide the design of algorithms\\nand subsystems that can realize that theory [102].',\n",
       "  'Top-down\\napproaches conduct ethical reasoning based on given ethical\\ntheories or moral principles. In top-down approaches, the moral\\nprinciples and ethical theories are used as rules to select ethically\\nappropriate actions [101] or are used to describe what the AI\\nagent ought to do in a speciﬁc situation. Thus, a top-down ap-\\nproach requires formally deﬁned rules, obligations, and rights to\\nguide the AI agent in its decision-making process. For instance,\\nAsimov’s three laws of robotics [103] that governed the behavior\\nof robots can be considered a top-down ethic system for robots\\n[101]. Many other implementations using top-down approaches\\ncan be found in [104]–[111] and so forth.',\n",
       "  'Top-down approaches are usually understood as having a\\nset of rules that can be transformed into an algorithm.',\n",
       "  'These\\nrules specify the duties of an agent or the need for the agent\\nto evaluate the consequences of the various possible actions it\\nmight take.',\n",
       "  'Top-down approaches differ in the ethical theory\\nthat is used. For instance, when consequentialist theory is used\\nin top-down approach, the reasoning model needs to evaluate\\nthe outcome or consequence of the actions as the basis for the\\ndecision, that is, an action that leads to good result is moral\\nand otherwise is unmoral; whereas if deontological theory is\\napplied, the reasoning model will consider the satisfaction of\\na given value for decision-making, i.e., an action obeying the\\nduties is moral and the one breaking the duties is immoral. b) Bottom-up approaches: The bottom-up approaches as-\\nsume that ethical or moral behavior is learned from observations\\nof the behaviors of others. In bottom-up approach, the emphasis\\nis put on creating an environment in which an AI agent explores\\nthe course of action and the morally praiseworthy action is\\nrewarded or selected [101]. Unlike top-down approaches, which\\nrequire ethical theories or principles to deﬁne what is and is not\\nmoral, ethical principles is discovered or learned from obser-\\nvations or experience in bottom-up approaches. This approach\\nhighlight that AI agent need to learn norms and morality, like\\nlittle children do, in order to become ethically competent. For\\ninstance, Honarvar and Agaee proposed the Casuist BDI-Agent\\n[112] which combine case-based reasoning method in AI and\\nbottom-up casuist approach in ethics to add the capability of\\nethical reasoning to belief-desire-intention (BDI)-Agent [113].',\n",
       "  '812\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL.',\n",
       "  '4, NO.',\n",
       "  '4, AUGUST 2023\\nOther implementations of bottom-up approaches can be found\\nin [114]–[118], etc. Bottom-up approaches can harness the wisdom of the crowd\\nas a means to inform the ethical judgment of the agent and\\nthen the agent can learn how to judge the morality of its action\\nand thus behave ethically.',\n",
       "  'Apparently, bottom-up approaches\\nassume that a sufﬁciently large amount of data or observations\\nabout ethical decisions and their outcomes can be collected from\\na suitable set of subjects or scenarios. This is the requirement for\\nusing bottom-up approaches to implement ethical AI systems.',\n",
       "  'However, in practice, this requirement is not easily satisﬁed. c) Hybrid approaches: The hybrid approach attempts to\\ncombine the advantages of top-down and bottom-up approaches. The top-down approaches make use of the ethical theories and\\nprinciples and emphasize the importance of explicit ethical\\nconcerns that arise from outside of the entity (the moral subject). While the bottom-up approaches focus more on the cultivation of\\nmorality that arise from within the entity through evolution and\\nlearning.',\n",
       "  'Both the top-down and bottom-up approaches embody\\ndifferent aspects of the moral sensibility. By combining these\\napproaches, we may be able to create AI agent that can maintain\\nthe dynamic and ﬂexible morality of bottom-up approach while\\nobeying the top-down principles. Different hybrid approaches\\nhave been implemented in [119]–[124]. As Gigerenzer [125] stated the nature of moral behavior\\nresults from the interplay between mind and environment. Ac-\\ncording to this view, both nature and nurture are important in\\nshaping the moral behavior.',\n",
       "  'The hybrid approach is consistent\\nwith this concept. In hybrid approach, the top-down approach\\nuses programmed rules and the bottom-up approach learned\\nrulesfromcontextobservationsorexperiences,whicharesimilar\\nto the nature and nurture aspects for morality, respectively. From\\nthis perspective, thus, both nature and nurture are considered in\\nhybrid approaches. d) Remarks on ethical approaches: The top-down ap-\\nproach instantiates the speciﬁed ethical theories and principles\\ninto ethical decision-making or converts given ethical theories\\nand principles into algorithms. The top-down approach is suit-\\nable for the design and realization of ethical AI agents with\\nknown ethical principles and ethical codes. The advantage of the\\ntop-down approach is that, based on preset ethical theories and\\nrules, the decisions and actions of ethical agents are predictable,\\nand the ethical norms or rules implemented through program\\ncodes or other means can be understood during ethical decision-\\nmaking process. Therefore, the credibility of the ethical AI agent\\ncreated by the top-down approach can be better guaranteed,\\nand its decision-making process has strong interpretability and\\ntransparency. The disadvantage of the top-down approach is that\\nthe ethical agent adopts predetermined ethical theories or ethical\\nrules, when making decisions in a complex and changeable\\nenvironment, this method lacks ﬂexibility and adaptability. The bottom-up approach emphasizes that ethical agents learn\\nmorality autonomously from the social environment, gradually\\npossess ethical reasoning and moral abilities, and can adapt to\\nenvironmental changes. The bottom-down approach is suitable\\nfor the design and implementation of ethical AI agents without\\nclear ethical theories and guidelines. The advantage of the\\ntop-down approach is that the agent can develop and evolve\\nthrough continuously learning, so as to adapt to environmental\\nchanges. This category of approaches has good adaptability and\\nﬂexibility, and it is possible to construct different and new ethical\\ntheories or guidelines for various application scenarios. The\\ndisadvantage of the top-down approach is that due to the lack\\nof guidance of ethical theories or rules, the decision-making\\nprocess of ethical AI agents has a certain degree of blind obedi-\\nence, and it is difﬁcult to complete the training in a short time\\nand make appropriate ethical decisions.',\n",
       "  'At the same time, it is\\ndifﬁcult to guarantee the interpretability and transparency of the\\ndecision-making process of the designed ethical AI agents. The hybrid approach combines the advantages of top-down\\nand bottom-up approaches and overcomes the shortcomings of\\nthe two methods to a certain extent. If a single approach (top-\\ndown or bottom-up) does not cover the requirements, a hybrid\\napproach is considered necessary and promising.',\n",
       "  'However, the\\nmain challenge is to properly combine the features of top-down\\nand bottom-up approaches. The features of the three approaches\\nfor implementing ethics in AI are summarized and listed in\\nTable VI.',\n",
       "  'B. Technological Approaches\\nIn this section, we brieﬂy summarize the research status about\\ntechnological approaches to address ethical issues of AI in line\\nwith the principles discussed in Section IV-B. Currently, the\\ntechnological approaches to mitigate the associate issues are\\nstill at infant development stage.',\n",
       "  'In recent years, AI research\\ncommunities have put certain efforts for addressing the issues\\nof AI ethics. For instance, ACM (the Association for Comput-\\ning Machinery) has held the annual ACM FAccT conference\\n(which brings together researchers and practitioners interested\\nin fairness, accountability, and transparency in socio-technical\\nsystems) since 2018, AAAI (the Association for the Advance-\\nment of Artiﬁcial Intelligence) and ACM have established the\\nAAAI/ACM Conference on Artiﬁcial Intelligence, Ethics, and\\nSociety (AIES) since 2018, and the 31st International Joint\\nConference on Artiﬁcial Intelligence and the 23rd European\\nConference on Artiﬁcial Intelligence (IJCAI-ECAI 2022) pro-\\nvides a special track on “AI for good.”\\nThe existing work, to the best of our knowledge, mainly\\nfocuses on a few major and key issues and principles, and the\\nother issues and principles are rarely involved. Thus, we only\\ngive a brief summary on technological approaches that involve\\nthe ﬁve key ethical principles.',\n",
       "  'Particularly, for ﬁve key principles\\n(i.e., transparency, fairness and justice, nonmaleﬁcence, respon-\\nsibility and accountability, and privacy), some representative\\nresearch topics and relevant references are listed in Table II of\\nthe Supplementary Materials. Explainable AI (XAI), which is also known as interpretable\\nAI, is currently the main research direction and technical method\\nto address the issues of lack of transparency in AI.',\n",
       "  'The goal of\\nXAI is to allow human users to comprehend the results and\\noutput provided by an AI system, especially by ML algorithms. Christophetal.[128]presentedabriefhistoryoftheﬁeldofXAI,\\ngiven an overview of state-of-the-art interpretation methods, and',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n813\\nTABLE VI\\nFEATURES OF THE THREE APPROACHES FOR IMPLEMENTING ETHICS IN AI [127]\\ndiscussed some research challenges. Additionally, Christoph has\\nwritten a book about interpretable ML [129], which is a popular\\npublication in XAI ﬁeld. As for the fairness principle, there are also many works\\ndedicated to eliminating or mitigating the bias or discrimination\\nexhibited by AI systems, particularly in ML. Fair AI [130],\\nwhich aims at preventing disparate harm (or beneﬁt) to different\\nsubgroups, is a very active research topic that devote to address-\\ning the issues of the lack of fairness in AI. In the survey of\\nfairnessinMLbySimonandChristian[131],differentschoolsof\\nthought and approaches to mitigate biases and increase fairness\\nin ML were reviewed.',\n",
       "  'Nonmaleﬁcence principle includes several codes, such as\\nsafety, security, and robustness.',\n",
       "  'Hence, there are some works\\nfor each of the codes associated with nonmaleﬁcence principle. Currently, safe AI, secure AI, and robust AI are three main\\nresearch directions to fulﬁll the nonmaleﬁcence principle in\\nAI. Interested readers can get more details through relevant\\nreferences listed in Table II of the Supplementary Materials.',\n",
       "  'As AI is widely used in our lives, responsible AI is becoming\\ncritical.',\n",
       "  'Responsibility is a relatively abstract and broad con-\\ncept. At present, there is no universal and uniﬁed deﬁnition or\\nnotion for responsible AI, which mainly involves accountability,\\nliability, fairness, robustness, and explainability [132].',\n",
       "  'Dorian\\net al. [133] proposed two frameworks for responsible AI by\\nintegrating ethical analysis into engineering practice in AI. Besides, paper [134] provides a systematic introduction about\\nresponsible AI. In order to handle the privacy issues in AI, researchers have\\nmade many efforts.',\n",
       "  'Differential privacy [135] is one of the\\nmain approaches to privacy-preserving ML and data analysis. Recently, a new ML paradigm, that is, Federated learning [136],\\n[137] (also called distributed ML), was proposed to mitigate\\nthe risk of privacy leakage in ML. In addition, some other\\nprivacy-preserving techniques for ML [138], [139] have been\\nproposed. As for the other principles, such as beneﬁcence, freedom and\\nautonomy, dignity, and so forth, we have not found relevant\\ntechnological approaches in the literature.',\n",
       "  'This may be due to the\\ndifﬁculty or unsuitability of using technical methods to address\\nthe issues related to these principles. In general, AI ethics is a\\nrelatively new area and approaches for fulﬁlling these principles\\nstill need to be studied in the future.',\n",
       "  'C. Legal Approaches: Legislation and Regulation\\nDue to the increasingly employment of AI technologies in\\nmany sectors and the exhibition of ethical issues and risks\\nin applications of AI, many laws and regulations have been\\nestablished by governments and organizations to govern the\\ndevelopment and application of AI. Legal approaches have\\nbecome one type of the means to address ethical issues in\\nAI.',\n",
       "  'In the following, we list several laws and regulations as-\\nsociated with AI that have been proposed during the past few\\nyears. 1) In 2016, European Parliament and Council of the Euro-\\npean Union (EU) has published the General Data Protec-\\ntion Regulation [140], which is a regulation in EU law on\\ndata protection and privacy in European Union and the\\nEuropean Economic Area. 2) In 2017, USA passed the bill “Safely Ensuring Lives\\nFuture Deployment and Research in Vehicle Evolution\\nAct” [141] for ensuring the safety of highly automated\\nvehicles by encouraging the testing and deployment of\\nsuch vehicles. 3) In 2018, Brazil enacted Law No. 13 709, the General Data\\nProtection Law (Lei Geral de Proteção de Dados) [142],\\nfor the protection of personal data in the country. 4) In 2021, the European Commission released the AI Act\\n[143], which sets out a cross-sectoral regulatory approach\\nto the use of AI systems across the EU and its market.',\n",
       "  'VI.',\n",
       "  'METHODS TO EVALUATE ETHICAL AI\\nThe goal of the discipline of AI ethics is to design ethical AI\\nsystems to behave ethically or adhere to the ethical and moral\\nprinciples and rules. How to evaluate or assess the ethicality\\nor morality (moral competence) of the designed ethical AI is\\ncrucial and necessary, because the designed AI systems need to\\nbe tested or evaluated whether an AI system meets the ethical\\nrequirements or not before deployment.',\n",
       "  'However, this aspect\\nis often ignored or overlooked in the existing literature. This\\nsection reviews three types of approaches, testing, veriﬁcation,\\nand standards, for evaluating the ethics of AI.',\n",
       "  'A. Testing\\nTesting is a typical method used to evaluate the ethical ca-\\npabilities of an AI system. Usually, when testing a system, the\\noutput of the system needs to be compared against a ground truth\\nor the expected output [100]. This section focuses on testing\\napproaches to evaluate ethical AI. 1) Moral Turing Test: In both ethical theories and daily\\ndiscussions about ethics, people usually hold different opinions\\non the morality of various actions. For instance, Kant claimed\\nthat lying is always immoral regardless of the consequence. Utilitarian ethicists would deny this and hold that lying is\\njustiﬁed as long as its consequences are sufﬁciently good in the',\n",
       "  '814\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO.',\n",
       "  '4, AUGUST 2023\\naggregate. Since different ethical theories have different evalu-\\nation standards for moral behavior, Allen et al. [144] proposed\\nto use the Moral Turing Test (MTT) to evaluate artiﬁcial moral\\nagents. In the standard version of Turing Test [145], a remote human\\ninterrogator is charged with distinguishing between a machine\\n(a computer) and a human subject based on their replies to\\nvarious questions posed by the interrogator. A machine passes\\nthe Turning Test if it is misidentiﬁed as the human subject with\\na sufﬁciently high chance, and the machine is considered as an\\nintelligent and thinking entity.',\n",
       "  'Turning Test directly conducts\\nbehavioral test so that it bypasses the disagreement about criteria\\nfor deﬁning intelligence or successful acquisition of natural\\nlanguage. The moral turning test (MTT) was similarly proposed\\nto bypass disagreements about ethical standards by restricting\\nthe conversations in the standard Turning Test to questions\\nrelated to morality. If the human interrogator cannot distinguish\\nthe machine from the human subject at a level above chance, the\\nmachine is a moral agent. However, Allen et al.',\n",
       "  '[144] admitted that one limitation of\\nMTT is that it emphasizes the ability of machines to articulate\\nmoral judgments clearly. Deontologists or Kantian might be\\nsatisﬁed with this emphasis, but consequentialists would argue\\nthat the MTT places too much emphasis on the ability to artic-\\nulate the reason for one’s actions. In order to shift the focus\\nfrom conversational ability to action, Allen et al.',\n",
       "  '[144] also\\nproposed an alternative MTT that was called the “comparative\\nMTT” (cMTT). In cMTT, the human interrogator is given pairs\\nof descriptions of actual, morally signiﬁcant actions of a human\\nsubject and a machine (or AI agent), purged of all references that\\nwould identify the actor. If the interrogator correctly identiﬁes\\nthe machine in a certain percentage, then the machine cannot\\npass the test. A problem of this version of MTT is that the way the\\nmachine behaves is easier to recognize than humans, because the\\nmachine behaves consistently in the same situation. Therefore,\\nthe interrogator should be asked to assess whether one actor is\\nless moral than the other instead of one is more moral than the\\nother. If the machine is not identiﬁed as the less moral one of the\\npair more frequently than the human, the machine has passed\\nthe test. Although cMTT has several problems, for example, someone\\nmight argue this standard is too low, Wallach and Allen [146]\\nbelieve that cMTT is a feasible and acceptable method for\\nevaluating the morality of AI agents, since there are no other\\nevaluation criteria that are commonly accepted and agreed. 2) Expert and Nonexpert Tests: Besides MTT, researchers\\nhave tried to assess the moral competence of AI systems through\\nexpert or nonexpert tests, in which the system outcome is com-\\nparedagainstthegroundtruthprovidedbynonexpertsorexperts. The expert test adopts the standard of experts in normative ethics\\nto assess the morality of AI agents.',\n",
       "  'Nonexpert tests take folk\\nmorals as the benchmark and evaluate the moral capability of the\\nAI agent or system on the relevant benchmark test. In nonexpert\\ntests, citizens can play their roles in assessing and evaluating the\\nethical capabilities of an AI system based on their own ethical\\nstances and scrutiny. Fig.',\n",
       "  '5. Formal veriﬁcation process (this ﬁgure is recreated based on [147]). B. Veriﬁcation\\nAnother category of approaches for evaluating the morality\\nof AI consists of proving that the AI system behaves correctly\\naccording to some known speciﬁcations. Seshia et al.',\n",
       "  '[147]\\ndiscussed this kind of approach. A typical formal veriﬁcation\\nprocess is shown in Fig. 5, where S is a model of the system\\nto be veriﬁed, E is a model of the environment, and Φ is the\\nproperty to be veriﬁed. The veriﬁcation program will output a\\nYes/No answer, indicating whether or not S satisﬁes the property\\nΦ in environment E. Typically, a No output is accompanied by a\\ncounterexample, which shows how the execution of the system\\nviolates property Φ.',\n",
       "  'And a proof of correctness is included a Yes\\nanswer in some formal veriﬁcation tools. Arnold and Scheutz [148] explored the ﬂaws of MTT and\\npointed out that MTT-based evaluations are vulnerable to decep-\\ntion, inadequate reasoning, and inferior moral performance, and\\nthey proposed the concept of “design veriﬁcation” to evaluate\\nthe moral competence of AI system. For the evaluation of AI ethical design, diversiﬁed evaluation\\ncriteria can be used.',\n",
       "  'Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.',\n",
       "  'C.',\n",
       "  'Standards\\nMany industry standards have been proposed to guide the\\ndevelopment and application of AI and to evaluate or assess\\nAI products. In this section, some AI-related standards are\\nintroduced. 1) In 2014, the Australian Computer Society developed the\\nASC Professional Code of Conduct to follow by all infor-\\nmation communication technology professionals, which\\nidentiﬁes six core ethical values and the associated re-\\nquirements for professional conduct. 2) In 2018, ACM updated the ACM Code of Ethics and\\nProfessional Conduct to respond to the changes in the\\ncomputing profession since 1992. This Code expresses the\\nconscience of the profession and is designed to inspire and\\nguide the ethical conduct of all computing professionals,\\nincluding current and aspiring practitioners, instructors,\\nstudents, inﬂuencers, and anyone who uses computing\\ntechnology in an impactful way. Additionally, the Code\\nserves as a basis for remediation when violations occur.',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n815\\nThe Code includes principles formulated as statements of\\nresponsibility, based on the understanding that the public\\ngoodisalwaystheprimaryconsideration.Eachprincipleis\\nsupplemented by guidelines, which provide explanations\\nto assist computing professionals in understanding and\\napplying the principle [149]. 3) The project of IEEE Global Initiative on Ethics of Au-\\ntonomous and Intelligent Systems [150] has approved the\\nIEEE P7000TM standards series [151] under development\\n(listed in Table III of the Supplementary Materials), which\\ncover topics from data collection to privacy, to algorithmic\\nbias and beyond. 4) The ISO/IEC JTC 1/SC 42 [152], which is a joint commit-\\ntee between ISO and IEC responsible for standardization\\nin the area of AI, dedicates to developing a large set of\\nstandards includes the areas of foundational AI standards,\\nbig data, AI trustworthiness, use cases, applications, gov-\\nernance implications of AI, computational approaches of\\nAI, ethical and societal concerns. The standards published\\nand under development by ISO/IEC JTC 1/SC 42 are listed\\nin Table IV of the Supplementary Materials. With the concerns about AI ethical issues, the interest in\\nAI standards to shape the design, deployment, and evaluation\\nof AI has been growing fast.',\n",
       "  'Although many standards have\\nbeen proposed, the gap between standards (or principles) and\\npractice is still large. Currently, only some large corporates,\\nsuch as IBM [153] and Microsoft [154], have implemented their\\nown industrial standards, frameworks, and guidelines to build a\\nculture of AI; but for smaller businesses with less resources, the\\nprinciples to practice gap is a major problem. Thus, many efforts\\nare still needed.',\n",
       "  'On one hand, it is necessary to put forward\\nwell-developed standards; on the other hand, it is required to\\nvigorously promote the practice of standards.',\n",
       "  'VII.',\n",
       "  'CHALLENGES AND FUTURE PERSPECTIVES\\nAs AI ethics is an emerging discipline, and there are still many\\nchallenges and problems need to be addressed in this ﬁeld. In this\\nsection, we discuss some challenges in AI ethics and give some\\nfuture perspective from our views. The purpose of this section is\\nto provide some possible research questions and directions for\\nfurther research in the future, thereby facilitating the research\\nprogress in the ﬁeld of AI ethics. A.',\n",
       "  'Challenges in AI Ethical Guidelines and Principles\\nAs reviewed in Section IV, a large number of guidelines\\nhave been proposed and released by different organizations,\\ncompanies and governments, and different principles can be\\nidentiﬁed in these guidelines. However, at present, there is still\\nno guideline that have been approved and adopted by various\\norganizations, sectors, and governments. In other words, dif-\\nferent organizations, companies from different ﬁelds, and even\\ndifferent companies from the same ﬁelds have different opinions\\non AI ethics. The consensus on ethics of AI has not yet been\\nreached and it is not clear what common principles and values\\nAI needs to follow. Additionally, different ethical principles may\\nberequiredwhenAIisappliedindifferentareas.Currently,study\\nand discussion on ethics of AI in different speciﬁc application\\nareas are rarely seen during our literature study. Thus, it is crucial and necessary that the basic and common\\nethical principles of AI should be reached and well-established\\nvia the discussion and cooperation among different organiza-\\ntions, areas, and governments.',\n",
       "  'Then, based on the basic and\\ncommon principles, each ﬁeld can further improve these prin-\\nciples so that they are generally applicable in this speciﬁc ﬁeld. Clarifying the ethical principles and values that an AI system\\nneeds to comply with is the prerequisite and foundation for\\ndesigning such a system that meets these requirements.',\n",
       "  'B.',\n",
       "  'Challenges in Implementing Ethics in AI\\nIn the implementation of ethics in AI, there are many chal-\\nlenges. This section analyzes the challenges that may be en-\\ncountered in practice when different types of ethical theories\\nare adopted. 1) Challenges of Virtue Ethics in Practice: According to\\nvirtue ethics, an action of an agent is morally good if the agent\\ninstantiates some virtue, i.e., acts and thinks according to some\\nmoral values [93]. It is not possible to judge whether an AI\\nsystem or agent is virtuous or not just by observing an action or a\\nseriesofactionsthatseemtoimplythatvirtue,thereasonsbehind\\nthese actions need to be clariﬁed, that is, the motives behind\\nthese actions need to be clear.',\n",
       "  'However, the motives behind\\nthe actions of AI systems usually are unclear and unknown to\\nus, and difﬁcult to ﬁgure out. This is the main challenge for\\nimplementing virtue ethics. Additionally, when we carry out the\\nethical design based on virtue ethics, which virtue characteristics\\nor traits AI system will align to is a difﬁcult question.',\n",
       "  'Even if\\nthe virtue traits have been carefully selected, how to characterize\\nand measure the virtue is still a challenging task. 2) Challenges of Deontological Ethics in Practice: Deontol-\\nogists regard an action as morally good if it adheres to some\\nmoral rules or duties, regulations, and norms.',\n",
       "  'Although the\\nrule-based nature of deontological ethics seems suitable for\\npractice, challenges arise during the implementation process. First, which ethical rules should be implemented in ethical\\ndesign. Second, there might be conﬂicts between rules in some\\nsituations.',\n",
       "  'Although ordering or weighing the ethical rules may\\nsolve this problem, determining the order of importance of\\ndifferent ethical rules is often difﬁcult. 3) Challenges of Consequentialism Ethics in Practice: Con-\\nsequentialist ethics assess the morality of an action solely on\\nthe basis of its outcome. Two main challenges are involved\\nduring the implementation of consequentialism ethics.',\n",
       "  'First,\\nit is difﬁcult to determine the consequences of an action or a\\ndecision. For the current AI system, the possible consequences\\nof its actions usually are not clear beforehand given the lack of\\ntransparency or interpretability of current AI models, especially\\nthe artiﬁcial neural networks. The second challenge is related to\\nquantifying the consequences.',\n",
       "  'As consequentialism ethics aims\\nto maximize the utility, how to deﬁne and calculate the utility is\\nan essential problem.',\n",
       "  '816\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL.',\n",
       "  '4, NO. 4, AUGUST 2023\\n4) Challenges of Coordination Among Different Ethical\\nStandards: Due to differences in culture, religion, and orga-\\nnizations, the ethical standards are also different even if they\\nare in the same context. The uniﬁed ethical standard proposal\\nis not only difﬁcult to achieve, but also unnecessary. Therefore,\\nhow to achieve coordination between ethical standards from dif-\\nferent countries and organizations is important and particularly\\nchallenging.',\n",
       "  'C.',\n",
       "  'Challenges in Developing Technological Approaches to\\nMitigate Ethical Issues of AI\\nAt present, improving the explainability, fairness, privacy\\nprotection, security, robustness, and other competences related\\nto requirements of ethical AI are hot research topics in AI\\ncommunities. However, most of the current research work are\\ncarried out from a single dimension of ethical principles, for\\ninstance, XAI focuses on enhancing the interpretability of AI,\\nand fair ML is dedicated to mitigating unfairness or bias of\\nML. There is still a lack of integration of multiple ethical\\nprinciples or requirements in current research work. Obviously,\\nthe integration of multiple ethical dimensions that enables syn-\\nergistic balances between multiple different ethical principles is\\nessential and critical for building ethical AI systems which can\\nmeets multiple ethical principles. But it is very challenging to\\nintegrate multiple ethical dimensions in an AI system through\\ntechnological approaches due to the conﬂicts or incompatibili-\\nties between different ethical requirements. D. Challenges in Evaluating Ethics in AI\\nEthics is inherently a qualitative concept that depends on\\nmany features that are hard to quantify, e.g., culturally or racially\\nrelatedfeatures.Hence,itisveryhard,ifnotimpossible,todeﬁne\\nethics precisely. As a result, the evaluation of AI ethics will\\nalways have some subjective elements, depending on the people\\nwho are assessing AI. This poses challenges to the research and\\napplications of AI ethics.',\n",
       "  'E. Future Perspectives\\nIn this section, some future perspectives are pointed out,\\nwhich may be valuable for future research. First, for imple-\\nmenting ethics in AI, it should be pointed out that humans\\nnever use only one single ethical theory, but will switch between\\ndifferent theories according to the situation or context they are\\nfacing [134]. This is not only because human beings are not\\npurely rational agents that economic theory wants us to believe,\\nbut also because strict adherence to any moral theory can lead\\nto undesirable results. This means that AI systems should be\\nprovided with representations of different ethical theories and\\nthe ability to choose between these ethical theories. Here we call\\nthis multitheory approach. In multitheory approach, AI systems\\ncan interchangeably apply different theories depending on the\\ntype of situation. Furthermore, the combination of normative\\nethical theories and domain-speciﬁc ethics which accepted by\\ndomain experts is worthy of implementing since an ethical AI\\nsystem need to be accepted by its users. In terms of technological approaches for addressing ethical\\nissues in AI, it is desirable to develop new ML and other AI\\ntechnologies under the guidance of the ethical guidelines and\\nprinciples reviewed in Section IV. Although it is challenging\\nto consider multiple different ethical principles simultaneously\\nwhen designing new AI agents, this will be a very important and\\nessential step in developing ethical AI in the future. From the review about morality evaluation approaches, it\\ncan be found that effective evaluation methods are urgently\\nneeded because we must evaluate the designed AI system\\nbefore deployment. At present, it is difﬁcult to propose a\\ngeneral evaluation method. So, researchers often focused on\\nspeciﬁc domains and addressed the moral competence assess-\\nment tasks in these domains.',\n",
       "  'Domain-speciﬁc benchmarks, e.g.,\\ncomprehensive datasets, for moral testing of AI systems also\\nseems important for some crucial application ﬁelds, such as\\nautonomous cars, and health care. Last but not least, as both nature and nurture are important in\\nshaping moral behaviors, we suggest combining the normative\\nethics and evolutionary ethics [155] to design ethical AI systems. The normative ethics is like the innate moral abilities, while\\nevolutionary ethics approach can acquire new moral competence\\nthrough continuous learning and evolution. This might be a\\npromising route to future ethical AI system development.',\n",
       "  'VIII. CONCLUSION\\nBased on our review of AI ethics and the many complexities\\nand challenges described in this article, it is clear that attempting\\nto address ethical issues in AI and to design ethical AI systems\\nthat are able to behave ethically is a tricky and complex task. However, whether AI can play an increasingly important role\\nin our future society largely depends on the success of ethical\\nAI systems. The discipline of AI ethics requires a joint effort\\nof AI scientists, engineers, philosophers, users, and government\\npolicymakers. This article provides a comprehensive overview of AI ethics\\nby summarizing and analyzing the ethical risks and issues raised\\nby AI, ethical guidelines and principles issued by different\\norganizations, approaches for addressing ethical issues in AI\\nor fulﬁlling ethical principles of AI, and methods for evaluating\\nthe ethics (or morality) of AI. Furthermore, some challenges in\\nthe practice of AI ethics and some future research directions are\\npointed out. However, AI ethics is a very broad and multidisciplinary\\nresearch area. It is impossible to cover all possible topics in\\nthis area with one review article. We hope this article can serve\\nas a starting point for people who are interested in AI ethics to\\ngain a sufﬁcient background and a bird’s eye view so that further\\ninvestigation can be pursued by them. REFERENCES\\n[1] M.',\n",
       "  'Haenlein and A. Kaplan, “A brief history of artiﬁcial intelligence: On\\nthe past, present, and future of artiﬁcial intelligence,” California Manage. Rev., vol.',\n",
       "  '61, no.',\n",
       "  '4, pp. 5–14, 2019.',\n",
       "  '[2] R. Vinuesa et al., “The role of artiﬁcial intelligence in achieving the\\nsustainable development goals,” Nature Commun., vol. 11, no.',\n",
       "  '1, 2020,\\nArt. no.',\n",
       "  '233.',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n817\\n[3] Gartner, “Chatbots will appeal to modern workers,” 2019. Ac-\\ncessed: Feb.',\n",
       "  '10, 2022.',\n",
       "  '[Online]. Available: https://www.gartner.com/\\nsmarterwithgartner/chatbots-will-appeal-to-modern-workers\\n[4] M. J.',\n",
       "  'Haleem, R. P.',\n",
       "  'Singh, and R. Suman, “Telemedicine for healthcare:\\nCapabilities, features, barriers, and applications,” Sensors Int., vol. 2,\\n2021, Art.',\n",
       "  'no.',\n",
       "  '100117.',\n",
       "  '[5] A. Morby, “Tesla driver killed in ﬁrst fatal crash using au-\\ntopilot,”\\n2016. Accessed:\\nFeb.',\n",
       "  '10,\\n2022.',\n",
       "  '[Online]. Available:\\nhttps://www.dezeen.com/2016/07/01/tesla-driver-killed-car-crash-\\nnews-driverless-car-autopilot/\\n[6] S. McGregor, Ed., “Incident number 6,” in AI Incident Database, 2016.',\n",
       "  '[Online]. Available: https://incidentdatabase.ai/cite/6\\n[7] R. V.',\n",
       "  'Yampolskiy, “Predicting future AI failures from historic examples,”\\nForesight, vol. 21, no.',\n",
       "  '1, pp. 138–152, 2019.',\n",
       "  '[8] C. Stupp, “Fraudsters used AI to mimic CEO’s voice in un-\\nusual cybercrime case: Scams using artiﬁcial intelligence are a\\nnew challenge for companies,” 2019. Accessed: Feb.',\n",
       "  '10, 2022.',\n",
       "  '[Online]. Available: https://www.wsj.com/articles/fraudsters-use-ai-to-\\nmimic-ceos-voice-in-unusual-cybercrime-case-11567157402\\n[9] C. Allen, W.',\n",
       "  'Wallach, and I. Smit, “Why machine ethics?,” IEEE Intell. Syst., vol. 21, no. 4, pp. 12–17, Jul./Aug. 2006.',\n",
       "  '[10] M. Anderson and S.',\n",
       "  'L. Anderson, “Machine ethics: Creating an ethical\\nintelligent agent,” AI Mag., vol. 28, no.',\n",
       "  '4, pp. 15–26, 2007. [11] K.',\n",
       "  'Siau and W. Wang, “Artiﬁcial intelligence (AI) ethics,” J. Database\\nManage., vol.',\n",
       "  '31, no. 2, pp. 74–87, 2020. [12] A. Jobin, M.',\n",
       "  'Ienca, and E. Vayena, “The global landscape of AI ethics\\nguidelines,” Nature Mach. Intell., vol.',\n",
       "  '1, no. 9, pp. 389–399, 2019. [13] M. Ryan and B.',\n",
       "  'C. Stahl, “Artiﬁcial intelligence ethics guidelines for de-\\nvelopers and users: Clarifying their content and normative implications,”\\nJICES, vol. 19, no.',\n",
       "  '1, pp. 61–86, 2021. [14] N. Mehrabi, F. Morstatter, N. Saxena, K.',\n",
       "  'Lerman, and A. Galstyan, “A\\nsurvey on bias and fairness in machine learning,” ACM Comput. Surv.,\\nvol. 54, no. 6, pp. 1–35, 2021. [15] J.',\n",
       "  'García and F. Fernández, “A comprehensive survey on safe reinforce-\\nment learning,” J. Mach.',\n",
       "  'Learn. Res., vol. 16, no. 42, pp. 1437–1480,\\n2015. [16] V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A.',\n",
       "  'Dehghantanha, and\\nG. Srivastava, “A survey on security and privacy of federated learning,”\\nFuture Gener. Comput.',\n",
       "  'Syst., vol. 115, pp. 619–640, 2021.',\n",
       "  '[17] X. Liu et al., “Privacy and security issues in deep learning: A survey,”\\nIEEE Access, vol. 9, pp.',\n",
       "  '4566–4593, 2021.',\n",
       "  '[18] B. Arrieta et al., “Explainable artiﬁcial intelligence (XAI): Concepts,\\ntaxonomies, opportunities and challenges toward responsible AI,” Inf. Fusion, vol.',\n",
       "  '58, pp. 82–115, 2020. [19] Y. Zhang, M. Wu, G. Y. Tian, G.',\n",
       "  'Zhang, and J. Lu, “Ethics and privacy\\nof artiﬁcial intelligence: Understandings from bibliometrics,” Knowl.-\\nBased Syst., vol. 222, 2021, Art.',\n",
       "  'no.',\n",
       "  '106994.',\n",
       "  '[20] D. Castelvecchi, “Can we open the black box of AI?,” Nature, vol. 538,\\nno.',\n",
       "  '7623, pp. 20–23, 2016. [21] S. Dilmaghani, M. R. Brust, G. Danoy, N. Cassagnes, J.',\n",
       "  'Pecero, and\\nP. Bouvry, “Privacy and security of big data in AI systems: A research\\nand standards perspective,” in Proc. IEEE Int.',\n",
       "  'Conf. Big Data, 2019,\\npp. 5737–5743. [22] J.',\n",
       "  'P. Sullins, “When is a robot a moral agent?,” in Machine Ethics, M. Anderson and S.',\n",
       "  'L. Anderson, Eds., Cambridge, U.K.: Cambridge Univ. Press, 2011, pp.',\n",
       "  '151–161. [23] J. Timmermans, B.',\n",
       "  'C. Stahl, V.',\n",
       "  'Ikonen, and E. Bozdag, “The ethics of\\ncloud computing: A conceptual review,” in Proc. IEEE 2nd Int. Conf. Cloud Comput. Technol. Sci., 2010, pp. 614–620. [24] W.',\n",
       "  'Wang and K. Siau, “Ethical and moral issues with AI: A case study on\\nhealthcare robots,” in Proc. 24th Americas Conf.',\n",
       "  'Inf. Syst., 2018, pp. 1–5.',\n",
       "  '[25] I. Bantekas and L. Oette, International Human Rights Law and Practice. Cambridge U.',\n",
       "  'K.: Cambridge Univ. Press, 2018.',\n",
       "  '[26] R. Rodrigues, “Legal and human rights issues of AI: Gaps, challenges and\\nvulnerabilities,” J. Responsible Technol., vol. 4, 2020, Art. no.',\n",
       "  '100005. [27] W.',\n",
       "  'Wang and K. Siau, “Artiﬁcial intelligence, machine learning, au-\\ntomation, robotics, future of work and future of humanity: A review\\nand research agenda,” J. Database Manage., vol.',\n",
       "  '30, no.',\n",
       "  '1, pp. 61–79,\\n2019. [28] W.',\n",
       "  'Wang and K. Siau, “Industry 4.0: Ethical and moral predicaments,”\\nCutter Bus. Technol. J., vol. 32, no. 6, pp. 36–45, 2019. [29] S. M. Liao, Ed., Ethics of Artiﬁcial Intelligence. New York, NY, USA:\\nOxford Univ. Press, 2020.',\n",
       "  '[30] A. Adadi, “A survey on data-efﬁcient algorithms in big data era,” J. Big\\nData, vol. 8, no. 1, pp. 1–54, 2021. [31] R.',\n",
       "  'S. Geiger et al., “Garbage in, garbage out? Do machine learning\\napplication papers in social computing report where human-labeled\\ntraining data comes from?,” in Proc. Conf.',\n",
       "  'Fairness, Accountability,\\nTransparency, 2020, pp. 325–336. [32] W. M. P. van der Aalst, V. Rubin, H. M. W. Verbeek, B. F. van Dongen,\\nE. Kindler, and C.',\n",
       "  'W. Günther, “Process mining: A two-step approach to\\nbalance between underﬁtting and overﬁtting,” Softw. Syst.',\n",
       "  'Model., vol. 9,\\nno.',\n",
       "  '1, pp. 87–111, 2010. [33] Z.',\n",
       "  'C. Lipton, “The mythos of model interpretability,” Queue, vol. 16,\\nno.',\n",
       "  '3, pp. 31–57, 2018. [34] Y.',\n",
       "  'Wang and M. Kosinski, “Deep neural networks are more accurate than\\nhumans at detecting sexual orientation from facial images,” J. Pers.',\n",
       "  'Social\\nPsychol., vol. 114, no. 2, pp. 246–257, 2018. [35] D. Guera and E.',\n",
       "  'J. Delp, “Deepfake video detection using recurrent neural\\nnetworks,” in Proc. IEEE Int.',\n",
       "  'Conf. Adv. Video Signal-based Surveill.,\\n2018, pp. 1–6.',\n",
       "  '[36] C. B. Frey and M.',\n",
       "  'A. Osborne, “The future of employment: How sus-\\nceptible are jobs to computerisation?,” Technological Forecasting Social\\nChange, vol. 114, pp.',\n",
       "  '254–280, 2017.',\n",
       "  '[37] R. Maines, “Love + sex with robots: The evolution of human-robot\\nrelationships (Levy, D.; 2007) [Book review],” IEEE Technol. Soc.',\n",
       "  'Mag.,\\nvol. 27, no.',\n",
       "  '4, pp. 10–12, Dec. 2008. [38] National AI Standardization General, “Artiﬁcial intelligence ethical risk\\nanalysis report,” 2019. Accessed: Apr.',\n",
       "  '19, 2022.',\n",
       "  '[Online]. Available:\\nhttp://www.cesi.cn/201904/5036.html\\n[39] A. Hannun, C.',\n",
       "  'Guo, and L. van der Maaten, “Measuring data leakage in\\nmachine-learning models with ﬁsher information,” in Proc. 37th Conf. Uncertainty Artif. Intell., 2021, pp. 760–770. [40] A. Salem, M.',\n",
       "  'Backes, and Y. Zhang, “Get a model! Model hijacking\\nattack against machine learning models,” Nov. 2021. [Online]. Available:\\nhttps://arxiv.org/pdf/2111.04394\\n[41] A.',\n",
       "  'Pereira and C. Thomas, “Challenges of machine learning ap-\\nplied to safety-critical cyber-physical systems,” MAKE, vol. 2, no.',\n",
       "  '4,\\npp. 579–602, 2020. [42] J.',\n",
       "  'A. McDermid, Y. Jia, Z.',\n",
       "  'Porter, and I. Habli, “Artiﬁcial intelligence\\nexplainability: The technical and ethical dimensions,” Philos. Trans..',\n",
       "  'Ser. A, Math. Phys. Eng. Sci., vol. 379, no. 2207, 2021, Art. no. 20200363. [43] J.-F.',\n",
       "  'Bonnefon, A.',\n",
       "  'Shariff, and I. Rahwan, “The social dilemma of\\nautonomous vehicles,” Science, vol. 352, no.',\n",
       "  '6293, pp. 1573–1576, 2016. [44] B.',\n",
       "  'C.',\n",
       "  'Stahl and D. Wright, “Ethics and privacy in AI and big data: Im-\\nplementing responsible research and innovation,” IEEE Secur. Privacy,\\nvol.',\n",
       "  '16, no. 3, pp. 26–33, May/Jun. 2018. [45] S. Ribaric, A.',\n",
       "  'Ariyaeeinia, and N. Pavesic, “De-identiﬁcation for privacy\\nprotection in multimedia content: A survey,” Signal Process., Image\\nCommun., vol. 47, pp.',\n",
       "  '131–151, 2016. [46] A. Julia, L. Jeff, M.',\n",
       "  'Surya, and K. Lauren, “Machine bias: There’s\\nsoftware used across the country to predict future criminals. And\\nit’s biased against blacks,” 2016. Accessed: Apr.',\n",
       "  '19, 2022.',\n",
       "  '[On-\\nline].',\n",
       "  'Available: https://www.propublica.org/article/machine-bias-risk-\\nassessments-in-criminal-sentencing\\n[47] J. Dastin, “Amazon scraps secret AI recruiting tool that showed bias\\nagainst women,” 2018.',\n",
       "  'Accessed: Apr.',\n",
       "  '19, 2022.',\n",
       "  '[Online].',\n",
       "  'Available:\\nhttps://www.reuters.com/article/us-amazon-com-jobs-automation-\\ninsight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-\\nagainst-women-idUSKCN1MK08G\\n[48] D. Castelvecchi, “AI pioneer: ‘The dangers of abuse are very real’,” Na-\\nture, Apr. 4, 2019, [Online].',\n",
       "  'Available: https://www.nature.com/articles/\\nd41586-019-00505-2\\n[49] K. Hristov, “Artiﬁcial intelligence and the copyright dilemma,” IDEA,\\nIP Law Rev., vol. 57, 2017, Art.',\n",
       "  'no.',\n",
       "  '3. [Online]. Available: https://ssrn. com/abstract=2976428\\n[50] C.',\n",
       "  'Bartneck, C.',\n",
       "  'Lütge, A.',\n",
       "  'Wagner, and S. Welsh, “Responsibility and lia-\\nbility in the case of AI systems,” in An Introduction to Ethics in Robotics\\nand AI (SpringerBriefs in Ethics), C. Bartneck, C.',\n",
       "  'Lütge, A.',\n",
       "  'Wagner, and\\nS. Welsh, Eds., Cham, Switzerland: Springer, 2021, pp. 39–44.',\n",
       "  '[51] E. Bird, J. Fox-Skelly, N. Jenner, R. Larbey, E.',\n",
       "  'Weitkamp, and A. Winﬁeld, “The ethics of artiﬁcial intelligence: Issues and initiatives,”\\nEuropean Parliamentary Research Service, Brussels, Belgium, 2020. Ac-\\ncessed: Apr.',\n",
       "  '19, 2022.',\n",
       "  '[Online].',\n",
       "  'Available: https://www.europarl.europa. eu/thinktank/en/document/EPRS_STU(2020)634452\\n[52] C. Lutz, “Digital inequalities in the age of artiﬁcial intelligence and big\\ndata,” Hum. Behav.',\n",
       "  'Emerg. Technol., vol. 1, no. 2, pp. 141–148, 2019. [53] L. Manikonda, A.',\n",
       "  'Deotale, and S. Kambhampati, “What’s up with\\nPrivacy? User preferences and privacy concerns in intelligent personal\\nassistants,” in Proc.',\n",
       "  'AAAI/ACM Conf. AI, Ethics Soc., 2018, pp. 229–235.',\n",
       "  '818\\nIEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 4, NO.',\n",
       "  '4, AUGUST 2023\\n[54] D. Roselli, J.',\n",
       "  'Matthews, and N. Talagala, “Managing bias in AI,” in Proc. World Wide Web Conf., 2019, pp.',\n",
       "  '539–544.',\n",
       "  '[55] Y. Gorodnichenko, T.',\n",
       "  'Pham, and O. Talavera, “Social media, sentiment\\nandpublicopinions:Evidencefrom#Brexitand#USElection,”Eur.Econ. Rev., vol.',\n",
       "  '136, Jul. 2021, Art. no. 103772.',\n",
       "  '[56] N. Thurman, “Making ‘The daily me’: Technology, economics and\\nhabit in the mainstream assimilation of personalized news,” Journalism,\\nvol. 12, no.',\n",
       "  '4, pp. 395–415, 2011.',\n",
       "  '[57] J. Donath, “Ethical issues in our relationship with artiﬁcial entities,” in\\nThe Oxford Handbook of Ethics of AI. M.',\n",
       "  'D. Dubber, F.',\n",
       "  'Pasquale, and S. Das, Eds., Oxford, U.K.: Oxford Univ. Press, 2020, pp.',\n",
       "  '51–73.',\n",
       "  '[58] E. Magrani, “New perspectives on ethics and the laws of artiﬁcial intel-\\nligence,” Internet Policy Rev., vol. 8, 2019, Art.',\n",
       "  'no. 3. [59] M. P.',\n",
       "  'Wellman and U. Rajan, “Ethical issues for autonomous trading\\nagents,” Minds Mach., vol. 27, no.',\n",
       "  '4, pp. 609–624, 2017.',\n",
       "  '[60] U. Pagallo, “The impact of AI on criminal law, and its two fold pro-\\ncedures,” in Research Handbook on the Law of Artiﬁcial Intelligence,\\nW. Barﬁeld and U.',\n",
       "  'Pagallo, Eds., Cheltenham U.K.: Edward Elgar\\nPublishing, 2018, pp. 385–409.',\n",
       "  '[61] E. Dacoronia, “Tort law and new technologies,” in Legal Challenges in\\nthe New Digital Age, A. M.',\n",
       "  'López Rodríguez, M. D. Green, and M.',\n",
       "  'L. Kubica, Eds., Leiden, The Netherlands: Koninklijke Brill NV, 2021,\\npp. 3–12.',\n",
       "  '[62] J. Khakurel, B. Penzenstadler, J. Porras, A.',\n",
       "  'Knutas, and W. Zhang, “The\\nrise of artiﬁcial intelligence under the lens of sustainability,” Technolo-\\ngies, vol. 6, no.',\n",
       "  '4, 2018, Art. no.',\n",
       "  '100.',\n",
       "  '[63] S. Herat, “Sustainable management of electronic waste (e-Waste),” Clean\\nSoil Air Water, vol. 35, no.',\n",
       "  '4, pp. 305–310, 2007. [64] E.',\n",
       "  'Strubell, A.',\n",
       "  'Ganesh, and A. McCallum, “Energy and policy consid-\\nerations for deep learning in NLP,” in Proc. 57th Annu.',\n",
       "  'Meeting Assoc.',\n",
       "  'Comput. Linguistics, 2019, pp. 3645–3650.',\n",
       "  '[65] V. Dignum, “Ethics in artiﬁcial intelligence: Introduction to the special\\nissue,” Ethics Inf. Technol., vol. 20, no. 1, pp. 1–3, 2018. [66] S. Corbett-Davies, E. Pierson, A. Feller, S.',\n",
       "  'Goel, and A. Huq, “Algo-\\nrithmic decision making and the cost of fairness,” in Proc. 23rd ACM\\nSIGKDD Int.',\n",
       "  'Conf. Knowl. Discov. Data Mining, 2017, pp. 797–806.',\n",
       "  '[67] R. Caplan, J. Donovan, L.',\n",
       "  'Hanson, and J. Matthews, “Algorithmic ac-\\ncountability: A primer,” Data Soc., vol. 18, pp.',\n",
       "  '1–13, 2018. [68] R.',\n",
       "  'V. Yampolskiy, “On controllability of AI,” Jul. 2020.',\n",
       "  '[Online]. Avail-\\nable: https://arxiv.org/pdf/2008.04071\\n[69] B. C.',\n",
       "  'Stahl, J.',\n",
       "  'Timmermans, and C. Flick, “Ethics of emerging informa-\\ntion and communication technologies,” Sci. Public Policy, vol. 44, no.',\n",
       "  '3,\\npp. 369–381, 2017. [70] L. Vesnic-Alujevic, S.',\n",
       "  'Nascimento, and A. Pólvora, “Societal and\\nethical impacts of artiﬁcial intelligence: Critical notes on Euro-\\npean policy frameworks,” Telecommun. Policy, vol.',\n",
       "  '44, no. 6, 2020,\\nArt. no.',\n",
       "  '101961. [71] U.',\n",
       "  'G. Assembly, “Universal declaration of human rights,” UN Gen. Assem., vol.',\n",
       "  '302, no.',\n",
       "  '2, pp. 14–25, 1948. [72] S. Russell, S. Hauert, R.',\n",
       "  'Altman, and M. Veloso, “Robotics: Ethics of\\nartiﬁcial intelligence,” Nature, vol. 521, no.',\n",
       "  '7553, pp. 415–418, 2015.',\n",
       "  '[73] A. Chouldechova, “Fair prediction with disparate impact: A study of\\nbias in recidivism prediction instruments,” Big Data, vol. 5, no.',\n",
       "  '2,\\npp. 153–163, 2017.',\n",
       "  '[74] J. van Dijck, “Dataﬁcation, dataism and dataveillance: Big data be-\\ntween scientiﬁc paradigm and ideology,” Surveill. Soc., vol.',\n",
       "  '12, no. 2,\\npp. 197–208, 2014. [75] E. de Souza Nascimento, I. Ahmed, E. Oliveira, M. P.',\n",
       "  'Palheta, I.',\n",
       "  'Stein-\\nmacher, and T. Conte, “Understanding development process of machine\\nlearning systems: Challenges and solutions,” in Proc. ACM/IEEE Int.',\n",
       "  'Symp. Empirical Softw. Eng. Meas., 2019, pp. 1–6. [76] K. A. Crockett, L. Gerber, A.',\n",
       "  'Latham, and E. Colyer, “Build-\\ning trustworthy AI solutions: A case for practical solutions for\\nsmall businesses,” IEEE Trans.',\n",
       "  'Artif. Intell., early access, 2021,\\ndoi: 10.1109/TAI.2021.3137091.',\n",
       "  '[77] D. Leslie, “Understanding artiﬁcial intelligence ethics and safety:\\nA guide for the responsible design and implementation of AI\\nsystems in the public sector,” 2019. Accessed: Apr.',\n",
       "  '19, 2022.',\n",
       "  '[Online]. Available:\\nhttps://www.turing.ac.uk/research/publications/\\nunderstanding-artiﬁcial-intelligence-ethics-and-safety\\n[78] B. Buruk, P.',\n",
       "  'E.',\n",
       "  'Ekmekci, and B. Arda, “A critical perspective on guide-\\nlines for responsible and trustworthy artiﬁcial intelligence,” Med. Health\\nCare Philosophy, vol. 23, no. 3, pp.',\n",
       "  '387–399, 2020. [79] UNESCO, “Recommendation on the ethics of artiﬁcial intelligence,”\\n2021. Accessed: Feb.',\n",
       "  '15 2022. [Online]. Available: https://en.unesco. org/artiﬁcial-intelligence/ethics\\n[80] B.',\n",
       "  'C. Stahl, Ed., Artiﬁcial Intelligence for a Better Future: An Ecosystem\\nPerspective on the Ethics of AI and Emerging Digital Technologies. Cham, Switzerland: Springer, 2021. [81] P.',\n",
       "  'D. Motloba, “Non-maleﬁcence - A disremembered moral obligation,”\\nSouth Afr. Dent.',\n",
       "  'J., vol. 74, 2019, Art. no.',\n",
       "  '1. [82] L.',\n",
       "  'Floridi and J. Cowls, “A uniﬁed framework of ﬁve principles for AI\\nin society,” in Ethics, Governance, and Policies in Artiﬁcial Intelligence\\n(Philosophical Studies Series), vol. 144, L.',\n",
       "  'Floridi, Ed. Cham, Switzer-\\nland: Springer, 2021, pp. 5–17.',\n",
       "  '[83] S. Jain, M. Luthra, S.',\n",
       "  'Sharma, and M. Fatima, “Trustworthiness of\\nartiﬁcial intelligence,” in Proc. 6th Int.',\n",
       "  'Conf. Adv. Comput. Commun. Syst., 2020, pp. 907–912.',\n",
       "  '[84] L. Floridi et al., “AI4People-An ethical framework for a good AI society:\\nOpportunities, risks, principles, and recommendations,” Minds Mach.,\\nvol. 28, no.',\n",
       "  '4, pp. 689–707, 2018. [85] R.',\n",
       "  'Nishant, M.',\n",
       "  'Kennedy, and J. Corbett, “Artiﬁcial intelligence for\\nsustainability: Challenges, opportunities, and a research agenda,” Int. J.',\n",
       "  'Inf.',\n",
       "  'Manage., vol. 53, 2020, Art. no.',\n",
       "  '102104. [86] C. S. Wickramasinghe, D. L. Marino, J.',\n",
       "  'Grandio, and M. Manic, “Trust-\\nworthy AI development guidelines for human system interaction,” in\\nProc. 13th Int.',\n",
       "  'Conf. Hum. Syst. Interaction, 2020, pp. 130–136.',\n",
       "  '[87] V. Dignum, “Can AI systems be ethical?,” in Artiﬁcial Intelligence:\\nFoundations Theory and Algorithms, Responsible Artiﬁcial Intelligence. V. Dignum, Ed., Cham, Switzerland: Springer, 2019, pp. 71–92.',\n",
       "  '[88] S. L.',\n",
       "  'Anderson and M. Anderson, “AI and ethics,” AI Ethics, vol. 1, no.',\n",
       "  '1,\\npp. 27–31, 2021. [89] V. Dignum, “Ethical decision-making,” in Artiﬁcial Intelligence: Foun-\\ndations, Theory, and Algorithms, Responsible Artiﬁcial Intelligence. V. Dignum, Ed., Cham, Switzerland: Springer, 2019, pp. 35–46.',\n",
       "  '[90] G. Sayre-McCord,\\n“Metaethics,”\\nin\\nThe\\nStanford\\nEncyclopedia\\nof Philosophy, E. N. Zalta, Ed., Stanford, CA, USA: Metaphys. Res. Lab,\\nStanford\\nUniv.,\\n2014.',\n",
       "  '[Online]. Available:\\nhttps:\\n//plato.stanford.edu/entries/metaethics/#:∼:text=Metaethics%20is%\\n20the%20attempt%20to,matter%20of%20taste%20than%20truth%3F\\n[91] Ethics | Internet Encyclopedia of Philosophy, 1995. Accessed: Aug.',\n",
       "  '2,\\n2021. [Online].',\n",
       "  'Available: https://iep.utm.edu/ethics/#SH2c\\n[92] R.',\n",
       "  'Hursthouse and G. Pettigrove, “Virtue ethics,” in The Stanford En-\\ncyclopedia of Philosophy, E. N.',\n",
       "  'Zalta, Ed. Stanford, CA, USA: Meta-\\nphys. Res. Lab, Stanford Univ., 2018. [Online]. Available: https://plato. stanford.edu/entries/ethics-virtue/\\n[93] N. Cointe, G.',\n",
       "  'Bonnet, and O. Boissier, “Ethical judgment of agents’\\nbehaviors in multi-agent systems,” in Proc. Int.',\n",
       "  'Conf. Auton. Agents\\nMultiagent Syst., 2016, pp. 1106–1114. [94] H. Yu, Z. Shen, C. Miao, C. Leung, V. R.',\n",
       "  'Lesser, and Q. Yang, “Building\\nethics into artiﬁcial intelligence,” in Proc. 27th Int.',\n",
       "  'Joint Conf.',\n",
       "  'Artif. Intell., 2018, pp. 5527–5533. [95] H. J. Curzer, Aristotle and the Virtues. New York, NY, USA: Oxford\\nUniv. Press, 2012.',\n",
       "  '[96] L.',\n",
       "  'Alexander and M. Moore, “Deontological ethics,” in The Stanford\\nEncyclopedia of Philosophy, E. N.',\n",
       "  'Zalta, Ed., 2020, Stanford, CA, USA:\\nMetaphys. Res. Lab, Stanford Univ., 2020. [Online]. Available: https:\\n//plato.stanford.edu/entries/ethics-deontological/\\n[97] W. Sinnott-Armstrong, “Consequentialism,” in The Stanford Encyclope-\\ndia of Philosophy, E. N.',\n",
       "  'Zalta, Ed., 2019. Stanford, CA, USA: Metaphys. Res. Lab, Stanford Univ., [Online]. Available: https://plato.stanford.edu/\\nentries/consequentialism/\\n[98] D. O. Brink, “Some forms and limits of consequentialism,” in Ox-\\nford Handbooks in Philosophy, The Oxford Handbook of Ethical The-\\nory. D.',\n",
       "  'Copp, Ed., New York, NY, USA: Oxford Univ. Press, 2006,\\npp.',\n",
       "  '380–423. [99] H. A. M.',\n",
       "  'J. ten Have, Ed., Encyclopedia of Global Bioethics. Cham,\\nSwitzerland: Springer, 2016. [100] S. Tolmeijer, M. Kneer, C. Sarasua, M.',\n",
       "  'Christen, and A. Bernstein,\\n“Implementations in machine ethics: A survey,” ACM Comput. Surv.,\\nvol. 53, no. 6, pp. 1–38, 2021. [101] C. Allen, I.',\n",
       "  'Smit, and W. Wallach, “Artiﬁcial morality: Top-down,\\nbottom-up, and hybrid approaches,” Ethics Inf. Technol., vol.',\n",
       "  '7, no. 3,\\npp. 149–155, 2005. [102] W.',\n",
       "  'Wallach and C. Allen, “Top-down morality,” in Moral Machines, W. Wallach and C.',\n",
       "  'Allen, Eds., Oxford, U.K: Oxford Univ. Press, 2009,\\npp.',\n",
       "  '83–98.',\n",
       "  '[103] I. Asimov, “Runaround,” Astounding Sci. Fiction, vol. 29, no. 1,\\npp. 94–103, 1942. [104] J.-G. Ganascia,\\n“Ethical\\nsystem\\nformalization\\nusing\\nnon-\\nmonotonic ogics,” in Proc. Annu. Meeting Cogn. Sci.',\n",
       "  'Soc., 2007,\\npp. 1013–1018.',\n",
       "  'HUANG et al.: OVERVIEW OF ARTIFICIAL INTELLIGENCE ETHICS\\n819\\n[105] K. Arkoudas, S.',\n",
       "  'Bringsjord, and P. Bello, “Toward ethical robots via\\nmechanized deontic logic,” in Proc. AAAI Fall Symp.',\n",
       "  'Mach. Ethics, 2005,\\npp. 17–23.',\n",
       "  '[106] S.',\n",
       "  'Bringsjord and J. Taylor, “Introducing divine-command robot ethics,”\\nin Robot Ethics: The Ethical and Social Implication of Robotics. 2012,\\npp.',\n",
       "  '85–108. [107] N.',\n",
       "  'S.',\n",
       "  'Govindarajulu and S. Bringsjord, “On automating the doctrine\\nof double effect,” in Proc. 26th Int.',\n",
       "  'Joint Conf. Artif. Intell., 2017,\\npp. 4722–4730.',\n",
       "  '[108] F. Berreby, G.',\n",
       "  'Bourgne, and J.-G. Ganascia, “A declarative modular\\nframework for representing and applying ethical principles,” in Proc. 16th Conf.',\n",
       "  'Auton. Agents MultiAgent Syst., 2017, pp. 96–104.',\n",
       "  '[109] V. Bonnemains, C.',\n",
       "  'Saurel, and C. Tessier, “Embedded ethics: Some\\nechnical and ethical challenges,” Ethics Inf. Technol., vol. 20, no. 1,\\npp. 41–58, 2018. [110] G. S. Reed, M. D. Petty, N. J. Jones, A. W. Morris, J. P. Ballenger,\\nand H.',\n",
       "  'S. Delugach, “A principles-based model of ethical considerations\\nin military decision making,” J. Defense Model.',\n",
       "  'Simul., vol. 13, no. 2,\\npp. 195–211, 2016. [111] L. Dennis, M. Fisher, M.',\n",
       "  'Slavkovik, and M. Webster, “Formal veriﬁcation\\nof ethical choices in autonomous systems,” Robot. Auton.',\n",
       "  'Syst., vol. 77,\\npp. 1–14, 2016. [112] A.',\n",
       "  'R.',\n",
       "  'Honarvar and N. Ghasem-Aghaee, “Casuist BDI-Agent: A new\\nextended BDI architecture with the capability of ethical reasoning,” in\\nProc. Int.',\n",
       "  'Conf. Artif. Intell. Comput. Intell., 2009, pp.',\n",
       "  '86–95. [113] A. S. Rao and M.',\n",
       "  'P. Georgeff, “BDI agents: From theory to practice,” in\\nProc. 1st Int.',\n",
       "  'Conf. Multiagent Syst., 1995, pp. 312–319.',\n",
       "  '[114] S. Armstrong, “Motivated value selection for artiﬁcial agents,” in Proc. AAAI Workshop Artif. Intell. Ethics, Jan. 2015, pp.',\n",
       "  '12–20. [115] U. Furbach, C.',\n",
       "  'Schon, and F. Stolzenburg, “Automated reasoning in\\ndeontic logic,” in Proc. 8th Int. Workshop Multi-Disciplinary Trends Artif. Intell., 2014, pp.',\n",
       "  '57–68. [116] D.',\n",
       "  'Howard and I. Muntean, “Artiﬁcial moral cognition: Moral function-\\nalism and autonomous moral agency,” in Philosophical Studies Series,\\nPhilosophy and Computing, T. M.',\n",
       "  'Powers, Ed. Cham, Switzerland:\\nSpringer, 2017, pp. 121–159. [117] Y.-H.',\n",
       "  'Wu and S.-D. Lin, “A low-cost ethics shaping approach for de-\\nsigning reinforcement learning agents,” in Proc. 32nd AAAI Conf.',\n",
       "  'Artif. Intell., 2018, pp. 1687–1694.',\n",
       "  '[118] R. Noothigattu et al., “A voting-based system for ethical decision mak-\\ning,” in Proc. 32nd AAAI Conf.',\n",
       "  'Artif. Intell., 2018, pp. 1587–1594.',\n",
       "  '[119] M. Guarini, “Particularism and the classiﬁcation and reclassiﬁcation of\\nmoral cases,” IEEE Intell. Syst., vol. 21, no. 4, pp. 22–28, Jul./Aug. 2006.',\n",
       "  '[120] M. Anderson and S.',\n",
       "  'L. Anderson, “GenEth: A general ethical dilemma\\nanalyzer,” in Proc. 28th AAAI Conf.',\n",
       "  'Artif. Intell., 2014, pp. 253–261.',\n",
       "  '[121] M. Azad-Manjiri, “A new architecture for making moral agents based on\\nC4.5 decision tree algorithm,” Int. J.',\n",
       "  'Inf. Technol. Comput. Sci., vol. 6,\\nno.',\n",
       "  '5, pp. 50–57, 2014. [122] L.',\n",
       "  'Yilmaz, A. Franco-Watkins, and T.',\n",
       "  'S. Kroecker, “Computational mod-\\nels of ethical decision-making: A coherence-driven reﬂective equilibrium\\nmodel,” Cogn. Syst.',\n",
       "  'Res., vol. 46, pp. 61–74, 2017. [123] T.',\n",
       "  'A. Han, A. Saptawijaya, and L.',\n",
       "  'M. Pereira, “Moral reasoning under\\nuncertainty,” in Logic For Programming, Artiﬁcial Intelligence, and\\nReasoning. Berlin, Germany: Springer, 2012, pp. 212–227. [124] M. Anderson, S.',\n",
       "  'Anderson, and C. Armen, “Towards machine ethics\\nimplementing two action-based ethical theories,” in Proc. AAAI Fall\\nSymp.',\n",
       "  'Mach. Ethics, 2005, pp. 1–7.',\n",
       "  '[125] G. Gigerenzer, “Moral satisﬁcing: Rethinking moral behavior as bounded\\nrationality,” Topics Cogn. Sci., vol.',\n",
       "  '2, no. 3, pp. 528–554, 2010.',\n",
       "  '[126] J. Skorin-Kapov, “Ethical positions and decision-making,” in Profes-\\nsional and Business Ethics Through Film, J. Skorin-Kapov, Ed., New\\nYork, NY, USA: Springer, 2018, pp. 19–54.',\n",
       "  '[127] T.-L.',\n",
       "  'Gu and L. Li, “Artiﬁcial moral agents and their design methodology:\\nRetrospect and prospect,” Chin. J. Comput., vol. 44, pp.',\n",
       "  '632–651, 2021. [128] C.Molnar,G.Casalicchio,andB.Bischl,“Interpretablemachinelearning\\n– A brief history, state-of-the-art and challenges,” in Proc. Joint Eur.',\n",
       "  'Conf. Mach. Learn. Knowl. Discov. Databases, 2020, pp. 417–431.',\n",
       "  '[129] C. Molnar, Interpretable Machine Learning: A Guide For Making Black\\nBox Models Interpretable. Morisville, NC, USA: Lulu, 2019.',\n",
       "  '[130] S. Feuerriegel, M.',\n",
       "  'Dolata, and G. Schwabe, “Fair AI,” Bus. Inf. Syst. Eng.,\\nvol. 62, no. 4, pp. 379–384, 2020. [131] S.',\n",
       "  'Caton and C. Haas, “Fairness in machine learning: A survey,”\\nOct. 2020.',\n",
       "  '[Online]. Available: https://arxiv.org/pdf/2010.04053\\n[132] S. E. Whang, K. H. Tae, Y.',\n",
       "  'Roh, and G. Heo, “Responsible AI challenges\\nn end-to-end machine learning,” Jan. 2021.',\n",
       "  '[Online]. Available: https:\\n//arxiv.org/pdf/2101.05967\\n[133] D. Peters, K.',\n",
       "  'Vold, D. Robinson, and R.',\n",
       "  'A. Calvo, “Responsible AI—Two\\nframeworksforethicaldesignpractice,”IEEETrans.Technol.Soc.,vol.1,\\nno. 1, pp.',\n",
       "  '34–47, Mar. 2020. [134] V.',\n",
       "  'Dignum, ed., Responsible Artiﬁcial Intelligence. Cham, Switzerland:\\nSpringer, 2019.',\n",
       "  '[135] C. Dwork, “Differential privacy: A survey of results,” in Proc. Int.',\n",
       "  'Conf. Theory Appl. Models Computation, 2008, pp. 1–19.',\n",
       "  '[136] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T.',\n",
       "  'Chen, and H. Yu, “Federated\\nlearning,” Synth. Lectures Artif. Intell. Mach. Learn., vol. 13, no. 3,\\npp. 1–207, 2019.',\n",
       "  '[137] M. Kirienko et al., “Distributed learning: A reliable privacy-preserving\\nstrategy to change multicenter collaborations using AI,” Eur. J.',\n",
       "  'Nucl. Med. Mol. Imag., vol. 48, no. 12, pp. 3791–3804.2021. [138] R.',\n",
       "  'Shokri and V.',\n",
       "  'Shmatikov, “Privacy-preserving deep learning,” in\\nProc. 22nd ACM SIGSAC Conf. Comput. Commun. Secur., 2015,\\npp. 1310–1321. [139] C. Meurisch, B.',\n",
       "  'Bayrak, and M. Mühlhäuser, “Privacy-preserving AI\\nservices through data decentralization,” in Proc. Web Conf., 2020,\\npp.',\n",
       "  '190–200. [140] UR-Lex - 02016R0679-20160504 - EN - EUR-Lex, 2016. Accessed:\\nJun.',\n",
       "  '28, 2021.',\n",
       "  '[Online]. Available: https://eur-lex.europa.eu/legal-\\ncontent/EN/TXT/?uri=CELEX%3A02016R0679-20160504&qid=\\n1532348683434\\n[141] R.',\n",
       "  'E. Latta, H.R.3388 - 115th Congress (2017-2018): SELF DRIVE\\nAct, 2017. Accessed: Jun.',\n",
       "  '28, 2021. [Online].',\n",
       "  'Available: https://www. congress.gov/bill/115th-congress/house-bill/3388\\n[142] 7. Lei No.',\n",
       "  '13, de 14 de Agosto de 2018, 2018. Accessed: Jun.',\n",
       "  '25, 2021.',\n",
       "  '[Online]. Available: http://www.planalto.gov.br/ccivil_03/_Ato2015-\\n2018/2018/Lei/L13709.html\\n[143] EUR-Lex - 52021PC0206 - EN - EUR-Lex, 2021. Accessed: Jun.',\n",
       "  '28,\\n2021.',\n",
       "  '[Online]. Available: https://eur-lex.europa.eu/legal-content/EN/\\nTXT/?qid=1623335154975&uri=CELEX%3A52021PC0206\\n[144] C. Allen, G.',\n",
       "  'Varner, and J. Zinser, “Prolegomena to any future artiﬁcial\\nmoral agent,” J. Exp.',\n",
       "  'Theor. Artif. Intell., vol. 12, no. 3, pp. 251–261,\\n2000. [145] A.',\n",
       "  'M. Turing, “Computing machinery and intelligence,” Mind, vol. LIX,\\nno.',\n",
       "  '236, pp. 433–460, 1950. [146] W. Wallach and C. Allen, Moral Machines: Teaching Robots Right From\\nWrong. Oxford, U.K.: Oxford Univ.',\n",
       "  'Press, 2009. [147] S.',\n",
       "  'A. Seshia, D. Sadigh, and S.',\n",
       "  'S. Sastry, “Towards veriﬁed artiﬁcial\\nintelligence,” Jun. 2016.',\n",
       "  '[Online]. Available: http://arxiv.org/pdf/1606. 08514v4\\n[148] T.',\n",
       "  'Arnold and M. Scheutz, “Against the moral turing test: Accountable\\ndesign and the moral reasoning of autonomous systems,” Ethics Inf. Technol., vol.',\n",
       "  '18, no. 2, pp. 103–115, 2016. [149] ACM Code of Ethics and Professional Conduct, 2018. Accessed: Jun.',\n",
       "  '25,\\n2021.',\n",
       "  '[Online]. Available: https://www.acm.org/code-of-ethics\\n[150] IEEE SA- The IEEE Global Initiative on Ethicsof Autonomousand Intel-\\nligent Systems, 2019. Accessed: Jun.',\n",
       "  '28 2021.',\n",
       "  '[Online]. Available: https:\\n//standards.ieee.org/industry-connections/ec/autonomous-systems.html\\n[151] IEEE Ethics In Action | Ethically Aligned Design, IEEE 7000TM\\nProjects, 2020. Accessed: Jun.',\n",
       "  '28, 2021.',\n",
       "  '[Online]. Available: https:\\n//ethicsinaction.ieee.org/p7000/\\n[152] ISO, ISO/IEC JTC 1/SC 42 - Artiﬁcial intelligence, 2017. Accessed:\\nJun.',\n",
       "  '28, 2021.',\n",
       "  '[Online]. Available: https://www.iso.org/committee/\\n6794475.html\\n[153] B. Goehring, F.',\n",
       "  'Rossi, and D. Zaharchuk, “Advancing AI ethics beyond\\ncompliance: From principles to practice,” IBM Corporation, Apr. 2020.',\n",
       "  'Accessed: Apr. 19, 2022.',\n",
       "  '[Online]. Available: https://www.ibm.com/\\nthought-leadership/institute-business-value/report/ai-ethics\\n[154] Responsible AI, 2017. Accessed: Apr.',\n",
       "  '19, 2022.',\n",
       "  '[Online].',\n",
       "  'Available:\\nhttps://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:\\nprimaryr6\\n[155] F. Allhoff, “Evolutionary ethics from Darwin to Moore,” Hist. Philosophy\\nLife Sci., vol.',\n",
       "  '25, no. 1, pp. 51–79, 2003.',\n",
       "  'Vol.:(0123456789)\\n1 3\\nJournal of Business Ethics (2023) 185:725–740 \\nhttps://doi.org/10.1007/s10551-023-05339-7\\nORIGINAL PAPER\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful \\nWork\\nSarah\\xa0Bankins1\\u200a \\xa0· Paul\\xa0Formosa2\\nReceived: 17 February 2021 / Accepted: 25 January 2023 / Published online: 11 February 2023 \\n© The Author(s) 2023, corrected publication 2023\\nAbstract\\nThe increasing workplace use of artificially intelligent (AI) technologies has implications for the experience of meaningful \\nhuman work. Meaningful work refers to the perception that one’s work has worth, significance, or a higher purpose.',\n",
       "  'The \\ndevelopment and organisational deployment of AI is accelerating, but the ways in which this will support or diminish oppor-\\ntunities for meaningful work and the ethical implications of these changes remain under-explored. This conceptual paper is \\npositioned at the intersection of the meaningful work and ethical AI literatures and offers a detailed assessment of the ways \\nin which the deployment of AI can enhance or diminish employees’ experiences of meaningful work. We first outline the \\nnature of meaningful work and draw on philosophical and business ethics accounts to establish its ethical importance. We \\nthen explore the impacts of three paths of AI deployment (replacing some tasks, ‘tending the machine’, and amplifying human \\nskills) across five dimensions constituting a holistic account of meaningful work, and finally assess the ethical implications. In doing so we help to contextualise the meaningful work literature for the era of AI, extend the ethical AI literature into the \\nworkplace, and conclude with a range of practical implications and future research directions. Keywords\\u2002 Meaningful work\\xa0· Artificial intelligence (AI)\\xa0· Ethical AI\\xa0· Future of work\\xa0· Technology and work\\nIntroduction\\nIncreasing organisational use of artificially intelligent (AI) \\ntechnologies will influence how people experience work \\n(World Economic Forum [WEF], 2018), including how and \\nwhether they experience meaningfulness in their work. AI \\nis the ability of computers and other artificial entities to do \\nthings typically classified as requiring intelligence were a \\nhuman to do them, such as reason, plan, problem solve, and \\nlearn from experience (Wang, 2019). Meaningful work is \\nthe perception that one’s work has worth, significance, or a \\nhigher purpose (Michaelson et\\xa0al., 2014), and this typically \\nrequires the coordinated exercise of varied and complex \\nskills to benefit others.',\n",
       "  'Providing opportunities for mean-\\ningful work supports positive outcomes for workers (Allan \\net\\xa0al., 2019) and is ethically important as a basis for human \\nwellbeing and flourishing (Bailey et\\xa0al., 2019; Lysova et\\xa0al., \\n2019). However, despite becoming an increasingly prevalent \\nfeature of workplaces, there remains a poor understanding of \\nhow AI use will influence opportunities for meaningful work \\nand the ethical implications of such changes. Historically technological advancements have, since at \\nleast the first\\xa0industrial revolution, significantly changed \\nopportunities for meaningful work by altering what work-\\ners do, the nature of their skills, and their feelings of aliena-\\ntion from or integration with the production process (Vallor, \\n2015). AI use will likely extend such changes, but its unique \\nfeatures and uses also generate new and conflicting impli-\\ncations for meaningful work.',\n",
       "  'Optimistic accounts suggest \\nthat\\xa0AI will expand the range of meaningful higher-order \\nhuman work tasks (WEF, 2018), whereas more pessimis-\\ntic accounts suggest that\\xa0AI will degrade and even elimi-\\nnate human work (Frey & Osborne, 2017). These ongoing \\ntensions point to a lack of conceptual clarity regarding the \\n *\\t Sarah Bankins \\n\\t\\nsarah.bankins@mq.edu.au\\n\\t\\nPaul Formosa \\n\\t\\npaul.formosa@mq.edu.au\\n1\\t\\nDepartment of\\xa0Management, Macquarie Business School, \\nMacquarie University, North Ryde Campus, Sydney, \\nNSW\\xa02109, Australia\\n2\\t\\nDepartment of\\xa0Philosophy, Faculty of\\xa0Arts, Macquarie \\nUniversity, North Ryde Campus, Sydney, NSW\\xa02109, \\nAustralia',\n",
       "  '726\\n\\t\\nS.',\n",
       "  'Bankins, P.',\n",
       "  'Formosa \\n1 3\\nimpacts of AI on meaningful work, leading to calls for more \\nresearch in this area (Parker & Grote, 2022). This conceptual paper aims to help\\xa0address such gaps \\nby examining how workplace use of AI has the potential \\nto both enhance and diminish experiences of meaningful \\nwork, depending largely on the implementation choices of \\nemployers. This research is positioned at the intersection of \\nthe meaningful work and ethical AI literatures and makes \\ntwo key contributions. First, we contextualise the meaning-\\nful work literature for the era of AI by developing concep-\\ntual resources to examine how the implementation of such \\ntechnologies affects workers’ opportunities for meaningful \\nwork and connect this assessment to the ethical implications \\nof these changes.',\n",
       "  'Second, we help remedy a neglected aspect \\nof the ethical AI literature by offering a detailed examination \\nof AI’s implications for meaningful work. We begin by outlining the nature of meaningful work and \\nits ethical importance, integrating philosophical and busi-\\nness ethics accounts. We then examine the impacts of three \\npaths of AI deployment—replacing some simple and com-\\nplex tasks (replacement), ‘tending the machine’ (creating \\nnew forms of human work), and amplifying human skills \\n(augmenting/assisting workers)—across five dimensions \\nof meaningful work. These dimensions integrate both job-\\nspecific (through Hackman & Oldham’s, 1976 job charac-\\nteristics model) and more holistic (through Lips-Wiersma & \\nMorris’, 2009 model) drivers of meaningful work. We then \\ndevelop the ethical implications of our analysis by drawing \\non the AI4People ethical AI framework (Floridi et\\xa0al., 2018) \\nand its five principles of beneficence, non-maleficence, \\nautonomy, justice, and explicability. We conclude with prac-\\ntical insights into how experiences of meaningful work will \\nchange as AI becomes more widespread and offer several \\ndirections for future research. AI and\\xa0Work: Uses and\\xa0Unique Features\\nCurrent AIs constitute artificial narrow intelligence, or AIs \\nthat can undertake actions only within restricted domains, \\nsuch as classifying pictures of cats (Boden, 2016). The “holy \\ngrail” of AI research is artificial general intelligence (Boden, \\n2016), or AIs that can perform at least as well as humans \\nacross the full range of intelligent activities. We focus only \\non narrow AI as it is already used across many diverse sec-\\ntors, including in healthcare, judicial, educational, manu-\\nfacturing, and military contexts, among many others (see \\nBankins & Formosa, 2021; Bekey, 2012; Walsh et\\xa0al., 2019). The established use of narrow AI also allows us to draw on \\npractical examples to ground our assessment of its effects on \\nmeaningful work. While considering the possible implica-\\ntions of artificial general intelligence for meaningful work \\nis important, and we discuss this in our future research \\ndirections, there remain persistent disagreements about \\nwhen, if ever, it will be achieved (Boden, 2016). This makes \\nit critical to examine the impacts of current AI capabilities \\non opportunities for meaningful work that are occurring now \\nand in the near-term (Webster & Ivanov, 2020).',\n",
       "  'Past research demonstrates the dual positive and negative \\neffects of technology upon aspects of meaningful work.',\n",
       "  'For \\nexample, technology use can upskill workers and enhance \\ntheir autonomy, but it can also deskill and serve to control \\nthem (Vallor, 2015; Mazmanian et\\xa0al., 2013), with meaning-\\nfulness generally elevated in the former case (Cheney et\\xa0al., \\n2008). Technology’s positive effects can also help individu-\\nals confirm pre-existing notions of meaningful work, but \\nits negative outcomes can require them to re-interpret and \\nadjust those meanings as the technology’s dual effects are \\nrealised, for example by providing on-demand connection to \\nwork but heightening distraction from other responsibilities \\n(Symon & Whiting, 2019). Such dual effects remain evident \\nin advancing forms of technology, such as workplace robot-\\nics that offer both benefits and threats to meaningful human \\nwork (see Smids et\\xa0al., 2020).',\n",
       "  'These findings are critical, but their focus is on broader \\ntypes of information and communication technologies, \\nwhereas we focus specifically upon AI and its implications \\nfor meaningful work.',\n",
       "  'While AI use should also generate \\nthese types of dual effects, its unique features warrant spe-\\ncific attention.',\n",
       "  'For example, compared to past technologies \\nAI can undertake more cognitive tasks, expanding beyond \\n‘blue collar’ work in manufacturing where technology’s role \\nin replacing human labour has a long history, and into more \\n‘white collar’ forms of work (Bankins & Formosa, 2020). Further, machine learning in AIs is often driven by large \\namounts of data, the acquisition of which raises serious con-\\ncerns about privacy, consent, and surveillance, with impli-\\ncations for worker autonomy (Bailey et\\xa0al., 2019). Potential \\nbiases in data collection, the use of AI models built from \\nbiased data, and the resultant replication of systemic injus-\\ntices (Walsh et\\xa0al., 2019), as already evidenced in some AI-\\ndriven recruitment practices (Dastin, 2018), raises further \\nconcerns about the potential for one’s AI-informed work \\nto harm others. The potential for such harms is then exac-\\nerbated given the scale at which AI can be deployed. The \\nway AIs expand opportunities to manipulate and control \\nhumans also raises important issues (Susser et\\xa0al., 2019), \\nparticularly through the way it can act as an information \\ngatekeeper for human workers (Kellogg et\\xa0al., 2020). Finally, \\nthe ‘blackbox’ nature of the neural networks many AIs use \\nmeans end-users and even AI developers cannot understand \\nhow an AI generates its outputs (Jarrahi, 2019). This can \\nmake it difficult to trust AIs, to feel competent in working \\nalongside them, and to build responsible systems for which \\nhuman workers can be held meaningfully accountable (Dahl, \\n2018). These features of AI have attendant consequences',\n",
       "  '727\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nfor meaningful work that we will explore. We first turn to \\nexplaining the components of meaningful work and its ethi-\\ncal importance.',\n",
       "  'What Constitutes Meaningful Work?',\n",
       "  'Several approaches outline what constitutes meaningful \\nwork. One dominant task-based framework is Hackman and \\nOldham’s (1976) job characteristics model (JCM), which \\nexamines how job and task design influences experiences \\nof meaningfulness in work (Pratt & Ashforth, 2003).1 Other \\nframeworks extend beyond a task focus to adopt a more \\n“humanistic” approach (Lips-Wiersma & Morris, 2009, p. 493).',\n",
       "  'For example, Lips-Wiersma and Morris (2009) suggest \\nthat\\xa0meaningful work derives from finding balance between \\n“being (true to self)-doing (making a difference)” and a \\nfocus on “self (self-actualisation)-others (serving others)”. This creates the meaningful work dimensions of “developing \\nand becoming self”, “serving others”, “unity with others”, \\nand “expressing one’s full potential” (Lips-Wiersma & Mor-\\nris, 2009, p.',\n",
       "  '501). To adopt a holistic approach for exploring the impacts of \\nAI on meaningful work we integrate aspects of meaningful \\njob design from the JCM (Hackman & Oldham, 1976) with \\ndimensions of work that facilitate the more wide-ranging \\nenhancement of oneself through development, contribu-\\ntion, and connection to others from Lips-Wiersma and \\nMorris’ (2009) framework. This harmonisation generates \\nfive meaningful work dimensions that we focus our analy-\\nsis upon.2 The first dimension we label task integrity. This \\nencompasses task identity from the JCM, or the range of \\ntasks an individual does and the opportunity to complete \\na whole piece of work. This ability to undertake integrated \\nrather than fragmented tasks then influences the extent to \\nwhich workers can fully develop themselves, their capaci-\\nties, and express their full potential as an integrated whole \\nperson (“developing and becoming self” and “expressing full \\npotential” from Lips-Wiersma & Morris, 2009). The second \\ndimension we label skill cultivation and use.',\n",
       "  'This encom-\\npasses skill variety and use from the JCM, or the ability to \\nuse and develop a range of skills at work. Like the types of \\ntasks to which they are applied, prospects for skill utilisation \\nthen influence opportunities for growth through learning and \\nthe broader cultivation of the self and one’s potential via \\ndeveloping, testing, and exercising a varied range of com-\\npetencies (“developing and becoming self” and “expressing \\nfull potential” from Lips-Wiersma & Morris, 2009). The third dimension is task significance (per the JCM) \\nwhich connects one’s work to the wider world.',\n",
       "  'This dimen-\\nsion reflects the extent to which individuals can see how \\ntheir work benefits, and contributes to the betterment of, oth-\\ners (“serving others” from Lips-Wiersma & Morris, 2009). The fourth dimension is autonomy (per the JCM), which \\nreflects how freely individuals can determine their work \\napproaches and the extent of their freedom from intrusive \\nsurveillance and monitoring. The more autonomy workers \\nexperience the greater their capacity to engage in activi-\\nties like job crafting to enhance fit between individual needs \\nand job requirements, and to undertake work that fosters \\nself-development, moral cultivation, and that affords align-\\nment with one’s values (“developing and becoming self” and \\n“expressing full potential” from Lips-Wiersma & Morris, \\n2009). The final dimension is belongingness, reflecting the \\nways that work can help us feel connected to a wider group \\nto generate meaningfulness through a sense of unity with \\nothers (Bailey et\\xa0al., 2019; Lips-Wiersma & Morris, 2009; \\nMartela & Riekki, 2018). Now that we know what underpins \\nexperiences of meaning in work, we can turn to explaining \\nthe ethical dimensions of both meaningful work and AI. The Ethics of\\xa0Meaningful Work and\\xa0Ethical AI\\nRecent philosophical discussions of meaningfulness tend to \\nfocus on what makes life itself, or the activities and rela-\\ntionships that compose a well-lived life, meaningful (Wolf, \\n2010). The paradigm of meaningless work is Sisyphus, who \\nis condemned as punishment to repeatedly roll a rock to the \\ntop of a mountain (Camus, 1955). Sisyphus’ work is bor-\\ning, repetitive, simple, does not benefit others, and is not \\nfreely chosen.3 By implication, meaningful work should be \\nengaging, varied, require the use of complex skills, benefit \\nothers, and be freely chosen. This emphasises two aspects \\nof meaningfulness that Wolf (2010) calls subjective (do you \\nexperience work as meaningful?) and objective (is the work \\nactually meaningful?) elements.',\n",
       "  'As we take meaningful \\nwork to be “personally significant and worthwhile” (Lys-\\nova et\\xa0al., 2019, p. 375), our definition is inclusive of these \\n1\\u2002 Pratt and Ashforth (2003) also discuss meaningfulness at work, or \\nthe ways leaders craft and convey organisational values to build feel-\\nings of organisational membership. To maintain a manageable scope, \\nour analysis only examines meaning in work, which is largely driven \\nby job design.',\n",
       "  '2\\u2002 The job characteristics model also includes feedback.',\n",
       "  'We draw on \\nthe model’s first three aspects as they are theorised to directly gen-\\nerate the psychological state of experienced meaningfulness at work, \\nand both autonomy and belongingness are viewed in the wider litera-\\nture as other critical components of meaningful work.',\n",
       "  'See Parker and \\nGrote (2022) for an assessment of technology’s impact on feedback \\nat work. 3\\u2002 Of course, Sisyphus’ story is more complicated than this, with \\nCamus (1955) arguing that Sisyphus finds a form of happiness in his \\nscornful embrace of the absurdity of his condition.',\n",
       "  '728\\n\\t\\nS. Bankins, P.',\n",
       "  'Formosa \\n1 3\\nsubjective (it is personally significant) and objective (it is \\nworthwhile) aspects. The Ethical Implications of\\xa0Meaningful Work: Why \\nis\\xa0it Ethically Important? Literature in business ethics and political philosophy explore \\nthe ethical significance of meaningful work (Michaelson \\net\\xa0al., 2014).',\n",
       "  'Meaningful work can be viewed as ethically \\nsignificant either because it is intrinsically valuable (first \\nbasis), or because it is a constitutive element of a broader \\ngood (second basis), or because it is an instrumental good \\nthat leads to other valuable goods (third basis) (Michaelson \\net\\xa0al., 2014). From these three bases we can see that there \\nare good grounds for holding meaningful work to be ethi-\\ncally important across each of our three most used ethical \\ntheories: Kantian ethics, Virtue Theory, and Utilitarianism. Regarding the first basis, Kantian ethical theories focus \\non treating people with dignity and respect as rational \\nagents who have normative authority over their lives, and \\nthis includes imperfect duties to promote and develop the \\nrational capacities and self-chosen ends of moral agents \\n(Formosa, 2017). Meaningful work is ethically significant \\nas it provides an important way to develop and exercise one’s \\nrational capacities and use them in ways that help others to \\nmeet their ends.',\n",
       "  'Bowie (1998, p.',\n",
       "  '1083) identifies six features \\nof meaningful work that explain why Kantians should care \\nabout it, including that the work is “freely entered into”, \\n“not paternalistic”, ‘‘provides a wage sufficient for physi-\\ncal welfare”, allows workers to exercise their “autonomy \\nand independence”, “develop” their “rational capacities”, \\nand promotes their “moral development”. In terms of the \\nsecond basis, many virtue ethicists argue that meaningful \\nwork is an integral part of flourishing as a human being. For example, Nussbaum (2011) argues that “being able to \\nwork as a human being” is a central human capability. This \\nmeans being able to exercise our practical reason, use our \\nsenses, imagination and thought, have some control over our \\nwork environment, and being able to have “meaningful rela-\\ntions of mutual recognition with other workers” (Nussbaum, \\n2011, p. 34).',\n",
       "  'The capability to pursue meaningful work is \\nthus an important right and component of human flourish-\\ning. In terms of the third basis, evidence shows the positive \\ninstrumental impacts that meaningful work has on wellbeing \\nand a range of other goods (Allan et\\xa0al., 2019). This gives us \\ngood reasons to care about meaningful work for the sake of \\nother important goods it contributes to and promotes, such \\nas human wellbeing, that are valued on a range of ethical \\ntheories, including Utilitarianism. Overall, according to all three of our most used moral \\ntheories there are good reasons to care about meaningful \\nwork given that it respects workers’ autonomy and their abil-\\nity to exercise complex skills in helping others, contributes \\nto their wellbeing, and allows them to flourish as complex \\nhuman beings. Given its ethically valuable nature, it fol-\\nlows that organisations have strong pro tanto reasons to \\npromote, support, and offer meaningful work (Michaelson \\net\\xa0al., 2014). Of course, pro tanto reasons are not indefeasi-\\nble reasons, and so other considerations may outweigh them, \\nsuch as improved efficiency, which means changes that lead \\nto less meaningful work are not necessarily unethical. Fur-\\nther, some workers may be willing to trade off less mean-\\ningful work for other gains, such as more income or leisure \\ntime.',\n",
       "  'Even so, meaningful work remains ethically important \\nand changes that impact the amount of meaningful work \\nfor humans\\xa0must be taken into ethical account, even if such \\nconsiderations are not always overriding. The Ethical Implications of\\xa0AI Use\\nGiven the ethical importance of meaningful work, more \\nscholarship is needed to explore the potential impacts of \\nAI upon it. The ethical significance of AI use is widely \\nrecognised and discussed (see\\xa0Floridi et\\xa0al., 2018; Hagen-\\ndorff, 2020; Jobin et\\xa0al., 2019), leading to various organi-\\nsational, national, and international documents outlining \\nethical principles for AI deployment. However, AI’s effects \\non meaningful work are not a focus of any of these prin-\\nciples. For example, Jobin et\\xa0al.’s (2019) meta-analysis of \\nethical AI guidelines identifies 11 principles, but none men-\\ntion meaningful work directly. Hagendorff’s (2020) analysis \\nalso does not identify it, although related issues around the \\n“future of employment” are discussed.',\n",
       "  'An analysis by Ryan \\nand Stahl (2020, p. 67) mentions the need to “retrain and \\nretool” human workers who are fully replaced by AI, but this \\nsidelines human-AI collaborations in workplaces and AI’s \\nbroader impacts on meaningful work. The AI4People frame-\\nwork also makes no direct\\xa0mention of meaningful work, but \\nit does note the possibility of AI liberating people from the \\n“drudgery” of some work (Floridi et\\xa0al., 2018, p. 691). While these frameworks do not mention meaningful work \\nexplicitly, we can nonetheless draw on them to identify ethi-\\ncal concerns that AI’s impacts on meaningful work raise. To do this we draw on the AI4People ethical AI framework \\n(Floridi et\\xa0al., 2018) and its five principles of beneficence, \\nnon-maleficence, autonomy, justice, and explicability. We \\nutilise this widely discussed framework as it emerged from \\na robust consensus-building program to formulate ethical \\nAI principles. The framework’s focus on the impacts of AI \\non “human dignity and flourishing” across its elements of \\n“autonomous self-realisation… human agency… individual \\nand societal capabilities... [and] societal cohesion” (Floridi \\net\\xa0al., 2018, p. 690) also fits our focus, given that the impacts \\nof AI on dignity, autonomous agency, social cohesion, skills \\nand capabilities, and human flourishing all relate to our \\ndimensions of meaningful work.',\n",
       "  'The foundational principles',\n",
       "  '729\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nof this framework (minus explicability) have also been uti-\\nlised in related work on the ethical design and deployment of \\nbroader information technologies (see Wright, 2011), which \\nagain emphasises the framework’s usefulness in our context. The five principles of the AI4People framework allow us \\nto explore the wide-ranging impacts of AI on meaningful \\nwork. The first principle is beneficence, or the benefits AI \\ncan bring toward promoting human wellbeing and preserv-\\ning human dignity in an environmentally sustainable way. Non-maleficence is about ensuring that AI does not harm \\nhumanity, and this includes not violating individuals’ pri-\\nvacy and maintaining the safety and security of AI systems.',\n",
       "  'Autonomy is about giving humans the power to decide what \\nAI does. A linking concern between the first two princi-\\nples is the use of AI, intentionally or not, to cause harm \\nby interfering with and disrespecting human autonomy by \\n“nudging… human behaviour in undesirable ways” (Floridi \\net\\xa0al., 2018, p. 697).',\n",
       "  'Nudging involves setting up the “choice \\narchitecture”, or decision context, to intentionally attempt to \\npush (or “nudge”) people to make certain choices (Thaler & \\nSunstein, 2008). Justice is about fairly distributing the bene-\\nfits and burdens from AI use and not undermining solidarity \\nand social cohesion.',\n",
       "  'Finally, explicability is about ensuring \\nthat AI operates in ways that are intelligible and accountable, \\nso that we can understand how it works and we can require \\nsomeone to be responsible for its actions. In the context of \\nmeaningful work, these principles lead us to focus on the \\nbenefits and harms that AI can bring to workers, includ-\\ning on their tasks, skills and social relations, the way AI \\nmight control, nudge, and manipulate workers’ autonomy, \\nthe distribution of the benefits and harms AI brings, and the \\nextent of intelligibility and accountability in AI workplace \\ndeployments. Our overall conceptual framework is presented in Fig.',\n",
       "  \"1 \\nand our analysis is structured as follows. We first outline the \\nimpacts of AI on the five dimensions of meaningful work by \\nanalysing its effects through three pathways (outlined below) \\nfor AI deployment: replacement; ‘tending the machine’; and \\namplifying. We then turn to the AI4People ethical frame-\\nwork to draw out the ethical implications of these impacts \\non meaningful work. This structure allows us to focus in \\na systematic way on each important set of analyses, first \\nrelated to AI’s effects on meaningful work and then the ethi-\\ncal implications of this, while highlighting how these effects \\nare often contingent on the ways in which AI is deployed. The Effects of\\xa0Artificial Intelligence \\non\\xa0Meaningful Work\\nAI represents a range of technologies that can be used in \\nmany ways alongside human workers doing many different \\ntasks. This makes it important to examine not only what \\ntasks the AI does, but also how human workers’ tasks change \\nfollowing AI deployment and the comparative meaningful-\\nness of their new work. While we briefly discuss the impacts \\nof full human replacement by AI upon meaningful work, \\nwe focus our analysis on meaningful work outcomes when \\nhumans work alongside AI.4 This is because such work con-\\nfigurations already, and will increasingly, characterise many \\nworkplaces (Jarrahi, 2018) and reflects our focus on clear \\ncurrent and near-term impacts of narrow AI. Technology’s Effects on\\xa0Work: Three Paths\\nOur analytical framework adapts and expands Langlois' \\n(2003) characterisation of how technology integrates into a \\nwork process. This structures our analysis around three key \\npaths through which AI will shape humans’ experiences of \\nmeaningful work.\",\n",
       "  'In the first path, AI assumes some tasks (either simple or \\ncomplex) while workers remain engaged elsewhere in the \\nFig. 1\\u2002 \\u2009Overview of conceptual \\nframework\\nFive Dimensions of \\nMeaningful Work\\n1. Task integrity\\n2.',\n",
       "  'Skill cultivation and\\nuse\\n3. Task significance\\n4.',\n",
       "  'Autonomy\\n5. Belongingness\\nFive Ethical AI \\nPrinciples\\n1. Beneficence\\n2.',\n",
       "  'Non-maleficence\\n3. Autonomy\\n4.',\n",
       "  'Justice\\n5. Explicability\\nThree AI \\nImplementation \\nPathways\\n1.',\n",
       "  'Replacing\\n2.',\n",
       "  'Tending the \\nmachine\\nManaging the \\nmachine\\nMinding the \\nmachine\\n3. Amplifying\\nThe implementation \\nof AI impacts \\nmeaningful work \\ndimensions in \\ndifferent ways\\nThe outcomes of AI \\nimplementation on \\nmeaningful work are \\nethically assessed\\n•\\n•\\n4\\u2002 We do not significantly detail the effects of AI fully replacing a \\nworker because, at least currently, AI is unlikely to predominantly \\nautomate entire jobs (Chui et\\xa0al., 2015). But where this does occur \\nthe impacts are clear, the unemployed worker has lost meaning-\\nful paid work until they can find another job (which may offer more \\nopportunities for meaningful work, see Cheney et\\xa0 al., 2008). This \\nalso raises broader issues, beyond our scope, around other sources of \\nmeaningfulness if increasingly sophisticated AI makes paid meaning-\\nful work rarer (see Bruun & Duka, 2018).',\n",
       "  '730\\n\\t\\nS. Bankins, P.',\n",
       "  'Formosa \\n1 3\\n(roughly similar) work process.',\n",
       "  'This is akin to AI replacing \\nhumans in some tasks. For example, if a personalised maths \\nlearning app is introduced in a classroom, the teacher may \\nre-focus upon other existing tasks (e.g., more time for les-\\nson planning) or undertake new work (e.g., individualised \\nmaths coaching), but the overall work process of ‘teach-\\ning’ remains similar (see such examples in\\xa0Acemoglu & \\nRestrepo, 2020). We also focus on the two ends of the skills \\nspectrum for illustrative purposes (i.e., simple and complex \\ntasks), and acknowledge that tasks will likely involve vari-\\nous skills. The key difference between this path and the next \\nis that here the replacement work undertaken by humans is \\nnot focused on managing the AI, but in the next path it is. In the second path, AI assumes a set of tasks resulting in \\nnew human work focused on “tending the machine” (Lan-\\nglois, 2003, p. 175). This is akin to creating new types of \\ntasks for workers.5 We further divide ‘tending the machine’ \\ninto two emerging forms of work associated with manag-\\ning AI: (1) what we term ‘managing the machine’, which \\ngenerates new, complex, and interesting forms of work for \\nhumans; and (2) what Langlois (2003, p. 175) terms “mind-\\ning the machine”, which generates more mundane, rote, and \\nlower-skilled work for humans. Again, we focus on two ends \\nof a spectrum for illustrative purposes, while acknowledging \\nthat human work may exist across both categories. ‘Man-\\naging the machine’ reflects integrated and complex work, \\nsuch as “coordination and buffering” roles (Langlois, 2003, \\np.',\n",
       "  '175), as well as trainer, explainer, and sustainer roles \\n(Daugherty & Wilson, 2018). Examples include: manag-\\ning the interactions between data, the wider organisation, \\nand other stakeholders (coordination and buffering); train-\\ning the AI to complete tasks and training others in AI use \\n(training); explaining and interpreting the AI’s operation \\nand outputs to stakeholders (explaining); and ensuring the \\nsystem’s continued explainability, accountability, and fair-\\nness (sustaining) (Daugherty & Wilson, 2018). In contrast, \\n‘minding the machine’ work involves tasks such as “AI prep-\\naration” (sourcing, annotating, and labelling data) and “AI \\nverification” through validating AI output (such as checking \\nimage recognition accuracy) (Tubaro et\\xa0al., 2020, p.',\n",
       "  '1). This \\ntype of work tends to reflect fragmented and disconnected \\nmicro-work tasks that are often outsourced to low wage and \\nlow skill workers (Tubaro et\\xa0al., 2020), leading to charac-\\nterisations of “janitor work” and new digitalised forms of \\nTaylorism (Jarrahi, 2019, p. 183).',\n",
       "  'In the third path, AI ‘amplifies’ or ‘assists’ workers \\nby improving how human workers do their existing work \\n(Daugherty & Wilson, 2018). This is akin to AI assisting \\nworkers with their tasks and/or augmenting and enhanc-\\ning workers’ abilities.',\n",
       "  'Here AI is neither assuming specific \\ntasks that a human previously did (as in the first path) nor \\ndoes managing the AI constitute a worker’s primary role \\n(as in the second path), but rather the technology assists the \\nworker to do her existing work better. For example, the AI \\nCorti provides real-time assistance to emergency operators \\nby analysing callers’ responses to questions, assessing the \\nseverity of their condition, and recommending actions to the \\noperator based on modelling of thousands of previous calls \\n(Formosa & Ryan, 2021). This amplifies, in a significant \\nnew way, the abilities of emergency operators to determine \\noptimal responses.',\n",
       "  'The use of AI to amplify a human worker \\naccords with Zuboff’s (1988) “informating” powers of tech-\\nnology, whereby it improves humans’ access to integrated \\nand more meaningful forms of data, often cross-functionally, \\nto generate new insights (see Jarrahi, 2019). We now analyse how, through each of these three deploy-\\nment pathways, AI use will impact the five dimensions of \\nmeaningful work.',\n",
       "  'While individual jobs could experience \\nelements of all three paths (e.g., some replacing, some ‘tend-\\ning the machine’ work, and some amplifying) and some \\noverlap may occur (e.g., AI replacing a rote human task also \\nassists the worker), we discuss each path as distinct for ana-\\nlytical purposes. The ethical implications of these impacts \\nare then assessed in the subsequent section.',\n",
       "  'Task Integrity and\\xa0Skill Cultivation and\\xa0Use\\nWorkers’ tasks can range from being highly fragmented to \\nbeing highly integrated, and the diversity of skills they can \\nactivate will also vary as a result. Both of\\xa0these aspects gen-\\nerate opportunities to achieve and develop one’s abilities \\nand potential through work (Lips-Wiersma & Morris, 2009). As the nature of what a worker does (i.e., tasks) strongly \\nimpacts what they need to do that work (i.e., skills), we dis-\\ncuss these two dimensions together. First, we consider the path of AI taking over some tasks \\nwhile leaving workers engaged in other work.',\n",
       "  'The tasks \\nthe AI assumes could be simple or complex (or anything in \\nbetween), but the predominance of narrow AI means it is \\nmainly deployed to replace humans in specific narrow tasks. An espoused benefit of AI is its ability to undertake simple \\ntasks that are often boring and unchallenging for humans, \\nsuch as collating information for meetings (Pulse\\u2009+\\u2009IT, \\n2020) or assessing fruit quality (Roberts, 2020). Deploying \\nAI in this way is unlikely to generate significant feelings \\nof marginalisation from a wider work process due to the \\nsimple nature of the tasks it is assuming, particularly when \\nthe human takes on other comparable or more interesting \\nwork. This should result in neutral or improved perceptions \\n5\\u2002 We acknowledge that other forms of new human work are also \\nlikely to emerge (see Acemoglu & Restrepo, 2020), but its nature \\nremains speculative. New work associated with AI management \\nalready exists or is emerging, aligning with our focus on near-term \\nwork implications of AI.',\n",
       "  '731\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nof task integrity and may free workers’ time to engage in \\nmore learning and development. However, when AI assumes more complex and significant \\ntasks then its implications, both positive and negative, may \\nbe more profound. For example, in human resource man-\\nagement an AI can shortlist candidates to progress to inter-\\nviews based on natural language processing of applications \\n(Bankins, 2021; Leicht-Deobald et\\xa0al., 2019). Shortlisting \\napplicants can be a complex and significant component of \\nthe recruitment and selection process.',\n",
       "  'Using AI for this task \\ncould then degrade workers’ experiences of task integrity as \\nthey no longer undertake a significant part of a work process, \\nassuming this work is not comparably replaced. Shifting \\nworkers to other more rote tasks, despite adding work that \\nmaintains their level of involvement in the work process, is \\nalso likely to compound feelings of reduced task integrity, as \\nthe worker moves from undertaking more significant to less \\nsignificant work. This can also limit the scope for workers to \\ndevelop and express their full capabilities at work and reduce \\ntheir opportunities for growth.',\n",
       "  'In contrast, if workers shift to new but similarly complex \\nor even more significant tasks elsewhere in the work process, \\nthen this should support task integrity as the worker contin-\\nues to contribute meaningfully to work outcomes. For exam-\\nple, the AI ‘AlphaFold’ developed by DeepMind is designed \\nto automate and accelerate the process of determining pro-\\ntein structures, an important step in developing new treat-\\nments for human diseases (Hassabis & Revell, 2021). While \\nAlphaFold can assume significant tasks previously done by \\nhuman scientists (i.e., determining protein structures) this \\nshould positively impact, or at least have a neutral effect, \\non task integrity if it allows scientists to re-focus their work \\nefforts on other important aspects of their broader goal of \\ncuring diseases. However, there remain risks to AI being \\nused in this way. Continuing with this example, if scientists \\nhave trained for many years to do the experimental work \\nthat AlphaFold can now do more quickly and accurately, \\nthis generates significant risks for their ability to exercise \\ntheir full capacities, demonstrate their mastery, and utilise \\nthe skills they have invested years in developing to reach \\ntheir full potential. Changes in skill cultivation and use due to technology \\nreplacing either simple or complex tasks also raises deskill-\\ning concerns, whereby skilled human work is offloaded to \\nmachines resulting in skill loss (Vallor, 2015). Ethically, it \\nis critical to establish whether the human skills lost (i.e., \\noffloaded to machines) are important and whether they can \\nbe exercised and maintained through other forms of work or \\nin other life domains (Michaelson et\\xa0al., 2014; Wolf, 2010). As simple and rote work generally requires basic skills that \\ncan be cultivated elsewhere or are not significant, there is \\nlimited scope for significant deskilling in this case.',\n",
       "  'How-\\never, complex tasks generally require complex skills, such \\nas judgement, intuition, context awareness, and ethical think-\\ning. From a deskilling perspective, these types of skills are \\nparticularly ethically problematic for workers to risk losing.',\n",
       "  'This means when workers are left with fewer overall com-\\nplex and significant tasks following AI deployment, then \\ntheir ability to cultivate and use important skills will likely \\ndecrease, negatively impacting this dimension of meaning-\\nful work. It is worth noting that where replacement involves AI \\nassuming a worker’s whole job, for example where the job \\nis constituted entirely of simple and rote tasks that are most \\nsusceptible to full automation (Gibbs, 2017), this will likely \\nlead to unemployment (if redeployment is not possible). This \\neffectively removes, at least temporarily, paid meaningful \\nwork from that worker’s life and poses the greatest risk to \\nthe ability to experience meaningful work. This also pro-\\nvides the conditions for a wide range of skills to be lost or \\ndegraded, as well as having significant negative impacts on \\nimportant self-attitudes, such as feelings of self-respect and \\nself-worth\\xa0(see Selenko et\\xa0al., 2022 for work on AI use and \\nemployees’ sense of\\xa0identity). This case also raises broader \\npolitical questions about how society should deal with such \\na scenario should it become more widespread (Hughes, \\n2014).',\n",
       "  'While these questions are beyond our focus here, \\nwe do highlight them in our discussion of future research \\ndirections. Second, we consider the path of workers ‘tending the \\nmachine’, whether in ‘managing’ or ‘minding’ forms. ‘Man-\\naging the machine’ work should enhance what Bourmault \\nand Anteby (2020, p. 1453) term “administrative responsi-\\nbility”, through offering a wider scope and variety of duties. This should enhance task integrity where the shift to coor-\\ndination and buffering work provides opportunities for inte-\\ngrated and challenging activities across training, explaining, \\nand sustaining roles through supervisory work, technology \\noversight, exceptions management, and cross-functional \\ncoordination of entire work processes.',\n",
       "  'Such coordination \\nand buffering work will also require the development of \\nflexible and wide-ranging skill sets (Langlois, 2003), sup-\\nporting skill cultivation and use and more broadly widening \\nand deepening one’s ability to learn, achieve, and develop \\nat work. In contrast, rather than generating more complex and \\ninteresting human work, ‘minding the machine’ produces a \\n“more benignant role for humans” through more mundane \\nand rote tasks (Langlois, 2003, p. 174).',\n",
       "  'This would reduce \\ntask integrity as workers become more distanced from their \\nwork outcomes. The generally repetitive and fragmented \\nnature of ‘minding the machine’ work also suggests its asso-\\nciated skills are low and narrow, offering little opportunity \\nfor varied skill cultivation. Such AI “janitor work” (Jarrahi, \\n2019, p. 183) risks degrading workers’ abilities to meaning-\\nfully develop their capabilities and reach and express their',\n",
       "  '732\\n\\t\\nS. Bankins, P.',\n",
       "  'Formosa \\n1 3\\nfull potential at work, leading to lower levels of meaningful-\\nness on this dimension. Third, when AI amplifies workers’ abilities to do their \\ncurrent tasks, positive impacts on task integrity and skill \\ncultivation and use should ensue. For example, in the polic-\\ning domain, machine learning technologies\\xa0can collate previ-\\nously disparate data sources to analyse characteristics and \\nhistories of domestic violence victims and perpetrators to \\nbetter predict, compared to current human-driven systems, \\nrepeat attacks and better prioritise preventative actions \\n(Grogger et\\xa0al., 2020).6 In such cases, experiences of task \\nintegrity are likely to remain consistent or improve as AI \\nsupports workers to better complete their tasks and achieve \\nwork goals. Skill cultivation and use should remain neutral \\nor improve as it is likely that workers, while maintaining \\ntheir current skills, will need to develop new ones to inter-\\npret and integrate AI output into their decision making. However, a feature of AI that may constrain skill use \\nacross all three paths is its ‘blackbox’ nature (Boden, 2016). While AI designers are developing ways to improve lay per-\\nson interfaces, the use of ‘blackbox’ (or unexplainable) AI \\nin workplaces may degrade workers’ skill cultivation, use, \\nand feelings of competence. For example, where workers \\nare highly reliant on the decision making of an AI, they may \\nfeel lower levels of competence in their use of it due to little \\nunderstanding of its functioning.',\n",
       "  'This effect will likely be \\nmore acutely felt where workers are expected to understand \\nand explain what the AI is doing. Poor explainability can \\nalso create opaque chains of accountability for decisions \\ninformed by AI (Dahl, 2018) and this risks making workers \\noverly dependent on an AI that they cannot comprehend. Task Significance\\nTask significance means employees see their work as having \\npositive impacts (Grant, 2008) through their service to oth-\\ners (Lips-Wiersma & Morris, 2009), within or outside the \\norganisation (Hackman & Oldham, 1975). Task significance \\nis influenced by how employees assess their job impact on \\nand contact with beneficiaries (Grant, 2007). Job impacts \\non beneficiaries are shaped by the dimensions of magni-\\ntude, scope, frequency, and focus (i.e., preventing harms \\nor promoting benefits) (Grant, 2007).',\n",
       "  'Contact with benefi-\\nciaries is shaped by the dimensions of frequency, duration, \\nphysical proximity (including virtual proximity), depth, and \\nbreadth of contact (Grant, 2007).',\n",
       "  'Given the range of these \\ndimensions, workers’ assessments of task significance can \\nbe complex. Evidence suggests that\\xa0employees can derive \\ntask significance from even objectively rote, mundane, and \\nlow skill work (the\\xa0objective element\\xa0of meaningful work), \\nwhen that work is framed in the right way (the\\xa0subjective \\nelement\\xa0of meaningful work). For example, Carton (2018, \\np. 323) shows that when leaders at NASA carefully framed \\nthe space agency’s goals, workers could connect work such \\nas “mopping the floor” to “helping put a man on the moon”. However, carrying out impactful tasks without opportuni-\\nties for “personal, emotional connections to the beneficiar-\\nies of those tasks” can impede overall experiences of task \\nsignificance (Grant, 2007, p.',\n",
       "  '398; Bourmault & Anteby, \\n2020).',\n",
       "  'This means that both job impact on beneficiaries and \\ncontact with them are important to assess. Given this, and \\nfollowing Grant (2007), we suggest that employees’ global \\nassessments of the impact of AI on their jobs, rather than on \\nspecific tasks, is most relevant when assessing perceptions \\nof task significance. First, we consider the path of AI taking over simple or \\ncomplex tasks while workers remain engaged elsewhere. Given our focus here\\xa0at the job level, if only some sim-\\nple tasks are assumed by an AI this should have limited \\nimpact on task significance, assuming the remaining or \\nnew tasks provide opportunities for workers to positively \\nimpact and connect with beneficiaries. In contrast, when \\nAI assumes more complex tasks, these are likely significant \\nto an individual’s overall assessments of task significance.',\n",
       "  'This may lead to more extensive and complex sensemak-\\ning of this change. To see this, we draw on construal-level \\ntheory (CLT), which describes the way individuals cogni-\\ntively represent people or events at either higher or lower \\nlevels of abstraction (Trope & Liberman, 2003). Higher lev-\\nels of abstraction involve “mental representations that are \\nrelatively broad, inclusive, (and) general”, such as higher-\\nlevel goals or principles (Wiesenfeld et\\xa0al., 2017, p. 368). Lower levels of abstraction involve “applying relatively \\nspecific, detailed, and contextualised representations”, such \\nas focusing on lower-level actions to achieve higher-level \\ngoals (Wiesenfeld et\\xa0al., 2017, p.',\n",
       "  '368). Returning to the \\nearlier\\xa0AlphaFold example, at a higher level of construal \\nworkers may perceive improved task significance regard-\\ning job impact as the AI is significantly contributing to the \\nhigher-level goal of treating diseases. This could facilitate \\nhigher perceptions of magnitude, scope, and frequency of \\npositive impact. At a lower level of construal, the worker \\nmay then ask: “but what am I doing to help meet this goal?”.',\n",
       "  'If workers can re-focus on other comparatively significant \\ntasks in the work process, they should experience higher \\ntask significance as the AI helps advance the field toward \\nreaching the overarching goal and the worker continues to \\nmeaningfully contribute toward that goal. However, where \\nthe remaining or new tasks fail, at lower levels of construal, \\nto deliver at least the same experiences of task significance \\n6\\u2002 Although in practice such predictive policing systems have been \\nshown to risk biased outcomes against minority groups, driven by \\nover-representation of those groups in policing statistics (Berk, 2021). We discuss these issues in a later section.',\n",
       "  '733\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nas before, then one’s perceived ability to ‘serve others’ is \\nlikely to degrade overall. In cases of both simple and complex task change, \\nwhere AI is used in ways that have sub-optimal, biased, \\nunjust, or harmful outcomes for end users, this could also \\ndecrease\\xa0workers’ perceptions of task significance. For \\nexample, where AI provides facial recognition and predic-\\ntive policing data for law enforcement agencies and the AI’s \\noutputs are biased against minority groups, then workers \\nmay see reduced task significance given their organisations’ \\nconnections to negative outcomes (via the negative magni-\\ntude, scope, and frequency dimensions of job impact). Impli-\\ncating workers in injustices and harms perpetrated by an \\nAI, through their involvement with or responsibility for the \\ntechnology, can particularly diminish the experience of serv-\\ning others and the autonomous ability to act in alignment \\nwith one’s values and morals (related to ‘developing and \\nbecoming self’), degrading overall work meaningfulness.',\n",
       "  'Second, we consider the ‘tending the machine’ path.',\n",
       "  'In \\nterms of ‘managing the machine’, the heightened adminis-\\ntrative responsibility associated with such work (Bourmault \\n& Anteby, 2020) should improve task significance through \\njob impact and more opportunities to benefit others, given \\nthe expansive duties this work entails (i.e., enhanced scope \\nof impact). However, such work can distance workers from \\nthose they serve, diminishing feelings of direct “personal \\nresponsibility” (Bourmault & Anteby, 2020, p. 1453) or feel-\\nings of having a direct and significant impact on the lives \\nof others, which can reduce task significance. For example, \\nwhen replaced by autonomously driven trains and moved to \\n‘managing the machine’ work, metro train drivers experi-\\nenced enhanced administrative responsibility but diminished \\npersonal responsibility, alongside lower task significance \\noverall, as they were no longer directly responsible for com-\\nmuters’ safety (Bourmault & Anteby, 2020). In terms of ‘minding the machine’ work we suggest that \\ntask significance will generally be reduced.',\n",
       "  'This is because \\nsuch fragmented work means workers may have little idea of \\nthe point of their labour and its impacts, potentially limiting \\nall job impact dimensions. As they may also\\xa0be working in \\nisolation from others because of outsourcing\\xa0(Tubaro et\\xa0al., \\n2020), potentially limiting all contact with beneficiaries, this \\nfurther disconnects workers’ tasks from the end user benefits \\ngenerated, eroding task significance. Third, when AI amplifies a worker’s abilities this should \\nhave significant and positive implications for task signifi-\\ncance, particularly through the magnitude and focus dimen-\\nsions of job impact and the duration and depth dimensions \\nof contact with beneficiaries.',\n",
       "  'Here, the AI is not focused \\non substantially changing the range of tasks in a work pro-\\ncess, but rather on improving something that humans were \\nalready doing in that process, leading to better outcomes for \\nbeneficiaries. Drawing on earlier amplification examples, \\nwhere AI can support police officers by collating and ana-\\nlysing new data sources to help them better prevent inci-\\ndences of domestic violence (assuming it does not do so in \\nunfair or biased ways), then this should heighten percep-\\ntions of both being able to achieve higher-level goals (e.g., \\npreventing crime) and seeing the importance and connec-\\ntion of lower-level tasks to reaching that goal (e.g., through \\ninterpreting better predictive analytics). Use of AI in this \\nway can also help reduce human biases in decision making, \\nsuch as through building fairness principles into AI systems \\n(see Selbst et\\xa0al., 2019). In recruitment, for example, AI \\ncan limit the impact of unconscious human biases and vari-\\nous other human constraints on rational decision making by \\nassessing all candidates’ applications against standard cri-\\nteria and providing auditable, transparent, and explainable \\ndecision trails (see Hagras, 2018; Bankins et\\xa0al., 2022). This \\npath demonstrates that a significant potential benefit of AI \\nis that it can elevate humans’ abilities to address complex \\nproblems, enhance the impact of their work, and thus better \\nserve others, through its analysis of large datasets to identify \\nnovel insights. Autonomy\\nAutonomy means self-rule.',\n",
       "  'Individually, that means being \\nable to do what you really want to do.',\n",
       "  'In addition to the free-\\ndom from interference needed to rule yourself, autonomy is \\nalso commonly taken to include competency (i.e., you have \\nthe skills and capacities needed to rule yourself) and authen-\\nticity (i.e., your ends are authentically your own and not the \\nresult of oppression, manipulation, or coercion) conditions \\n(Formosa, 2021). In the workplace, autonomy refers to “the \\ndegree to which the job provides substantial freedom, inde-\\npendence, and discretion to the individual in scheduling the \\nwork and in determining the procedures to be used in carry-\\ning it out” (Hackman & Oldham, 1976, p.',\n",
       "  '258).',\n",
       "  'AI’s impact \\non individuals’ autonomy is a key issue for the ethical AI \\nliterature. A particular concern is that ceding authority to AI \\ndiminishes human autonomy (Floridi et\\xa0al., 2018).',\n",
       "  'However, \\npotential benefits for human autonomy can also accrue from \\nincreasing AI’s autonomy. We assess these different impacts \\nof AI at work as either promoting or diminishing autonomy \\nacross competency and authenticity conditions. In terms of promoting human autonomy, this depends on \\nwhat work the AI assumes but also, and more importantly, \\non what work takes its place and what control and input \\nworkers have over AI deployment. When AI assumes simple \\nor complex tasks that workers find boring or repetitive, then \\nthis potentially promotes autonomy by freeing up time for \\nworkers to build their autonomy competencies through doing \\nother more challenging or authentic work. For example, if an \\nAI prioritises a worker’s emails so that she only sees those \\nrequiring a response, this may free her to work on other',\n",
       "  '734\\n\\t\\nS. Bankins, P.',\n",
       "  'Formosa \\n1 3\\nmore valuable tasks. In terms of ‘managing the machine’, \\nthis path could promote autonomy if new work is more skil-\\nful and engaging than the work it replaces, and if workers \\nhave a degree of control over how that work is done. Where \\nAI amplifies workers by giving them more power and use-\\nful information, then this can improve worker autonomy by \\nhelping them to better achieve their self-given ends.',\n",
       "  'In terms of diminishing human autonomy, these impacts \\nare partly the converse of the above. In our first path, if \\ncomplex, interesting, and creative tasks that workers want \\nto do are assumed by AIs, this potentially diminishes auton-\\nomy. There may also be good reasons why humans should \\nremain engaged in certain complex tasks and decisions, such \\nas due to their moral complexity. This means that where AI \\nassumes these tasks it can diminish human achievement of \\nvaluable ends, degrade important human skills, and limit \\nopportunities for moral development (Lips-Wiersma & Mor-\\nris, 2009). For example, when we delegate to AI decisions \\nregarding ethically sensitive aspects of human resource man-\\nagement, the skills associated with that work can degrade \\nand thereby diminish important autonomy competencies.',\n",
       "  'AI \\ncan also make our autonomy more vulnerable by making us \\ndependent on it, which means our autonomy can diminish \\nif access to the technology is removed. Across the ‘tending \\nthe machine’ path, through ‘managing the machine’ work AI \\ncan diminish worker autonomy by filtering and potentially \\nrestricting the information that is made available for humans \\nto view and use (Kellogg et\\xa0al., 2020). Such constraints can \\nlimit the ability for workers to authentically develop them-\\nselves and their capabilities at work. Broader autonomy con-\\ncerns also exist with ‘minding the machine’ work, which \\nis itself mundane and boring, making workers feel like a \\n‘slave to the machine’ (Engel, 2019) and thereby experienc-\\ning\\xa0diminished autonomy at work. Across all paths, a more pernicious threat to autonomy \\nmay exist through surveillance and manipulation by AI. This reflects what Foucault calls the rise of a “surveillance \\nsociety”, which seeks to control bodies through making \\npeople feel permanently monitored (Abrams, 2004). When \\npeople are surveilled they tend to feel constrained and act \\nin less authentic and autonomous ways (Molitorisz, 2020).',\n",
       "  'The use of AI to surveil workers will likely have similar \\nimpacts and can be a way for employers to use their power \\nto exert control over employees. For example, the use of \\nAI-powered cameras to surveil Amazon delivery drivers \\ncould make them more self-conscious in their trucks, which \\ncould lead them to feel more constrained and unable to act \\nautonomously (Asher-Schapiro, 2021). A similar example \\nis when AI is implemented to monitor online meetings and \\nmeasure whether workers are engaged and contributing to \\nthe discussion (see Pardes, 2020), which could lead to stress \\nand inauthentic behaviour. Such monitoring could also result \\nin workers engaging in intentional “deviance” to challenge \\nthe control of surveillance (Abrams, 2004), by trying to \\n“game” the AI by matching or openly flouting what the \\nAI is expecting in terms of eye contact and body language \\n(Pardes, 2020), or finding other ways to operate outside the \\ngaze of the surveillance system. Belongingness\\nBelongingness refers to “the meaningfulness of work-\\ning together with other human beings” (Lips-Wiersma & \\nWright, 2012, p.',\n",
       "  '673).',\n",
       "  'Across all our paths, we argue that \\nAI may impact workers’ belongingness in two main ways: \\nthrough generating the conditions for more or less meaning-\\nful connections and a sense of unity with others; and through \\nits implementation creating differences across workers that \\nundermines solidarity.',\n",
       "  'In terms of the first way, where AI assumes tasks that \\nmay otherwise have required in-person and face-to-face \\ninteraction with other workers or customers, this can create \\nless human contact in the workplace. For example, where \\nan AI chatbot allows workers to access information previ-\\nously provided by a human worker, this lessens that worker’s \\ninteractions with other humans and reduces opportunities \\nfor forming connections with others that are the bedrock for \\ngenerating a sense of belonging (Seppala et\\xa0al., 2013). In \\ncontrast, AI use may increase opportunities for human inter-\\naction, for example through ‘managing the machine’ work \\nwhere workers are responsible for supervising AI deploy-\\nment that requires extensive human-to-human training. In terms of the second way, a key concern in the ethical \\nAI literature is how AI use may disproportionately and nega-\\ntively affect lower-skilled and lower-paid workers, while its \\nbenefits may disproportionately accrue to those with higher \\nskills and wages (Ernst et\\xa0al., 2018), effectively creating new \\ntypes of workplace in-groups and out-groups. For example, \\nmany of the negative impacts of AI at work, such as surveil-\\nlance and simplistic ‘minding the machine’ work, will tend \\nto fall on less skilled ‘blue collar’ workers, whereas more of \\nthe amplifying and autonomy-enhancing benefits associated \\nwith taking on even more interesting and engaging work \\nwill tend to fall to already privileged workers. This creates \\njustice concerns around how the benefits and burdens of AI \\nin workplaces are being distributed, potentially undermining \\nsolidarity between those who benefit from AI’s introduction \\nand those who do not. For example, in a call centre context \\nan AI may be used to monitor and evaluate the calls of every \\ncall centre operator.',\n",
       "  'Such heightened surveillance may be \\nperceived by operators as intrusive and diminishing their \\nautonomy. However, using AI in this way may amplify the \\nwork of quality assurance staff in the same organisation, \\nproviding them with more information and assisting them \\nin better training and managing operators. This shows how \\nAI may generate distinct groups experiencing very different',\n",
       "  '735\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nimpacts, such as being viewed as unnecessary surveillance \\nby some but as an amplifying source of information by oth-\\ners. Such outcomes particularly threaten the ability to create \\na sense of belongingness and shared values (Lips-Wiersma \\n& Morris, 2009), which underpins the ‘unity with others’ \\ndimension of meaningful work. Ethical Implications: AI and\\xa0Meaningful \\nWork\\nWe have analysed how the three paths of AI deployment \\nmay enhance or diminish opportunities for meaningful work \\nacross five dimensions. We now surface the ethical implica-\\ntions of this analysis via the five principles of the AI4Peo-\\nple ethical AI framework (Floridi et\\xa0al, 2018): beneficence; \\nnon-maleficence; autonomy; justice; and explicability.',\n",
       "  'As \\nwith any principlist framework there are potential conflicts \\nand tensions between principles (Formosa et\\xa0al., 2021). For \\nexample, there may be benefits for some from AI deploy-\\nment (beneficence) while others suffer harm (non-malefi-\\ncence) or interference with their autonomy. As identified \\nearlier, the\\xa0provision of meaningful work is not always the \\nonly or most important ethical value at stake, and so less \\nmeaningful work may not be ethically worse overall if there \\nare other ethical benefits, such as improved wellbeing for \\nothers through higher productivity. To assess ethical implications, we synthesise and summa-\\nrise how the three paths (replacing, ‘tending the machine’, \\nand amplifying) support or limit experiences of meaningful \\nwork and so contribute to, or diminish, meeting the AI4Peo-\\nple principles.',\n",
       "  'We summarise these impacts in Table\\xa01 across \\nthe five ethical principles (beneficence and non-maleficence \\nare combined in the Table as the latter reflects the converse \\nof the former), while noting the main deployment pathways \\nthrough which these impacts occur.',\n",
       "  'In terms of the beneficence principle, there can be sig-\\nnificant benefits for employees when AI use supports the \\nvarious dimensions of meaningful work.',\n",
       "  'When AI amplifies \\na worker’s skills it can support them to complete their tasks, \\nundertake more complex tasks, and utilise higher-order \\nthinking and analysis skills (task integrity and skill cultiva-\\ntion and use). It can also afford workers the opportunity to \\nachieve better outcomes and enhance the positive impact of \\ntheir work on beneficiaries (task significance), give them \\nmore control over their work through improved access to \\ninformation (autonomy), and potentially generate new con-\\nnections with other workers and stakeholders (belonging-\\nness). Similarly, when AI assumes some simple or complex \\ntasks and the human worker can re-focus on other impor-\\ntant and challenging tasks in the work process, then posi-\\ntive experiences across all dimensions of meaningful work \\nshould be maintained or improved. ‘Managing the machine’ \\nwork can also improve meaningfulness through a wider \\nscope of enriched work (task integrity and skill cultivation \\nand use) and a wider positive job impact within and outside \\nthe organisation (task significance), as well as greater inter-\\naction with a range of stakeholders through coordination and \\nsupervisory work (belongingness). In terms of the non-maleficence principle, we also show \\nthe harms that AI can create when it is deployed in ways that \\nlead to less (or no) meaningful work, or other related harms.',\n",
       "  'Two paths generate greatest risk of harms through signifi-\\ncantly reducing experiences of meaningful work.',\n",
       "  'First, when \\nAI replaces some tasks, the risk of degraded task integ-\\nrity, deskilling, reduced task significance, and constrained \\nautonomy is greatest when it assumes more complex tasks \\nand the worker is not afforded any new comparable or more \\ninteresting work. This is because complex tasks generally \\nconstitute a large and significant part of the work process \\nand undertaking them exercises a range of important skills.',\n",
       "  'Being removed from such work can also distance workers \\nfrom the output of their labour and lower perceptions of \\nbeneficiary impact. In the worst case, it could involve the \\ncomplete loss of paid meaningful work where AI replaces \\nwhole jobs, which removes workers from important social \\nrelationships and denies them the opportunity to skilfully \\nutilise their talents to help others. Second, ‘minding the \\nmachine’ work, as we have characterised its fragmented, \\npiecemeal, and micro-work nature, threatens these same \\naspects of meaningful work and feelings of belongingness \\nwhen work is outsourced to disconnected workers.',\n",
       "  'Other \\npaths can also generate harms, but arguably at lower lev-\\nels.',\n",
       "  'For example, we identified that while ‘managing the \\nmachine’ work may increase meaningful work experiences \\noverall through heightened administrative responsibility, it \\ncan lessen feelings of task significance by increasing dis-\\ntance between workers and their beneficiaries and reducing \\nfeelings of personal responsibility. In terms of the autonomy principle, across each path we \\nshow how autonomy is supported when AI is used to free \\nup humans to focus their time on other more valued tasks, \\nallows them to develop new or enhanced autonomy com-\\npetencies, and gives them more control over their work. In \\nparticular, the task replacement, ‘managing the machine’, \\nand amplifying paths that afford employees access to bet-\\nter data and information, the opportunity to engage in more \\ninteresting work, and exercise more control over how their \\nwork is done, can all promote autonomy as a dimension of \\nmeaningful work.',\n",
       "  'However, many of these positive impacts \\nalso depend on whether workers have input into how AI \\nis deployed in their organisations. A particular risk to \\nautonomy is the use of AI to surveil and monitor, which can \\nundermine authenticity and encourage workers to align their \\nbehaviours with the AI’s implicit expectations or seek ways \\nto subvert or avoid its control.',\n",
       "  '736\\n\\t\\nS.',\n",
       "  'Bankins, P.',\n",
       "  'Formosa \\n1 3\\nThe justice principle centres on ensuring fair, just, and \\nnon-discriminatory outcomes from AI and requires a focus \\non how the benefits and burdens of AI use are distributed. For example, the amplifying path generally achieves strongly \\npositive outcomes for meaningful work, but there is evidence \\nthat such benefits are disproportionately allocated to already \\nprivileged workforces (i.e., higher-skilled and higher-paid \\nworkers). In contrast, the ‘minding the machine’ path gen-\\nerally achieves strongly negative outcomes for meaningful \\nwork, but such burdens tend to disproportionately impact \\nless privileged workforces (i.e., lower-paid and lower-skilled \\nworkers). Lower-skilled workers are also more likely to \\nhave their entire jobs replaced by AI (Gibbs, 2017). This \\nuneven distribution raises important justice concerns and \\ncan undermine solidarity and feelings of belongingness \\nwithin and across work groups.',\n",
       "  'However, AI can also be \\ndeployed to promote justice, which can positively impact \\ntask significance. For example, when AI is used to mini-\\nmise bias and maximise evidence-based decision making \\nthrough giving workers access to new data-driven insights \\n(such as through amplification, ‘managing the machine’, or \\nreplacing complex tasks paths), this promotes fair outcomes \\nwhile also enhancing task significance through a greater \\npositive impact on beneficiaries. But the converse also holds \\nwhen the justice principle is threatened by an AI trained on \\nbiased datasets and deployed in workplaces where it gener-\\nates unjust outcomes that can decrease task significance and \\nimplicate workers in injustices. Finally, the explicability principle relates to the explain-\\nability, transparency, and accountability of AI. In paths \\nwhere AI plays a significant role alongside human workers, \\nsuch as the amplifying, ‘managing the machine’, and the \\nreplacement of complex tasks paths, an inability of work-\\ners to understand an AI’s operation, particularly where they \\nTable\\u202f1\\u2002 \\u2009Ethical impacts of AI for enhancing or diminishing meaningful work\\nEthical aspects of meaningful work (AI4 \\nPeople principles)\\nHow AI could enhance meaningful work (and \\nmain pathways)\\nHow AI could diminish meaningful work (and \\nmain pathways)\\nBeneficence & Non-maleficence\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Less boring and repetitive human work\\n• Increased opportunities for new and/or more \\nchallenging human work\\n• Enhanced human learning, skills, and \\ndevelopment\\n• Augmenting and assisting human workers\\n• Facilitating higher positive impacts on \\nothers\\nMain pathways: Replacing & minding the \\nmachine\\n• Loss of work for humans\\n• Reduced human role in the work process\\n• Human deskilling\\n• More boring and fragmented ‘minding the \\nmachine’ work\\n• Less belonging and less human interaction\\nAutonomy\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Improved autonomy competencies (e.g., \\nthrough more time for valuable work)\\n• More control and power for workers through \\ngreater access to information\\nMain pathways: All pathways, especially mind-\\ning the machine\\n• Reduced autonomy competencies (e.g., \\nthrough deskilling)\\n• Surveillance\\n• Manipulation & nudging (e.g., controlling \\nhuman behaviour)\\n• Employers exerting more power and control \\nover workers\\n• Worker vulnerability & dependence on AI\\n• Inauthentic behaviours (e.g., acting more self-\\nconsciously)\\n• Resistance to AI and greater deviance (e.g., \\n‘gaming’ the AI)\\nJustice\\nMain pathways: Replacing & amplifying\\n• More data-driven choices\\n• Less human bias in decision making\\n• Fairer decision making\\nMain pathways: All pathways\\n• Unfair distribution of benefits and burdens \\n(e.g., lower-skilled workers suffer more bur-\\ndens and receive less benefits)\\n• Undermining solidarity\\n• Implicating workers in injustices resulting \\nfrom AI bias\\nExplicability (explainability & accountability) Main pathways: Managing the machine & \\namplifying\\n• Informating (i.e., improved access to infor-\\nmation)\\n• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making',\n",
       "  '737\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\nare highly reliant upon it and are\\xa0accountable for it, can \\nconstrain skill use and feelings of competence. This could \\npotentially undermine the benefits AI may otherwise bring.',\n",
       "  'This suggests that training workers not only in what AI does \\nbut also how it does it, and making chains of accountability \\nclear, will be important for supporting experiences of mean-\\ningfulness at work. Practical Implications\\nOrganisational use of AI can reap many benefits through \\nimproved service range and quality, efficiency, and profit-\\nability. However, the ethical deployment of AI requires \\nweighing up its many costs and benefits.',\n",
       "  'We help articulate \\nsome of those costs and benefits for workers in terms of \\nAI’s impacts on meaningful work. Practically, this is impor-\\ntant because some authors suggest an emerging trend is for \\norganisations to use AI for full automation (Acemoglu & \\nRestrepo, 2020), without also considering opportunities to \\nuse it for enhancing human work, and then poorly preparing \\ntheir workforces for the changes that\\xa0AI use entails (Hal-\\nloran & Andrews, 2018). For organisations we highlight \\nthose pathways, such as ‘minding the machine’ work, that \\nare likely to significantly limit opportunities for meaningful \\nwork, which implies that other considerations such as effi-\\nciency benefits must strongly outweigh the harms to workers \\nthat AI used in this way can generate, in order to justify its \\nuse. We also highlight that, when considering meaningful-\\nness, it is insufficient to focus only on the AI itself, as the \\nimplications of its deployment are strongly driven by what \\nwork remains for humans, which is something that organi-\\nsations can directly influence and decide. Overall, we offer \\nguidance on how organisations can maintain or build oppor-\\ntunities for meaningful work when they implement AI and \\npoint leaders toward specific areas for intervention to sup-\\nport meaningful work experiences. For example, task sig-\\nnificance is critical for meaningful work (Grant, 2007), yet \\nthe ways AI can distance workers from beneficiaries threat-\\nens these experiences. However, there are ways in which \\norganisations can remedy this, such as by sharing end users’ \\npositive stories with workers (Grant, 2008). Future Research Directions\\nAlthough we did not frame explicit propositions from our \\nconceptual work, there are several relationships we sug-\\ngest warrant empirical examination. For example, assess-\\ning\\xa0whether AI performing simple tasks enhances task \\nintegrity, but AI performing complex tasks degrades this \\ndimension and perceptions of task significance, and\\xa0exam-\\nining whether the ‘managing the machine’ and amplifying \\npaths enhance task integrity and skill cultivation and use \\noverall, but ‘minding the machine’ work diminishes these \\naspects. We also suggest several contingencies will affect \\nthese relationships, such as what other or new work employ-\\nees do following AI implementation (task-related factors) \\nand how aspects of the technology, such as its explainabil-\\nity or potential for bias, shape workers’ experiences of it \\n(technology-related factors). While we adapted Langlois’ (2003) work to develop \\nour three pathways, these may manifest in different ways \\nand will likely overlap.',\n",
       "  'Future research could explore how \\neach path operates in workplaces, how they may differ from \\nour conceptualisation, and whether there are other path \\nconfigurations to AI deployment that our framework does \\nnot capture. There may also be nuances within pathways \\nthat warrant investigation.',\n",
       "  'For example, Jarrahi (2018, p. 3) suggests that advances in AI could create new forms of \\n“human–machine symbiosis” that result in “both parties \\n(becoming) smarter over time”.',\n",
       "  'This could generate new \\nforms of human skills, tasks, and perhaps whole jobs that \\nhave not yet been imagined, with implications for meaning-\\nful work. Another area for future work is examining how lead-\\ners construct and influence subjective perceptions of the \\nmeaningfulness of work, particularly through the values, \\nstrategies, and vision that underpin how they implement AI \\n(Pratt & Ashforth, 2003). For example, if an organisation is \\nfocused on full automation and replacing human workers, \\nit will likely deploy AI toward this end and degrade oppor-\\ntunities for meaningful work. But if leaders adopt multi-\\nstakeholder governance approaches that support ethical AI \\ndeployment (Wright & Schultz, 2018), such participatory \\npractices may enhance perceptions of meaningful work fol-\\nlowing AI deployment.',\n",
       "  'Finally, while we centred our analysis on the meaning-\\nful work implications of narrow AI, future work could uti-\\nlise conceptual tools such as thought experiments (Bankins \\n& Formosa, 2020) and work on posthumanism (Gladden, \\n2016) to prospectively analyse the impacts of potential \\nfuture forms and deployments of more advanced AI. For \\nexample, developments in virtual and augmented reality are \\ncreating movements toward a metaverse, or a persistent form \\nof virtual world that is accessible through various devices \\nand that people combine with their existence in the physi-\\ncal world (Ravenscraft, 2021). Such technologies have the \\npotential to transform the nature of social interactions and \\nthus impact the belongingness dimension of meaningful \\nwork. Likewise, advances in natural language processing \\nand speech interfaces could result in workers having multi-\\nple “digital assistants” (Zhou et\\xa0al., 2021, p. 258), which will',\n",
       "  '738\\n\\t\\nS. Bankins, P.',\n",
       "  'Formosa \\n1 3\\nimpact the nature of workers’ tasks and relationships and the \\nskills they will require in the future. Extending even further, artificial general intelligence \\n(AGI) would constitute “a new general-purpose technol-\\nogy” (Naudé & Dimitri, 2020) that has been predicted to \\npose existential threats such as eradicating large swathes of \\nhuman work (Bruun & Duka, 2018) and even risking human-\\nity’s annihilation (Torres, 2019). The reality of such tech-\\nnologies would inevitably lead to more extreme conclusions \\nfor the future of meaningful work than we have generated \\nhere through our focus on narrow AI, as they would likely \\nrender all but our replacing path largely obsolete. The pos-\\nsibilities of such technologies may therefore lead us back to \\nsubstantive discussions on the value of work generally, and \\nwhat forms of human work we believe must be preserved or \\nnewly created no matter what technologies are developed. This also raises questions of what broader social changes, \\nsuch as increased volunteering, provision of other forms of \\nmeaningful activity, or a Universal Basic Income (Hughes, \\n2014), will be required to cushion negative impacts should \\nAGI deployment ever become a reality. It also augurs the \\npotential for heavier regulation of the development and use \\nof AI (and potentially AGI) to maintain meaningful forms of \\nhuman employment, and to place limits on where, how, and \\nwhy AI is used. However, at this point the discussion relies \\non largely technical questions about whether AGI is indeed \\npossible (Boden, 2016).',\n",
       "  'In the meantime, the impacts of \\nnarrow AI on meaningful work are ones we need to address \\nhere and now. Conclusion\\nThis paper focused on a neglected aspect of the ethical \\nimplications of AI deployment, namely the impacts of AI \\non meaningful work. This is an important contribution as \\nthe ethical AI literature, while focused on the impacts of \\nunemployment resulting from AI, needs to also\\xa0attend\\xa0to \\nthe impacts of AI on meaningful work for the remaining \\nworkforce. Given the ethical importance of meaningful \\nwork and its considerable impacts on human wellbeing, \\nautonomy, and flourishing, this is a significant omission \\nthat we help to remedy. We have done so by examining the \\nimpacts of three paths of AI deployment (replacing tasks, \\n‘tending the machine’, and amplifying) across five dimen-\\nsions of meaningful work (task integrity, skill cultivation \\nand use, task significance, autonomy, and belongingness). Using this approach, we identify specific ways in which AI \\ncan both promote and diminish experiences of meaningful \\nwork across these dimensions and draw out the ethical impli-\\ncations of this by utilising five key ethical AI principles. Finally, we offer practical guidance for organisations by \\narticulating the ways that AI can be implemented to support \\nmeaningful work and suggest opportunities for future \\nresearch. Overall, we show that AI has the potential to make \\nwork more meaningful for some workers by undertaking less \\nmeaningful tasks for them and amplifying their capabilities, \\nbut that it can also make work less meaningful for others by \\ncreating new boring tasks, restricting worker autonomy, and \\nunfairly distributing the benefits of AI away from less-skilled \\nworkers. This suggests that AI’s future impacts on meaning-\\nful work will be both significant and mixed. Acknowledgements\\u2002 The authors would like to sincerely thank the \\nSpecial Issue Guest Editors, their Action Editor Associate Professor \\nLuke Fletcher, and the anonymous reviewers for their insightful and \\nconstructive feedback during the review process.',\n",
       "  'Funding\\u2002 Open Access funding enabled and organized by CAUL and \\nits Member Institutions.',\n",
       "  \"Declarations\\u2002\\nConflict of interest\\u2002 The authors have no conflicts of interest to declare \\nthat are relevant to the content of this article. Open Access\\u2002 This article is licensed under a Creative Commons Attri-\\nbution 4.0 International License, which permits use, sharing, adapta-\\ntion, distribution and reproduction in any medium or format, as long \\nas you give appropriate credit to the original author(s) and the source, \\nprovide a link to the Creative Commons licence, and indicate if changes \\nwere made. The images or other third party material in this article are \\nincluded in the article's Creative Commons licence, unless indicated \\notherwise in a credit line to the material.\",\n",
       "  \"If material is not included in \\nthe article's Creative Commons licence and your intended use is not \\npermitted by statutory regulation or exceeds the permitted use, you will \\nneed to obtain permission directly from the copyright holder. To view a \\ncopy of this licence, visit http://\\u200bcreat\\u200biveco\\u200bmmons.\\u200borg/\\u200blicen\\u200bses/\\u200bby/4.\\u200b0/. References\\nAbrams, J.\",\n",
       "  'J.',\n",
       "  '(2004). Pragmatism, artificial intelligence, and posthuman \\nbioethics: Shusterman, Rorty, Foucault. Human Studies, 27(3), \\n241–258.',\n",
       "  'Acemoglu, D., & Restrepo, P. (2020). The wrong kind of AI?',\n",
       "  'Artificial \\nintelligence and the future of labour demand. Cambridge Journal \\nof Regions, Economy and Society, 13, 25–35. Allan, B.',\n",
       "  'A., Batz-Barbarich, C., Sterling, H. M., & Tay, L. (2019). Outcomes of meaningful work: A meta-analysis. Journal of Man-\\nagement Studies, 56(3), 500–528. Asher-Schapiro, A.',\n",
       "  '(2021). Amazon AI van cameras spark surveil-\\nlance concerns. News.Trust.Org. https://\\u200bnews.\\u200btrust.\\u200borg/\\u200bitem/\\u200b\\n20210\\u200b20513\\u200b2207-\\u200bc0mz7/\\nBailey, C., Yeoman, R., Madden, A., Thompson, M., & Kerridge, G. (2019).',\n",
       "  'A review of the empirical literature on meaningful work: \\nProgress and research agenda. Human Resource Development \\nReview, 18(1), 83–113.',\n",
       "  'Bankins, S.',\n",
       "  '(2021). The ethical use of artificial intelligence in human \\nresource management: A decision-making framework. Ethics and \\nInformation Technology, 23, 841–854. Bankins, S., & Formosa, P.',\n",
       "  '(2020). When AI meets PC: Exploring \\nthe implications of workplace social robots and a human-robot',\n",
       "  '739\\nThe Ethical Implications of\\xa0Artificial Intelligence (AI) For\\xa0Meaningful Work\\ufeff\\t\\n1 3\\npsychological contract. European Journal of Work and Organi-\\nzational Psychology, 29(2), 215–229.',\n",
       "  'Bankins, S., & Formosa, P.',\n",
       "  '(2021). Ethical AI at work: The social \\ncontract for artificial intelligence and its implications for the work-\\nplace psychological contract. In: M. Coetzee & A. Deas (Eds.), \\nRedefining the Psychological Contract in the Digital Era: Issues \\nfor Research and Practice (pp. 55–72).',\n",
       "  'Springer: Switzerland.',\n",
       "  'Bankins, S., Formosa, P., Griep, Y., & Richards, D.',\n",
       "  \"(2022). AI decision \\nmaking with dignity? Contrasting workers' justice perceptions of \\nhuman and AI decision making in a human resource management \\ncontext. Information Systems Frontiers, 24(3), 857–875. Bekey, G.\",\n",
       "  'A. (2012). Current trends in robotics. In P. Lin, K. Abney, & \\nG. A. Bekey (Eds.), Robot ethics (pp. 17–34). MIT Press:\\xa0Cam-\\nbridge, Mass. Berk, R.',\n",
       "  'A. (2021). Artificial intelligence, predictive policing, and risk \\nassessment for law enforcement. Annual Review of Criminology, \\n4(1), 209–237.',\n",
       "  'Boden, M.',\n",
       "  'A. (2016). AI. Oxford University Press: UK. Bourmault, N., & Anteby, M.',\n",
       "  '(2020). Unpacking the managerial blues: \\nHow expectations formed in the past carry into new jobs. Organi-\\nzation Science, 31(6), 1452–1474.',\n",
       "  'Bowie, N.',\n",
       "  'E. (1998). A Kantian theory of meaningful work. Journal \\nof Business Ethics, 17, 1083–1092. Bruun, E., & Duka, A.',\n",
       "  '(2018).',\n",
       "  'Artificial intelligence, jobs and the \\nfuture of work. Basic Income Studies, 13(2), 1–15.',\n",
       "  'Camus, A.',\n",
       "  '(1955). The myth of Sisyphus and other essays. Hamish \\nHamilton. Carton, A.',\n",
       "  'M.',\n",
       "  '(2018). I’m not mopping the floors, I’m putting a man \\non the moon: How NASA leaders enhanced the meaningfulness \\nof work by changing the meaning of work. Administrative Science \\nQuarterly, 63(2), 323–369.',\n",
       "  'Cheney, G., Zorn Jr, T.',\n",
       "  ...],\n",
       " 'uris': None,\n",
       " 'included': ['embeddings', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': None}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_vector.get(include=['embeddings','documents'])\n",
    "\n",
    "# semantic_vector.get(include=['embeddings','documents','metadatas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "378e0476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['8f40dfe8-404d-4a9b-943a-7cbd63a656f0'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['Published online: 07 Feb 2025. Submit your article to this journal \\nArticle views: 34975\\nView related articles \\nView Crossmark data\\nCiting articles: 52 View citing articles \\nFull Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=uaai20'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development',\n",
       "   'creationdate': '2025-02-07T21:22:17+05:30',\n",
       "   'author': '',\n",
       "   'creationDate': \"D:20250207212217+05'30'\",\n",
       "   'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)',\n",
       "   'total_pages': 42,\n",
       "   'creator': 'PTC Arbortext Publishing Engine',\n",
       "   'page': 0,\n",
       "   'moddate': '2025-02-07T21:22:17+05:30',\n",
       "   'format': 'PDF 1.5',\n",
       "   'subject': '',\n",
       "   'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf',\n",
       "   'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf',\n",
       "   'trapped': '',\n",
       "   'modDate': \"D:20250207212217+05'30'\",\n",
       "   'keywords': ''}]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = semantic_vector._client.get_collection(\"semantic_chunks_sample\")\n",
    "doc_data = collection.get(ids=['8f40dfe8-404d-4a9b-943a-7cbd63a656f0'])\n",
    "\n",
    "doc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0926379",
   "metadata": {},
   "source": [
    "RETERVIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10c898",
   "metadata": {},
   "source": [
    "SO basically we are having 4 Retervials \n",
    "\n",
    "1. Similarity Retervier\n",
    "2. Maximum Marginal Retervier\n",
    "3. Multi Query Retervier\n",
    "4. Context Compressed Retriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94163ed0",
   "metadata": {},
   "source": [
    "SO WE WILL BE USING RECURSIVE VECTORS BY KEEPING THESE ON 4 RETERVIALS SO WE GET WHICH ONE GET VERY GOOD RETERVIED WE TAKE THAT RETERVIER AND VECTOR STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505b6cf",
   "metadata": {},
   "source": [
    "1. Similarity Retervier on Recursive Vector and Taking Top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "feaf56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'What are the main ethical concerns in AI deployment'\n",
    "similarity_retervier_recursive = recursive_vector.as_retriever(search_type='similarity',search_kwargs={'k':5})\n",
    "\n",
    "similarity_retervial_docs = similarity_retervier_recursive.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53166297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'author': '', 'creator': '', 'creationDate': \"D:20230711214741+05'30'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'format': 'PDF 1.4', 'total_pages': 21, 'modDate': \"D:20230720162718-04'00'\", 'title': 'An Overview of Artificial Intelligence Ethics', 'page': 1, 'trapped': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationdate': '2023-07-11T21:47:41+05:30', 'moddate': '2023-07-20T16:27:18-04:00', 'keywords': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503'}, page_content='have begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.\\nAfter clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI. We'),\n",
       " Document(metadata={'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'page': 11, 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'author': 'Sarah Bankins', 'moddate': '2023-07-03T17:42:49+05:30', 'total_pages': 16, 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'modDate': \"D:20230703174249+05'30'\", 'creationdate': '2023-02-24T14:25:09+05:30', 'creationDate': \"D:20230224142509+05'30'\", 'format': 'PDF 1.4', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'trapped': '', 'creator': 'Springer'}, page_content='• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making'),\n",
       " Document(metadata={'title': 'An Overview of Artificial Intelligence Ethics', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'author': '', 'page': 6, 'keywords': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'creator': '', 'trapped': '', 'moddate': '2023-07-20T16:27:18-04:00', 'creationdate': '2023-07-11T21:47:41+05:30', 'format': 'PDF 1.4', 'modDate': \"D:20230720162718-04'00'\", 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\"}, page_content='receive adequate compensation. Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68]. This is largely related to people’s fear'),\n",
       " Document(metadata={'creationDate': \"D:20230711214741+05'30'\", 'author': '', 'format': 'PDF 1.4', 'creationdate': '2023-07-11T21:47:41+05:30', 'modDate': \"D:20230720162718-04'00'\", 'title': 'An Overview of Artificial Intelligence Ethics', 'total_pages': 21, 'trapped': '', 'moddate': '2023-07-20T16:27:18-04:00', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creator': '', 'page': 11, 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf'}, page_content='In contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be\\nforbidden by principles that such people could not “reasonably\\nreject” [96].\\nConsequentialist ethics: Consequentialist ethics, as its name\\nsuggests, emphasizes the utilitarian outcomes of actions [97].'),\n",
       " Document(metadata={'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'moddate': '2023-07-03T17:42:49+05:30', 'creationDate': \"D:20230224142509+05'30'\", 'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'format': 'PDF 1.4', 'modDate': \"D:20230703174249+05'30'\", 'trapped': '', 'creationdate': '2023-02-24T14:25:09+05:30', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'creator': 'Springer', 'author': 'Sarah Bankins', 'total_pages': 16, 'page': 10, 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work'}, page_content='also depend on whether workers have input into how AI \\nis deployed in their organisations. A particular risk to \\nautonomy is the use of AI to surveil and monitor, which can \\nundermine authenticity and encourage workers to align their \\nbehaviours with the AI’s implicit expectations or seek ways \\nto subvert or avoid its control.')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_retervial_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "afcd4f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result 1:\n",
      "have begun to discuss and seek possible frameworks, guidelines,\n",
      "and principles for solving AI ethics issues. These guidelines and\n",
      "principles provide valuable directions for practicing ethical AI.\n",
      "After clarifying the existing ethical issues and guidelines, we\n",
      "review the approaches to solving the ethical issues in AI. We\n",
      "\n",
      " Result 2:\n",
      "• Greater transparency in decision making\n",
      "• Upskilling in understanding AI\n",
      "Main pathways: Replacing, amplifying, & \n",
      "managing the machine\n",
      "• Feelings of incompetence due to poor AI \n",
      "explainability\n",
      "• Unclear chains of accountability when AI is \n",
      "involved in decision making\n",
      "\n",
      " Result 3:\n",
      "receive adequate compensation. Thus, accountability is crucial\n",
      "to ensure the trust of AI.\n",
      "Control: Another issue that affects the public trust in AI is the\n",
      "controllability of AI [68]. This is largely related to people’s fear\n",
      "\n",
      " Result 4:\n",
      "In contractualist deontological theories, morally wrong acts are\n",
      "those acts that would be forbidden by principles that people in a\n",
      "suitably described social contract would accept, or that would be\n",
      "forbidden by principles that such people could not “reasonably\n",
      "reject” [96].\n",
      "Consequentialist ethics: Consequentialist ethics, as its name\n",
      "suggests, emphasizes the utilitarian outcomes of actions [97].\n",
      "\n",
      " Result 5:\n",
      "also depend on whether workers have input into how AI \n",
      "is deployed in their organisations. A particular risk to \n",
      "autonomy is the use of AI to surveil and monitor, which can \n",
      "undermine authenticity and encourage workers to align their \n",
      "behaviours with the AI’s implicit expectations or seek ways \n",
      "to subvert or avoid its control.\n"
     ]
    }
   ],
   "source": [
    "similarity_retervial_docs_page_content = []\n",
    "for i, doc in enumerate(similarity_retervial_docs, 1):\n",
    "    print(f\"\\n Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    similarity_retervial_docs_page_content.append(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "83617f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.\\nAfter clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI. We',\n",
       " '• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making',\n",
       " 'receive adequate compensation. Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68]. This is largely related to people’s fear',\n",
       " 'In contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be\\nforbidden by principles that such people could not “reasonably\\nreject” [96].\\nConsequentialist ethics: Consequentialist ethics, as its name\\nsuggests, emphasizes the utilitarian outcomes of actions [97].',\n",
       " 'also depend on whether workers have input into how AI \\nis deployed in their organisations. A particular risk to \\nautonomy is the use of AI to surveil and monitor, which can \\nundermine authenticity and encourage workers to align their \\nbehaviours with the AI’s implicit expectations or seek ways \\nto subvert or avoid its control.']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_retervial_docs_page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9d60e",
   "metadata": {},
   "source": [
    "2. Maximum Marginal Retervier on Recursive Vector and Taking Top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc320800",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'What are the main ethical concerns in AI deployment'\n",
    "mmr_retervier_recursive = recursive_vector.as_retriever(search_type='mmr',search_kwargs={'k':5})\n",
    "\n",
    "mmr_retervial_docs = mmr_retervier_recursive.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0bc8580f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'creationDate': \"D:20230711214741+05'30'\", 'keywords': '', 'page': 1, 'title': 'An Overview of Artificial Intelligence Ethics', 'total_pages': 21, 'format': 'PDF 1.4', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'creator': '', 'author': '', 'moddate': '2023-07-20T16:27:18-04:00', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'modDate': \"D:20230720162718-04'00'\", 'creationdate': '2023-07-11T21:47:41+05:30', 'trapped': ''}, page_content='have begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.\\nAfter clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI. We'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'trapped': '', 'moddate': '2023-07-03T17:42:49+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'file_path': 'Books\\\\Ethical Paper.pdf', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'format': 'PDF 1.4', 'page': 11, 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'creationDate': \"D:20230224142509+05'30'\", 'total_pages': 16, 'modDate': \"D:20230703174249+05'30'\", 'creationdate': '2023-02-24T14:25:09+05:30', 'creator': 'Springer'}, page_content='• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making'),\n",
       " Document(metadata={'total_pages': 21, 'trapped': '', 'author': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'format': 'PDF 1.4', 'title': 'An Overview of Artificial Intelligence Ethics', 'moddate': '2023-07-20T16:27:18-04:00', 'creator': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'keywords': '', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'page': 6, 'creationDate': \"D:20230711214741+05'30'\", 'modDate': \"D:20230720162718-04'00'\"}, page_content='receive adequate compensation. Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68]. This is largely related to people’s fear'),\n",
       " Document(metadata={'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'trapped': '', 'author': '', 'creationDate': \"D:20230711214741+05'30'\", 'title': 'An Overview of Artificial Intelligence Ethics', 'format': 'PDF 1.4', 'modDate': \"D:20230720162718-04'00'\", 'page': 11, 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'moddate': '2023-07-20T16:27:18-04:00', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'total_pages': 21}, page_content='In contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be\\nforbidden by principles that such people could not “reasonably\\nreject” [96].\\nConsequentialist ethics: Consequentialist ethics, as its name\\nsuggests, emphasizes the utilitarian outcomes of actions [97].'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'modDate': \"D:20230720162718-04'00'\", 'keywords': '', 'author': '', 'trapped': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'title': 'An Overview of Artificial Intelligence Ethics', 'creator': '', 'creationDate': \"D:20230711214741+05'30'\", 'creationdate': '2023-07-11T21:47:41+05:30', 'moddate': '2023-07-20T16:27:18-04:00', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'format': 'PDF 1.4', 'page': 4, 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503'}, page_content='decision risk or dilemma of AI. For instance, autonomous ve-\\nhicles should reduce trafﬁc accidents, but sometimes they have\\nto choose between two evils, such as crushing pedestrians or\\nsacriﬁcing themselves and passengers to save pedestrians [43].')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_retervial_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "23ac4d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result 1:\n",
      "have begun to discuss and seek possible frameworks, guidelines,\n",
      "and principles for solving AI ethics issues. These guidelines and\n",
      "principles provide valuable directions for practicing ethical AI.\n",
      "After clarifying the existing ethical issues and guidelines, we\n",
      "review the approaches to solving the ethical issues in AI. We\n",
      "\n",
      " Result 2:\n",
      "• Greater transparency in decision making\n",
      "• Upskilling in understanding AI\n",
      "Main pathways: Replacing, amplifying, & \n",
      "managing the machine\n",
      "• Feelings of incompetence due to poor AI \n",
      "explainability\n",
      "• Unclear chains of accountability when AI is \n",
      "involved in decision making\n",
      "\n",
      " Result 3:\n",
      "receive adequate compensation. Thus, accountability is crucial\n",
      "to ensure the trust of AI.\n",
      "Control: Another issue that affects the public trust in AI is the\n",
      "controllability of AI [68]. This is largely related to people’s fear\n",
      "\n",
      " Result 4:\n",
      "In contractualist deontological theories, morally wrong acts are\n",
      "those acts that would be forbidden by principles that people in a\n",
      "suitably described social contract would accept, or that would be\n",
      "forbidden by principles that such people could not “reasonably\n",
      "reject” [96].\n",
      "Consequentialist ethics: Consequentialist ethics, as its name\n",
      "suggests, emphasizes the utilitarian outcomes of actions [97].\n",
      "\n",
      " Result 5:\n",
      "decision risk or dilemma of AI. For instance, autonomous ve-\n",
      "hicles should reduce trafﬁc accidents, but sometimes they have\n",
      "to choose between two evils, such as crushing pedestrians or\n",
      "sacriﬁcing themselves and passengers to save pedestrians [43].\n"
     ]
    }
   ],
   "source": [
    "mmr_retervial_docs_page_content =[]\n",
    "\n",
    "for i, doc in enumerate(mmr_retervial_docs, 1):\n",
    "    print(f\"\\n Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    mmr_retervial_docs_page_content.append(doc.page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e8d284ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.\\nAfter clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI. We',\n",
       " '• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making',\n",
       " 'receive adequate compensation. Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68]. This is largely related to people’s fear',\n",
       " 'In contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be\\nforbidden by principles that such people could not “reasonably\\nreject” [96].\\nConsequentialist ethics: Consequentialist ethics, as its name\\nsuggests, emphasizes the utilitarian outcomes of actions [97].',\n",
       " 'decision risk or dilemma of AI. For instance, autonomous ve-\\nhicles should reduce trafﬁc accidents, but sometimes they have\\nto choose between two evils, such as crushing pedestrians or\\nsacriﬁcing themselves and passengers to save pedestrians [43].']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_retervial_docs_page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97b029",
   "metadata": {},
   "source": [
    "3. Multi Query Retervier on Recursive Vector and Taking Top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d9fbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'What are the main ethical concerns in AI deployment'\n",
    "\n",
    "multiQuery_retervier_recursive = MultiQueryRetriever.from_llm(\n",
    "    retriever=recursive_vector.as_retriever(search_kwargs ={'k':5},search_type='mmr'),\n",
    "    llm=model\n",
    ")\n",
    "\n",
    "# So basically here it uses LLM to produce Multiple Query so it will Produce Multiple Query and also from that it uses similarity or mmr so basically i have taken mmr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiQuery_retervial_docs = multiQuery_retervier_recursive.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e857720e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'creationDate': \"D:20230224142509+05'30'\", 'creator': 'Springer', 'author': 'Sarah Bankins', 'file_path': 'Books\\\\Ethical Paper.pdf', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'moddate': '2023-07-03T17:42:49+05:30', 'trapped': '', 'page': 11, 'creationdate': '2023-02-24T14:25:09+05:30', 'modDate': \"D:20230703174249+05'30'\", 'format': 'PDF 1.4', 'source': 'Books\\\\Ethical Paper.pdf', 'total_pages': 16}, page_content='• Greater transparency in decision making\\n• Upskilling in understanding AI\\nMain pathways: Replacing, amplifying, & \\nmanaging the machine\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making'),\n",
       " Document(metadata={'creationDate': \"D:20230711214741+05'30'\", 'total_pages': 21, 'page': 1, 'format': 'PDF 1.4', 'moddate': '2023-07-20T16:27:18-04:00', 'modDate': \"D:20230720162718-04'00'\", 'trapped': '', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'author': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'title': 'An Overview of Artificial Intelligence Ethics', 'keywords': '', 'creator': ''}, page_content='have begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.\\nAfter clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI. We'),\n",
       " Document(metadata={'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'moddate': '2023-07-20T16:27:18-04:00', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creator': '', 'keywords': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'title': 'An Overview of Artificial Intelligence Ethics', 'modDate': \"D:20230720162718-04'00'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'page': 4, 'format': 'PDF 1.4', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'total_pages': 21, 'trapped': '', 'author': '', 'creationDate': \"D:20230711214741+05'30'\"}, page_content='decision risk or dilemma of AI. For instance, autonomous ve-\\nhicles should reduce trafﬁc accidents, but sometimes they have\\nto choose between two evils, such as crushing pedestrians or\\nsacriﬁcing themselves and passengers to save pedestrians [43].'),\n",
       " Document(metadata={'author': '', 'moddate': '2025-02-07T21:22:17+05:30', 'format': 'PDF 1.5', 'page': 25, 'creationdate': '2025-02-07T21:22:17+05:30', 'total_pages': 42, 'modDate': \"D:20250207212217+05'30'\", 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'creator': 'PTC Arbortext Publishing Engine', 'creationDate': \"D:20250207212217+05'30'\", 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'subject': '', 'trapped': '', 'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'keywords': ''}, page_content='research, scored lower.\\nNational Security\\nNational security considerations, particularly regarding the development of \\nautonomous weapons systems (AWS) and AI-enhanced cybersecurity, are \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-25'),\n",
       " Document(metadata={'modDate': \"D:20230720162718-04'00'\", 'moddate': '2023-07-20T16:27:18-04:00', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'format': 'PDF 1.4', 'total_pages': 21, 'keywords': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'page': 11, 'creator': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'creationdate': '2023-07-11T21:47:41+05:30', 'trapped': '', 'author': ''}, page_content='In contractualist deontological theories, morally wrong acts are\\nthose acts that would be forbidden by principles that people in a\\nsuitably described social contract would accept, or that would be\\nforbidden by principles that such people could not “reasonably\\nreject” [96].\\nConsequentialist ethics: Consequentialist ethics, as its name\\nsuggests, emphasizes the utilitarian outcomes of actions [97].'),\n",
       " Document(metadata={'file_path': 'Books\\\\Ethical Paper.pdf', 'source': 'Books\\\\Ethical Paper.pdf', 'trapped': '', 'format': 'PDF 1.4', 'page': 10, 'total_pages': 16, 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'modDate': \"D:20230703174249+05'30'\", 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'author': 'Sarah Bankins', 'creationDate': \"D:20230224142509+05'30'\", 'moddate': '2023-07-03T17:42:49+05:30', 'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creationdate': '2023-02-24T14:25:09+05:30', 'creator': 'Springer'}, page_content='also depend on whether workers have input into how AI \\nis deployed in their organisations. A particular risk to \\nautonomy is the use of AI to surveil and monitor, which can \\nundermine authenticity and encourage workers to align their \\nbehaviours with the AI’s implicit expectations or seek ways \\nto subvert or avoid its control.'),\n",
       " Document(metadata={'total_pages': 21, 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'keywords': '', 'format': 'PDF 1.4', 'moddate': '2023-07-20T16:27:18-04:00', 'modDate': \"D:20230720162718-04'00'\", 'author': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'title': 'An Overview of Artificial Intelligence Ethics', 'creationDate': \"D:20230711214741+05'30'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'page': 6, 'trapped': '', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30'}, page_content='receive adequate compensation. Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68]. This is largely related to people’s fear'),\n",
       " Document(metadata={'modDate': \"D:20250207212217+05'30'\", 'keywords': '', 'total_pages': 42, 'page': 27, 'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'creationdate': '2025-02-07T21:22:17+05:30', 'moddate': '2025-02-07T21:22:17+05:30', 'trapped': '', 'subject': '', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'creator': 'PTC Arbortext Publishing Engine', 'author': '', 'creationDate': \"D:20250207212217+05'30'\", 'format': 'PDF 1.5', 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development'}, page_content='concerns of stakeholders and help establish ethical guidelines for AI develop\\xad\\nment and use.\\nExtended Analysis and Global Implications\\nIn this section, we expand into an evaluation matrix that incorporates addi\\xad\\ntional dimensions that are increasingly relevant to AI ethics. These dimensions \\nAPPLIED ARTIFICIAL INTELLIGENCE\\ne2463722-27'),\n",
       " Document(metadata={'creationdate': '2023-07-11T21:47:41+05:30', 'modDate': \"D:20230720162718-04'00'\", 'format': 'PDF 1.4', 'author': '', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'creationDate': \"D:20230711214741+05'30'\", 'total_pages': 21, 'page': 5, 'trapped': '', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creator': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf'}, page_content='about by the public.\\nDemocracy. The implementation and adoption of AI can\\nthreaten democracy in several ways. First, the concentration\\nof technological, economic, and political power related to AI\\namong a few mega corporations could allow them to pose undue\\ninﬂuence over the government. Second, AI may damage democ-\\nracy by affecting political elections [55]. With the aid of AI and')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiQuery_retervial_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "07455152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result 1:\n",
      "Testing or evaluating whether an AI system\n",
      "meets the ethical requirements or not is an essential part\n",
      "of AI ethics.\n",
      "\n",
      " Result 2:\n",
      "Regardless of the way AI conducts moral\n",
      "reasoning, it is most critical that its moral activities conform to\n",
      "the goals of ethical design.\n",
      "\n",
      " Result 3:\n",
      "There are many other examples concerned with the\n",
      "failure, fairness, bias, privacy, and other ethical issues of AI\n",
      "systems [7].\n",
      "\n",
      " Result 4:\n",
      "It involves the ethical or moral values and\n",
      "principles that determine what is morally right and wrong.\n",
      "\n",
      " Result 5:\n",
      "Thus, accountability is crucial\n",
      "to ensure the trust of AI.\n",
      "\n",
      " Result 6:\n",
      "AI\n",
      "is increasingly taking over human tasks and replacing human\n",
      "decision-making.\n",
      "\n",
      " Result 7:\n",
      "Continuously mon­\n",
      "itoring and maintaining the AI system post-deployment is essential.\n",
      "\n",
      " Result 8:\n",
      "As AI is widely used in our lives, responsible AI is becoming\n",
      "critical.\n"
     ]
    }
   ],
   "source": [
    "multiQuery_retervial_docs_page_content = []\n",
    "for i, doc in enumerate(multiQuery_retervial_docs, 1):\n",
    "    print(f\"\\n Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    multiQuery_retervial_docs_page_content.append(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a2310a",
   "metadata": {},
   "source": [
    "4. Contextual Compressor Retervier on Recursive Vector and Taking Top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bc7d2072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics.',\n",
       " 'Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.',\n",
       " 'There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].',\n",
       " 'It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.',\n",
       " 'Thus, accountability is crucial\\nto ensure the trust of AI.',\n",
       " 'AI\\nis increasingly taking over human tasks and replacing human\\ndecision-making.',\n",
       " 'Continuously mon\\xad\\nitoring and maintaining the AI system post-deployment is essential.',\n",
       " 'As AI is widely used in our lives, responsible AI is becoming\\ncritical.']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiQuery_retervial_docs_page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5e72eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.document_compressors.chain_extract import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "650114b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'What are the main ethical concerns in AI deployment'\n",
    "compressor = LLMChainExtractor.from_llm(model)\n",
    "\n",
    "contextual_retervier_recursive = ContextualCompressionRetriever(\n",
    "    base_compressor= compressor,\n",
    "    base_retriever=recursive_vector.as_retriever(search_kwargs ={'k':5},search_type='mmr')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c964b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_retervial_docs =contextual_retervier_recursive.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "98eb4bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'moddate': '2023-07-20T16:27:18-04:00', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'trapped': '', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creationDate': \"D:20230711214741+05'30'\", 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creator': '', 'page': 1, 'modDate': \"D:20230720162718-04'00'\", 'creationdate': '2023-07-11T21:47:41+05:30', 'format': 'PDF 1.4', 'total_pages': 21}, page_content='have begun to discuss and seek possible frameworks, guidelines,\\nand principles for solving AI ethics issues. These guidelines and\\nprinciples provide valuable directions for practicing ethical AI.\\nAfter clarifying the existing ethical issues and guidelines, we\\nreview the approaches to solving the ethical issues in AI.'),\n",
       " Document(metadata={'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work', 'creationDate': \"D:20230224142509+05'30'\", 'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'creationdate': '2023-02-24T14:25:09+05:30', 'source': 'Books\\\\Ethical Paper.pdf', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'moddate': '2023-07-03T17:42:49+05:30', 'author': 'Sarah Bankins', 'file_path': 'Books\\\\Ethical Paper.pdf', 'format': 'PDF 1.4', 'trapped': '', 'creator': 'Springer', 'total_pages': 16, 'modDate': \"D:20230703174249+05'30'\", 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'page': 11}, page_content='• Greater transparency in decision making\\n• Feelings of incompetence due to poor AI \\nexplainability\\n• Unclear chains of accountability when AI is \\ninvolved in decision making'),\n",
       " Document(metadata={'format': 'PDF 1.4', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'creationdate': '2023-07-11T21:47:41+05:30', 'total_pages': 21, 'page': 6, 'modDate': \"D:20230720162718-04'00'\", 'trapped': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'creator': '', 'author': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'moddate': '2023-07-20T16:27:18-04:00', 'keywords': ''}, page_content='Thus, accountability is crucial\\nto ensure the trust of AI.\\nControl: Another issue that affects the public trust in AI is the\\ncontrollability of AI [68].'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'total_pages': 21, 'moddate': '2023-07-20T16:27:18-04:00', 'modDate': \"D:20230720162718-04'00'\", 'author': '', 'creator': '', 'format': 'PDF 1.4', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'title': 'An Overview of Artificial Intelligence Ethics', 'page': 4, 'keywords': '', 'trapped': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\"}, page_content='decision risk or dilemma of AI. For instance, autonomous ve-\\nhicles should reduce trafﬁc accidents, but sometimes they have\\nto choose between two evils, such as crushing pedestrians or\\nsacriﬁcing themselves and passengers to save pedestrians [43].')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_retervial_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a09bb9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result 1:\n",
      "It involves the ethical or moral values and\n",
      "principles that determine what is morally right and wrong.\n",
      "\n",
      " Result 2:\n",
      "Thus, accountability is crucial\n",
      "to ensure the trust of AI.\n",
      "\n",
      " Result 3:\n",
      "There are many other examples concerned with the\n",
      "failure, fairness, bias, privacy, and other ethical issues of AI\n",
      "systems [7].\n"
     ]
    }
   ],
   "source": [
    "contextual_retervial_docs_page_content = []\n",
    "for i, doc in enumerate(contextual_retervial_docs, 1):\n",
    "    print(f\"\\n Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    contextual_retervial_docs_page_content.append(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "19c667d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.',\n",
       " 'Thus, accountability is crucial\\nto ensure the trust of AI.',\n",
       " 'There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_retervial_docs_page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f6a942a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can see here we got only 4 results the ambigous data got deleted by contextual compressor now let's try with semantic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c23834",
   "metadata": {},
   "source": [
    "Semantic Vector Store with \n",
    "1. Similarity \n",
    "2. MMR\n",
    "3. Multi Query\n",
    "4. Contextual Compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44321c9",
   "metadata": {},
   "source": [
    "Similarity Retervial with Semantic Vector to Reterview the top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c9782b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'What are the main ethical concerns in AI deployment'\n",
    "similarity_retervier_semantic = semantic_vector.as_retriever(search_type='similarity',search_kwargs={'k':5})\n",
    "\n",
    "similarity_retervial_semantic_docs = similarity_retervier_semantic.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "05b6d294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'format': 'PDF 1.4', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'modDate': \"D:20230720162718-04'00'\", 'title': 'An Overview of Artificial Intelligence Ethics', 'creationDate': \"D:20230711214741+05'30'\", 'total_pages': 21, 'creator': '', 'author': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'page': 15, 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'trapped': ''}, page_content='Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.'),\n",
       " Document(metadata={'trapped': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'total_pages': 21, 'page': 1, 'creationDate': \"D:20230711214741+05'30'\", 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'title': 'An Overview of Artificial Intelligence Ethics', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'author': '', 'modDate': \"D:20230720162718-04'00'\", 'format': 'PDF 1.4', 'creator': '', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00'}, page_content='Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics.'),\n",
       " Document(metadata={'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'page': 1, 'moddate': '2023-07-20T16:27:18-04:00', 'creator': '', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'title': 'An Overview of Artificial Intelligence Ethics', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'creationdate': '2023-07-11T21:47:41+05:30', 'author': '', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'total_pages': 21, 'format': 'PDF 1.4', 'keywords': ''}, page_content='It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.'),\n",
       " Document(metadata={'modDate': \"D:20230720162718-04'00'\", 'total_pages': 21, 'author': '', 'keywords': '', 'page': 6, 'creationDate': \"D:20230711214741+05'30'\", 'format': 'PDF 1.4', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'creator': '', 'trapped': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'title': 'An Overview of Artificial Intelligence Ethics', 'moddate': '2023-07-20T16:27:18-04:00'}, page_content='Thus, accountability is crucial\\nto ensure the trust of AI.'),\n",
       " Document(metadata={'creationDate': \"D:20230711214741+05'30'\", 'modDate': \"D:20230720162718-04'00'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'moddate': '2023-07-20T16:27:18-04:00', 'page': 0, 'total_pages': 21, 'author': '', 'creator': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'creationdate': '2023-07-11T21:47:41+05:30', 'keywords': '', 'format': 'PDF 1.4', 'trapped': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf'}, page_content='There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_retervial_semantic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "757a1899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result 1:\n",
      "Regardless of the way AI conducts moral\n",
      "reasoning, it is most critical that its moral activities conform to\n",
      "the goals of ethical design.\n",
      "\n",
      " Result 2:\n",
      "Testing or evaluating whether an AI system\n",
      "meets the ethical requirements or not is an essential part\n",
      "of AI ethics.\n",
      "\n",
      " Result 3:\n",
      "It involves the ethical or moral values and\n",
      "principles that determine what is morally right and wrong.\n",
      "\n",
      " Result 4:\n",
      "Thus, accountability is crucial\n",
      "to ensure the trust of AI.\n",
      "\n",
      " Result 5:\n",
      "There are many other examples concerned with the\n",
      "failure, fairness, bias, privacy, and other ethical issues of AI\n",
      "systems [7].\n"
     ]
    }
   ],
   "source": [
    "similarity_retervial_semantic_docs_page_content = []\n",
    "for i, doc in enumerate(similarity_retervial_semantic_docs, 1):\n",
    "    print(f\"\\n Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    similarity_retervial_semantic_docs_page_content.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9b873c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.',\n",
       " 'Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics.',\n",
       " 'It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.',\n",
       " 'Thus, accountability is crucial\\nto ensure the trust of AI.',\n",
       " 'There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_retervial_semantic_docs_page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538c1c0",
   "metadata": {},
   "source": [
    "MMR Retervial with Semantic Vector to Reterview the top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7ec0ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'What are the main ethical concerns in AI deployment'\n",
    "mmr_retervier_semantic = semantic_vector.as_retriever(search_type='mmr',search_kwargs={'k':5})\n",
    "\n",
    "mmr_retervial_semantic_docs = mmr_retervier_semantic.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b93a57c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'format': 'PDF 1.4', 'keywords': '', 'author': '', 'creationDate': \"D:20230711214741+05'30'\", 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'moddate': '2023-07-20T16:27:18-04:00', 'title': 'An Overview of Artificial Intelligence Ethics', 'trapped': '', 'page': 15, 'total_pages': 21, 'modDate': \"D:20230720162718-04'00'\"}, page_content='Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.'),\n",
       " Document(metadata={'modDate': \"D:20230720162718-04'00'\", 'creator': '', 'trapped': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'title': 'An Overview of Artificial Intelligence Ethics', 'total_pages': 21, 'format': 'PDF 1.4', 'page': 6, 'creationDate': \"D:20230711214741+05'30'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'author': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'creationdate': '2023-07-11T21:47:41+05:30', 'keywords': '', 'moddate': '2023-07-20T16:27:18-04:00'}, page_content='Thus, accountability is crucial\\nto ensure the trust of AI.'),\n",
       " Document(metadata={'title': 'An Overview of Artificial Intelligence Ethics', 'trapped': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'author': '', 'moddate': '2023-07-20T16:27:18-04:00', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'keywords': '', 'modDate': \"D:20230720162718-04'00'\", 'total_pages': 21, 'format': 'PDF 1.4', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'page': 0, 'creator': ''}, page_content='There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].'),\n",
       " Document(metadata={'total_pages': 16, 'source': 'Books\\\\Ethical Paper.pdf', 'creationdate': '2023-02-24T14:25:09+05:30', 'creator': 'Springer', 'file_path': 'Books\\\\Ethical Paper.pdf', 'creationDate': \"D:20230224142509+05'30'\", 'format': 'PDF 1.4', 'modDate': \"D:20230703174249+05'30'\", 'author': 'Sarah Bankins', 'trapped': '', 'subject': 'Journal of Business Ethics, https://doi.org/10.1007/s10551-023-05339-7', 'title': 'The Ethical Implications of Artificial Intelligence (AI) For Meaningful Work', 'producer': 'Acrobat Distiller 10.1.8 (Windows)', 'page': 3, 'moddate': '2023-07-03T17:42:49+05:30', 'keywords': 'Meaningful work; Artificial intelligence (AI); Ethical AI; Future of work; Technology and work'}, page_content='The foundational principles'),\n",
       " Document(metadata={'author': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'format': 'PDF 1.4', 'page': 19, 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'moddate': '2023-07-20T16:27:18-04:00', 'keywords': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'creationdate': '2023-07-11T21:47:41+05:30', 'creator': '', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503'}, page_content='Available: https://iep.utm.edu/ethics/#SH2c\\n[92] R.')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_retervial_semantic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "14b4a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result 1:\n",
      "Regardless of the way AI conducts moral\n",
      "reasoning, it is most critical that its moral activities conform to\n",
      "the goals of ethical design.\n",
      "\n",
      " Result 2:\n",
      "Thus, accountability is crucial\n",
      "to ensure the trust of AI.\n",
      "\n",
      " Result 3:\n",
      "There are many other examples concerned with the\n",
      "failure, fairness, bias, privacy, and other ethical issues of AI\n",
      "systems [7].\n",
      "\n",
      " Result 4:\n",
      "The foundational principles\n",
      "\n",
      " Result 5:\n",
      "Available: https://iep.utm.edu/ethics/#SH2c\n",
      "[92] R.\n"
     ]
    }
   ],
   "source": [
    "mmr_retervial_semantic_docs_page_content = []\n",
    "for i, doc in enumerate(mmr_retervial_semantic_docs, 1):\n",
    "    print(f\"\\n Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    mmr_retervial_semantic_docs_page_content.append(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "36655557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.',\n",
       " 'Thus, accountability is crucial\\nto ensure the trust of AI.',\n",
       " 'There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].',\n",
       " 'The foundational principles',\n",
       " 'Available: https://iep.utm.edu/ethics/#SH2c\\n[92] R.']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_retervial_semantic_docs_page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4627db5",
   "metadata": {},
   "source": [
    "Multi Query Retervial with Semantic Vector to Reterview the top 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "71df6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'What are the main ethical concerns in AI deployment'\n",
    "\n",
    "multiQuery_retervier_semantic = MultiQueryRetriever.from_llm(\n",
    "    retriever=semantic_vector.as_retriever(search_kwargs ={'k':5},search_type='similarity'),\n",
    "    llm=model\n",
    ")\n",
    "\n",
    "# So basically here it uses LLM to produce Multiple Query so it will Produce Multiple Query and also from that it uses similarity or mmr so basically i have taken similarity because mmr got not good \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "055110a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiQuery_retervial_docs = multiQuery_retervier_semantic.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67d8241e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'keywords': '', 'modDate': \"D:20230720162718-04'00'\", 'author': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'creationdate': '2023-07-11T21:47:41+05:30', 'page': 1, 'creator': '', 'total_pages': 21, 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'trapped': '', 'moddate': '2023-07-20T16:27:18-04:00', 'format': 'PDF 1.4', 'creationDate': \"D:20230711214741+05'30'\"}, page_content='Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics.'),\n",
       " Document(metadata={'page': 15, 'author': '', 'keywords': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'moddate': '2023-07-20T16:27:18-04:00', 'title': 'An Overview of Artificial Intelligence Ethics', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'total_pages': 21, 'format': 'PDF 1.4', 'modDate': \"D:20230720162718-04'00'\", 'creator': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'trapped': ''}, page_content='Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'page': 0, 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationdate': '2023-07-11T21:47:41+05:30', 'creationDate': \"D:20230711214741+05'30'\", 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'trapped': '', 'keywords': '', 'creator': '', 'modDate': \"D:20230720162718-04'00'\", 'moddate': '2023-07-20T16:27:18-04:00', 'total_pages': 21, 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'format': 'PDF 1.4', 'author': '', 'title': 'An Overview of Artificial Intelligence Ethics'}, page_content='There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].'),\n",
       " Document(metadata={'modDate': \"D:20230720162718-04'00'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'moddate': '2023-07-20T16:27:18-04:00', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'keywords': '', 'trapped': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'total_pages': 21, 'author': '', 'page': 1, 'creator': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'format': 'PDF 1.4'}, page_content='It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.'),\n",
       " Document(metadata={'creationdate': '2023-07-11T21:47:41+05:30', 'total_pages': 21, 'format': 'PDF 1.4', 'creationDate': \"D:20230711214741+05'30'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'title': 'An Overview of Artificial Intelligence Ethics', 'moddate': '2023-07-20T16:27:18-04:00', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'author': '', 'page': 6, 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'keywords': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creator': '', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\"}, page_content='Thus, accountability is crucial\\nto ensure the trust of AI.'),\n",
       " Document(metadata={'creationDate': \"D:20230711214741+05'30'\", 'moddate': '2023-07-20T16:27:18-04:00', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'keywords': '', 'author': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'total_pages': 21, 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'format': 'PDF 1.4', 'page': 0}, page_content='AI\\nis increasingly taking over human tasks and replacing human\\ndecision-making.'),\n",
       " Document(metadata={'keywords': '', 'source': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'subject': '', 'modDate': \"D:20250207212217+05'30'\", 'file_path': 'Books\\\\AI Ethics  Integrating Transparency  Fairness  and Privacy in AI Development.pdf', 'total_pages': 42, 'trapped': '', 'author': '', 'creationDate': \"D:20250207212217+05'30'\", 'creator': 'PTC Arbortext Publishing Engine', 'moddate': '2025-02-07T21:22:17+05:30', 'creationdate': '2025-02-07T21:22:17+05:30', 'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)', 'page': 13, 'title': 'AI Ethics: Integrating Transparency, Fairness, and Privacy in AI Development', 'format': 'PDF 1.5'}, page_content='Continuously mon\\xad\\nitoring and maintaining the AI system post-deployment is essential.'),\n",
       " Document(metadata={'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'total_pages': 21, 'trapped': '', 'creator': '', 'moddate': '2023-07-20T16:27:18-04:00', 'modDate': \"D:20230720162718-04'00'\", 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'page': 14, 'keywords': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'creationDate': \"D:20230711214741+05'30'\", 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'format': 'PDF 1.4', 'creationdate': '2023-07-11T21:47:41+05:30', 'author': '', 'title': 'An Overview of Artificial Intelligence Ethics'}, page_content='As AI is widely used in our lives, responsible AI is becoming\\ncritical.')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiQuery_retervial_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1781e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result 1:\n",
      "Testing or evaluating whether an AI system\n",
      "meets the ethical requirements or not is an essential part\n",
      "of AI ethics.\n",
      "\n",
      " Result 2:\n",
      "Regardless of the way AI conducts moral\n",
      "reasoning, it is most critical that its moral activities conform to\n",
      "the goals of ethical design.\n",
      "\n",
      " Result 3:\n",
      "There are many other examples concerned with the\n",
      "failure, fairness, bias, privacy, and other ethical issues of AI\n",
      "systems [7].\n",
      "\n",
      " Result 4:\n",
      "It involves the ethical or moral values and\n",
      "principles that determine what is morally right and wrong.\n",
      "\n",
      " Result 5:\n",
      "Thus, accountability is crucial\n",
      "to ensure the trust of AI.\n",
      "\n",
      " Result 6:\n",
      "AI\n",
      "is increasingly taking over human tasks and replacing human\n",
      "decision-making.\n",
      "\n",
      " Result 7:\n",
      "Continuously mon­\n",
      "itoring and maintaining the AI system post-deployment is essential.\n",
      "\n",
      " Result 8:\n",
      "As AI is widely used in our lives, responsible AI is becoming\n",
      "critical.\n"
     ]
    }
   ],
   "source": [
    "multiQuery_retervial_semantic_docs_page_content = []\n",
    "for i, doc in enumerate(multiQuery_retervial_docs, 1):\n",
    "    print(f\"\\n Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    multiQuery_retervial_semantic_docs_page_content.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "829c93cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics.',\n",
       " 'Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.',\n",
       " 'There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].',\n",
       " 'It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.',\n",
       " 'Thus, accountability is crucial\\nto ensure the trust of AI.',\n",
       " 'AI\\nis increasingly taking over human tasks and replacing human\\ndecision-making.',\n",
       " 'Continuously mon\\xad\\nitoring and maintaining the AI system post-deployment is essential.',\n",
       " 'As AI is widely used in our lives, responsible AI is becoming\\ncritical.']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiQuery_retervial_semantic_docs_page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "323bf3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'What are the main ethical concerns in AI deployment'\n",
    "compressor = LLMChainExtractor.from_llm(model)\n",
    "\n",
    "contextual_retervier_semantic = ContextualCompressionRetriever(\n",
    "    base_compressor= compressor,\n",
    "    base_retriever=semantic_vector.as_retriever(search_kwargs ={'k':5},search_type='similarity')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "83ae6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_retervial_docs_semantic = contextual_retervier_semantic.invoke(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d032d0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'creator': '', 'moddate': '2023-07-20T16:27:18-04:00', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'page': 15, 'modDate': \"D:20230720162718-04'00'\", 'title': 'An Overview of Artificial Intelligence Ethics', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'author': '', 'creationdate': '2023-07-11T21:47:41+05:30', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'trapped': '', 'total_pages': 21, 'format': 'PDF 1.4', 'keywords': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503'}, page_content='Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.'),\n",
       " Document(metadata={'trapped': '', 'modDate': \"D:20230720162718-04'00'\", 'title': 'An Overview of Artificial Intelligence Ethics', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'total_pages': 21, 'author': '', 'page': 1, 'creationdate': '2023-07-11T21:47:41+05:30', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'format': 'PDF 1.4', 'moddate': '2023-07-20T16:27:18-04:00', 'keywords': '', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503'}, page_content='Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationDate': \"D:20230711214741+05'30'\", 'modDate': \"D:20230720162718-04'00'\", 'page': 1, 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'format': 'PDF 1.4', 'creationdate': '2023-07-11T21:47:41+05:30', 'moddate': '2023-07-20T16:27:18-04:00', 'creator': '', 'total_pages': 21, 'trapped': '', 'keywords': ''}, page_content='It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.'),\n",
       " Document(metadata={'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creator': '', 'trapped': '', 'page': 6, 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'keywords': '', 'modDate': \"D:20230720162718-04'00'\", 'format': 'PDF 1.4', 'creationDate': \"D:20230711214741+05'30'\", 'author': '', 'title': 'An Overview of Artificial Intelligence Ethics', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'moddate': '2023-07-20T16:27:18-04:00', 'total_pages': 21, 'creationdate': '2023-07-11T21:47:41+05:30', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV'}, page_content='Thus, accountability is crucial\\nto ensure the trust of AI.'),\n",
       " Document(metadata={'title': 'An Overview of Artificial Intelligence Ethics', 'author': '', 'creationDate': \"D:20230711214741+05'30'\", 'modDate': \"D:20230720162718-04'00'\", 'keywords': '', 'page': 0, 'total_pages': 21, 'format': 'PDF 1.4', 'moddate': '2023-07-20T16:27:18-04:00', 'subject': 'IEEE Transactions on Artificial Intelligence;2023;4;4;10.1109/TAI.2022.3194503', 'producer': 'Acrobat Distiller 11.0 (Windows); modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'trapped': '', 'source': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf', 'creationdate': '2023-07-11T21:47:41+05:30', 'creator': '', 'file_path': 'Books\\\\An_Overview_of_Artificial_Intelligence_Ethics.pdf'}, page_content='There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_retervial_docs_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7f68e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result 1:\n",
      "Regardless of the way AI conducts moral\n",
      "reasoning, it is most critical that its moral activities conform to\n",
      "the goals of ethical design.\n",
      "\n",
      " Result 2:\n",
      "Testing or evaluating whether an AI system\n",
      "meets the ethical requirements or not is an essential part\n",
      "of AI ethics.\n",
      "\n",
      " Result 3:\n",
      "It involves the ethical or moral values and\n",
      "principles that determine what is morally right and wrong.\n",
      "\n",
      " Result 4:\n",
      "Thus, accountability is crucial\n",
      "to ensure the trust of AI.\n",
      "\n",
      " Result 5:\n",
      "There are many other examples concerned with the\n",
      "failure, fairness, bias, privacy, and other ethical issues of AI\n",
      "systems [7].\n"
     ]
    }
   ],
   "source": [
    "contextual_retervial_semantic_docs_page_content=[]\n",
    "for i, doc in enumerate(contextual_retervial_docs_semantic, 1):\n",
    "    print(f\"\\n Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    contextual_retervial_semantic_docs_page_content.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "479cd18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regardless of the way AI conducts moral\\nreasoning, it is most critical that its moral activities conform to\\nthe goals of ethical design.',\n",
       " 'Testing or evaluating whether an AI system\\nmeets the ethical requirements or not is an essential part\\nof AI ethics.',\n",
       " 'It involves the ethical or moral values and\\nprinciples that determine what is morally right and wrong.',\n",
       " 'Thus, accountability is crucial\\nto ensure the trust of AI.',\n",
       " 'There are many other examples concerned with the\\nfailure, fairness, bias, privacy, and other ethical issues of AI\\nsystems [7].']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_retervial_semantic_docs_page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb06b9b",
   "metadata": {},
   "source": [
    "So Now We have 4 Reterviers and 2 Text Spitters so we will be using Evaluation method to find which Spitter and Retervier is Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-evaluation (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-evaluation\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150ac74",
   "metadata": {},
   "source": [
    "These docs cover the langchain-classic package. This package will be maintained for security vulnerabilities until December 2026. Users are encouraged to migrate to the langchain package for the latest features and improvements. See docs for langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b46b6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we will asking LLM Only Which Document is Best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8f21497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert evaluator assessing how well a retrieved text answers a query about **ethical concerns in AI deployment**.\n",
    "\n",
    "---\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### Retrieved Text (Page Content Only):\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Criteria:\n",
    "Judge this text based on:\n",
    "1. **Relevance** — How directly and completely it answers the query.\n",
    "2. **Diversity** — Whether it introduces unique insights (not generic or repetitive).\n",
    "3. **Coverage** — Whether it mentions or implies major AI ethical concerns:\n",
    "   - Bias or fairness\n",
    "   - Transparency or explainability\n",
    "   - Accountability or control\n",
    "   - Privacy or security\n",
    "   - Human autonomy / job impact\n",
    "\n",
    "---\n",
    "\n",
    "### Scoring Guide (0–5):\n",
    "Use the **entire scale confidently** based on these examples:\n",
    "- **5 (Excellent):** Directly answers the query with strong, clear, detailed coverage of multiple ethical aspects.\n",
    "- **4 (Good):** Relevant and partially comprehensive; mentions some ethical concerns clearly.\n",
    "- **3 (Average):** Somewhat related, but lacks completeness or depth; generic discussion.\n",
    "- **2 (Weak):** Only loosely related; minimal useful information.\n",
    "- **1 (Poor):** Barely connected; vague or off-topic.\n",
    "- **0 (Irrelevant):** No connection to the query.\n",
    "\n",
    "---\n",
    "\n",
    "### Task:\n",
    "Consider all three criteria **together**, and output:\n",
    "- One concise **final comment** explaining your reasoning (max 2 lines).\n",
    "- One **final_rating** (integer 0–5) reflecting the overall strength.\n",
    "\n",
    "Return strictly in this JSON format:\n",
    "{{\n",
    "  \"final_comment\": \"your short evaluation here\",\n",
    "  \"final_rating\": <integer 0–5>\n",
    "}}\n",
    "\"\"\",\n",
    "input_variables=['query','chunk']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "45681eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "555bf081",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_similarity_recursive = template1 | model | parser\n",
    "\n",
    "result_s_rec = chain_similarity_recursive.invoke({'chunk': similarity_retervial_docs_page_content,'query':Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a9d1d45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"final_comment\": \"The text touches on several relevant concerns (transparency, accountability, autonomy, control) but misses key issues like bias/fairness and privacy, and is fragmented rather than a clear answer.\",\\n  \"final_rating\": 3\\n}'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_s_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0ac42276",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_mmr_recursive = template1 | model | parser\n",
    "\n",
    "result_mmr_rec = chain_mmr_recursive.invoke({'chunk': mmr_retervial_docs_page_content,'query':Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "841be395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"final_comment\": \"It mentions transparency, explainability, accountability and control, yet omits bias, privacy and job impact, so it only partially answers the question.\",\\n  \"final_rating\": 3\\n}'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_mmr_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "76bd56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multiQuery_recursive = template1 | model | parser\n",
    "\n",
    "result_multi_rec = chain_multiQuery_recursive.invoke({'chunk': multiQuery_retervial_docs_page_content,'query':Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "194b76f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"final_comment\": \"The text mentions several concerns (bias, fairness, privacy, accountability) but is vague, missing key points like transparency and detailed job impact, and offers no structured answer.\",\\n  \"final_rating\": 3\\n}'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_multi_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3c95cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_contextual_recursive = template1 | model | parser\n",
    "\n",
    "result_contexual_rec = chain_contextual_recursive.invoke({'chunk': contextual_retervial_docs_page_content,'query':Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2c034a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"final_comment\": \"The text is relevant and mentions several key concerns (bias, fairness, accountability, privacy) but omits transparency and human impact, giving a partial answer.\",\\n  \"final_rating\": 4\\n}'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_contexual_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "708e49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_similarity_semantic = template1 | model | parser\n",
    "\n",
    "result_s_sem = chain_similarity_semantic.invoke({'chunk': similarity_retervial_semantic_docs_page_content,'query':Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "10829b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"final_comment\": \"Relevant and mentions bias, fairness, privacy, and accountability, but lacks depth and omits key concerns like transparency and human impact.\",\\n  \"final_rating\": 3\\n}'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_s_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "36160261",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_mmr_semantic = template1 | model | parser\n",
    "\n",
    "result_mmr_sem = chain_mmr_semantic.invoke({'chunk': mmr_retervial_semantic_docs_page_content,'query':Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fadb109a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"final_comment\": \"The snippet mentions bias/fairness, privacy, and accountability, but is very brief and omits major concerns like transparency and human autonomy, providing limited depth.\",\\n  \"final_rating\": 3\\n}'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_mmr_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8b9be1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multi_semantic = template1 | model | parser\n",
    "\n",
    "result_multi_sem = chain_multi_semantic.invoke({'chunk': multiQuery_retervial_semantic_docs_page_content,'query':Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4261dc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"final_comment\": \"The snippet mentions bias/fairness, privacy, accountability and job impact, but is fragmented and omits transparency/explainability, providing only a partial answer.\",\\n  \"final_rating\": 3\\n}'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_multi_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "74b9a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_contextual_semantic = template1 | model | parser\n",
    "\n",
    "result_contextual_sem = chain_contextual_semantic.invoke({'chunk': contextual_retervial_semantic_docs_page_content,'query':Query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5166729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"final_comment\": \"Relevant and mentions bias, fairness, accountability, privacy, but lacks depth and omits transparency, autonomy, and job impact.\",\\n  \"final_rating\": 3\\n}'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_contextual_sem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660adfe",
   "metadata": {},
   "source": [
    "So By Running I got 4 points to Contextual Recursive Spiltter so For continuation we use Contextual recursive Character Text Spitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c59d3",
   "metadata": {},
   "source": [
    "Picture You can see here :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8187f1",
   "metadata": {},
   "source": [
    "![AI Ethics](contextualRecursiveBest.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24e700",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c80db15",
   "metadata": {},
   "source": [
    "Augementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f58b08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert AI assistant generating a professional, human-readable summary from retrieved documents.\n",
    "\n",
    "---\n",
    "\n",
    "### Query:\n",
    "{query}\n",
    "\n",
    "### Retrieved Documents:\n",
    "Each document contains **page content** and **metadata** (e.g., source name and page number).\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "1. Analyze all retrieved documents carefully and identify information **directly relevant** to the query.  \n",
    "2. Organize your findings under **clear, concise subtopics** such as “Accountability”, “Fairness & Bias”, “Privacy”, “Reliability”, etc.  \n",
    "3. Present the answer using **real line breaks** (press Enter between lines), **not escaped `\\\\n` characters**.  \n",
    "4. Use this bullet format exactly:\n",
    "   - **Bold Subtopic** — one-sentence explanation. *(Source: [DocumentName], Page(s): X)*  \n",
    "5. Finish with a section titled **Overall Insight**, summarizing the main conclusion in one or two sentences.  \n",
    "6. Output only the formatted text — no code blocks, quotes, or JSON.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Format (strict):\n",
    "\n",
    "Final Answer:\n",
    "\n",
    "- [Subtopic] — short explanation. (Source: [DocumentName], Page(s): X)  \n",
    "- [Subtopic] — short explanation. (Source: [DocumentName], Page(s): Y)  \n",
    "- …\n",
    "\n",
    "Overall Insight:  \n",
    "Brief concluding statement.\n",
    "\n",
    "---\n",
    "\n",
    "Your response **must** contain actual line breaks and indentation, formatted for direct display in Markdown or a text report — no literal “\\\\n”.\n",
    "\"\"\",\n",
    "    input_variables=[\"query\", \"context\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "13423dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_result = template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8a33c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_summary = chain_result.invoke({'query':Query,\"context\":contextual_retervial_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "38117b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Final Answer:\\n\\n- **Accountability** — Ensuring mechanisms to hold AI systems and their creators responsible is essential for building trust. (Source: An_Overview_of_Artificial_Intelligence_Ethics.pdf, Page(s): 6)  \\n- **Fairness & Bias** — AI must avoid discriminatory outcomes and treat all users equitably. (Source: An_Overview_of_Artificial_Intelligence_Ethics.pdf, Page(s): 0)  \\n- **Privacy** — Protecting personal data from unauthorized collection or inference by AI systems is a key ethical requirement. (Source: An_Overview_of_Artificial_Intelligence_Ethics.pdf, Page(s): 0)  \\n- **Reliability (Failure)** — Systems should be robust, predictable, and avoid harmful failures in operation. (Source: An_Overview_of_Artificial_Intelligence_Ethics.pdf, Page(s): 0)  \\n\\nOverall Insight:  \\nAI deployment raises core ethical concerns—accountability, fairness and bias, privacy, and reliability—that must be addressed to ensure trustworthy and socially responsible technology.'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6dfb94",
   "metadata": {},
   "source": [
    "------------------------------------------------------THANK YOU FOR YOUR TIME -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a961e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
